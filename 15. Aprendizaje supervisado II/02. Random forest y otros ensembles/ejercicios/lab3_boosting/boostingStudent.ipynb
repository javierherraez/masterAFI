{"cells": [{"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["# Lab assignment: comparison of Boosting methods"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/yakushima.jpg\" style=\"width:600px;\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"float: right;\">(Forest at 屋久島(Kagoshima), Japan, photo by <a href=https://www.flickr.com/photos/myneur/10795515823>Indrik Myneur</a>)</div>"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["In this assignment we will perform an experimental comparison of different ensemble methods, with a focus on boosting strategies. By testing the effectiveness of these methods across a variety of datasets we will draw some conclusions on their general applicability."]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["## Guidelines"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n", "\n", "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#2655ad>\n", "This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.\n", "</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#259b4c>\n", "This is an advanced exercise that can help you gain a deeper knowledge into the topic. Good luck!</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To avoid missing packages and compatibility issues you should run this notebook under one of the [recommended Ensembles environment files](https://github.com/albarji/teaching-environments-ensembles)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["The following code will embed any plots into the notebook instead of generating a new window:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Shift+Tab to produce a pop-out with related documentation. This will only work inside code cells. \n", "\n", "Let's go!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data loading"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this assignment we will use the same datasets as in the Random Forests and Ensembles assignment. If you managed to create the pickle file with all datasets information, you can just copy it to this notebook's folder, and run the following code."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pickle as pkl\n", "\n", "with open('datasets.pkl', 'rb') as file:\n", "    datasets = pkl.load(file)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you have loaded the data properly, the following should print the whole structure:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(datasets)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Experimental setup"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our objective here is to test four different ensemble strategies to discover which one is more effective in attaining high accuracy rates on classification problems. The algorithms under test will be:\n", "\n", "* Random Forests\n", "* AdaBoost\n", "* Gradient Boosting\n", "* Extreme Gradient Boosting\n", "\n", "The first three methods are implemented in the <a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\">**sklearn** package</a>. The Extreme Gradient Boosting method, however, is an <a href=\"https://github.com/dmlc/xgboost\">independent package</a>, which however features a wrapper that allows seamless integration into scikit-learn workflows."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n", "from xgboost import XGBClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To test all methods in a comparable setting, we will perform hyper-parameter optimization, finding the set of parameters that produces the best accuracies in cross-validation over the training set. The test sets will be used only for measuring accuracies. In all four ensemble strategies we will use 50 decision trees."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n", "\n", "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "Compute the test accuracies for the Random Forest, AdaBoost, Gradient Boosting and Extreme Gradient Boosting methods for all the datasets. To do so, you must perform an optimization of hyper-parameters through cross-validation. In particular, the hyper-parameters to tune and recommended ranges are:<br>\n", "<br>\n", "     <b>Random Forest</b>\n", "    <ul>\n", "     <li>\"max_depth\": [3, 5, 10, 15, 20, 25, 30, None]</li>\n", "     <li>\"max_features\": [\"sqrt\", \"log2\", None]</li>\n", "     <li>\"min_samples_split\": [2, 4, 8, 16, 32, 64]</li>\n", "     <li>\"min_samples_leaf\": [1, 2, 4, 8, 16, 32, 64]</li>\n", "     <li>\"bootstrap\": [True, False]</li>\n", "     <li>\"criterion\": [\"gini\", \"entropy\"]</li>\n", "    </ul>\n", "     <b>AdaBoost</b>\n", "    <ul>\n", "     <li>\"learning_rate\": np.logspace(-5,0,12)</li>\n", "    </ul>\n", "     <b>Gradient Boosting</b>\n", "    <ul>\n", "     <li>\"loss\": [\"deviance\", \"exponential\"]</li>\n", "     <li>\"learning_rate\": np.logspace(-5,0,12)</li>\n", "     <li>\"max_depth\": [3, 5, 10, 15, 20, 25, 30, None]</li>\n", "     <li>\"min_samples_split\": [2, 4, 8, 16, 32, 64]</li>\n", "     <li>\"min_samples_leaf\": [1, 2, 4, 8, 16, 32, 64]</li>\n", "     <li>\"subsample\" : [0.1, 0.2, 0.5, 0.9, 1]</li>\n", "     <li>\"max_features\": [\"sqrt\", \"log2\", None]</li>\n", "    </ul>\n", "     <b>Extreme Gradient Boosting</b>\n", "    <ul>\n", "     <li>'gamma' : [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]</li>\n", "     <li>'max_depth': [6, 9, 12]</li>\n", "     <li>'subsample': [0.5, 0.9, 1.0]</li>\n", "     <li>'colsample_bytree': [0.5, 0.9, 1.0]</li>\n", "     <li>'reg_lambda' : [0, 1e-3, 1e-2, 1e-1, 1, 10, 100]</li>\n", "    </ul>\n", "     To perform the tuning it is recommended that you use <b>RandomizedSearchCV</b> from sklearn. Make sure that around 50 hyper-parameter choices are tested in each tuning. Note that RandomizedSearchCV will produce a warning for those parameter grids where the total amount of possible parameter choices is smaller than 50, but it will still produce correct results.</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#2655ad>\n", "Long waiting times? Make sure to look into the options of RandomizedSearchCV. Configuring a correct parallelization will help you accelerate training times.\n", "</font>\n", "\n", "***"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Analyzing the results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have run all the experiments, we will create a visualization to analyze the results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n", "\n", "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "  Devise some visualization that shows the mean accuracies of each ensemble algorithm across all datasets, in a way that makes clear which algorithm obtains the best results.</font>\n", "\n", "***"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Bonus round: extreme tuning"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#259b4c>\n", "Perform a more through hyper-parameter optimization, by means of changing the parameter ranges, exploring other model parameters, or increasing the number of trees in the ensembles. How much better can you fare?</font>\n", "\n", "***"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.11"}}, "nbformat": 4, "nbformat_minor": 1}