{"cells": [{"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["# Lab assignment: getting explanations from ensemble models"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/madridAutomata.jpg\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"float: right;\">(MadriD: Automata, picture by <a href=https://www.deviantart.com/albarji/art/MadriD-Automata-721505521>Albarji</a>)</div>"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["In this assignment we will perform learn how to obtain explanations from ensemble models, that allows us the get some intuition of how the model is making its decisions. We will make use of the inspection methods available in scikit-learn, together with the explanations library <a href=https://github.com/slundberg/shap#citations>SHAP</a>."]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["## Guidelines"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n", "\n", "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#2655ad>\n", "This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.\n", "</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "<font color=#259b4c>\n", "This is an advanced exercise that can help you gain a deeper knowledge into the topic. Good luck!</font>\n", "\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To avoid missing packages and compatibility issues you should run this notebook under one of the [recommended Ensembles environment files](https://github.com/albarji/teaching-environments-ensembles)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["The following code will embed any plots into the notebook instead of generating a new window:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Shift+Tab to produce a pop-out with related documentation. This will only work inside code cells. \n", "\n", "Let's go!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data loading"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For this exercise we will work the [Adult Dataset](https://archive.ics.uci.edu/ml/datasets/Adult). This dataset is made of 1994 census data of the United States of America. The objective of the dataset is to predict whether each individual earns more than 50K$/year, using demographic information. This dataset is readily available as part of the SHAP library, both in a form amenable for scikit-learn models and in its original form."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import shap\n", "shap.initjs()\n", "\n", "X,y = shap.datasets.adult()\n", "X_display,y_display = shap.datasets.adult(display=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since we are going to test several model interpretation methods, let's add a random column to simulate a useless feature."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "rng = np.random.RandomState(seed=42)\n", "X['Random Number'] = X_display['Random Number'] = rng.randn(X.shape[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test, X_display_train, X_display_test = train_test_split(X, y, X_display, test_size=0.2, random_state=7)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Random Forest feature importance (Mean Decrease in Impurity)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's start with the interpretation methods already bundled in scikit-learn. Tree-based methods can compute the importance of each feature as the reduction in impurity they obtain when they are used in the tree. Similarly an ensemble of trees can estimate the imporance of a feature by computing the mean impurity decrease among trees."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "Train a RandomForest model over the training data. You can use the default parameters. Name the trained model <b>rf</b>.\n", "</font>\n", "\n", "***"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's check that the model is able to obtain reasonable accuracy on the train and test data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"RF train accuracy: %0.3f\" % rf.score(X_train, y_train))\n", "print(\"RF test accuracy: %0.3f\" % rf.score(X_test, y_test))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since we are not using any pruning strategies, most likely your model has overfitted the data. But we will nevertheless use this model and see how different inspection methods behave."]}, {"cell_type": "markdown", "metadata": {}, "source": ["To obtain feature importances we can use the `feature_importances_` attribute of the model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rf.feature_importances_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The importances are an array with the Mean Decrease in Impurity of each feature in the whole forest. For easier understanding we can create a barplot showing these importances."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tree_feature_importances = rf.feature_importances_\n", "sorted_idx = tree_feature_importances.argsort()\n", "\n", "feature_names = X_train.columns\n", "y_ticks = np.arange(0, len(feature_names))\n", "fig, ax = plt.subplots()\n", "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n", "ax.set_yticklabels(feature_names[sorted_idx])\n", "ax.set_yticks(y_ticks)\n", "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n", "fig.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note how the most important feature is *Random Number*, even though this is a useless feature! This is because this feature can perfectly split all the training datapoints, and thus produces the largest decrease in impurity. Basing our model in a random feature is clearly overfitting, but since the Mean Decrease in Impurity is computed over the data used in the model training, it is impossible to know this is a bad feature. That is the main reason why this method does not provide a good metric to assess feature relevance. Let's move on to a better approach!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Permutation importances"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A more suitable and general method to measuring relevances is Permutation Importances. To analyze the relevance of a feature, the column with the values of such feature is rearranged through a random permutation, and the loss in model performance is understood has the relevance of such feature. We can perform this analysis over the test data to obtain less biased interpretations. Also, the permutation can be repeated a number of times (*n_repeats*) to obtain more accurate relevance estimates."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.inspection import permutation_importance\n", "\n", "result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=8)\n", "result"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Permutation Importances returns the estimated importance obtained for each feature and repetition (permutation), along with pooled means and standard deviations of importances for each feature. A good way to visualize these is by making use of boxplots:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sorted_idx = result.importances_mean.argsort()\n", "\n", "fig, ax = plt.subplots()\n", "ax.boxplot(result.importances[sorted_idx].T,\n", "           vert=False, labels=X_test.columns[sorted_idx])\n", "ax.set_title(\"Permutation Importances (test set)\")\n", "fig.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The Permutation Importance analysis reveals how the *Random Number* feature we introduced is not relevant at all, while other, more reasonable features such as *Capital Gain* are now very relevant."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Partial dependence plots"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A more fine-grained analysis of the influence of each feature on the target can be obtained through **partial dependence plots**. These plots show how changes along the value of each feature change the probability of predicting a positive class in the model. For instance, to compute this plot for the first feature in the data (*Age*) we just need to run"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.inspection import plot_partial_dependence\n", "\n", "column = X_test.columns.get_loc('Age')\n", "plot_partial_dependence(rf, X_test, features=[column])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "Generate partial dependences plot for the random feature we introduced, as well as for another relevant feature.</font>\n", "\n", "***"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## SHAP local explanations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A non-obvious problem with the approaches above is that these explanation methods **do not have consistency** when trying to explain the relevance of each feature for an specific data point. Let's suppose we have two models: A and B. Model A has larger changes in its predictions than B if a certain feature $x_i$ is removed. However, if our explanation method is non-consistent, it might happen that the computed feature relevances show a larger relevance of feature $x_i$ in model B than in model A!\n", "\n", "The SHAP library makes use of the [Shapley values](https://en.wikipedia.org/wiki/Shapley_value) from game theory to guarantee consistency, among other useful properties such as local accuracy and missingness. It can also be shown mathematically that Shapely values are the only ones that meet all of these properties simultaneously.\n", "\n", "SHAP integrates well with many ensemble method libraries. For instance, it can efficiently obtain accurate explanations for Extreme Gradient Boosting models."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "     Train an Extreme Gradient Boosting model over the training data. You can use the default parameters. Name the trained model <b>xgb</b>.\n", "\n", "***"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The SHAP library includes several explaning methods for different kind of models. For an ensemble the **TreeExplainer** is the best choice."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explainer = shap.Explainer(xgb)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With this explainer we can obtain SHAP values for every feature in every data point, as follows:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explanation = explainer(X_test)\n", "explanation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The returned object is of type `Explanation`, and contains three elements:\n", "* The SHAP `values` for each data point\n", "* The `base_values`, that is, the $\\phi_0$ bias coefficient for each data point. This represents the model prediction when no features at all are used.\n", "* The original `data` points we are explaining."]}, {"cell_type": "markdown", "metadata": {}, "source": ["An `Explanation` can be indexed as if it were an array. So for instance, we can get the information about the first data point as"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explanation[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["These values must be interpreted as follows: by default the prediction of the model will be the `base_value`. However, after analyzing each of the values of the input features, this default prediction is impacted by each one the SHAP `values`. Positive SHAP values mean the corresponding feature increases the probability of predicting positive class, while negative values reduce this probability. This can be visualized with a force plot:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.force(explanation[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The plot shows as `base value` the default value predicted by the model, and as <font color=\"red\">red</font> and <font color=\"blue\">blue</font> the features that increase or decrease the output value. The final **output value** shown in bold is the prediction made by the model after considering all the features. This representation is useful to understand why the model decided to produce its classification."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Alternatively, we can also use a waterfall plot to represent the same information"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.waterfall(explanation[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## SHAP feature relevances"]}, {"cell_type": "markdown", "metadata": {}, "source": ["By grouping all the computed SHAP values by features and drawing a scatterplot we can visualize the global relevance of each feature in the model:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.beeswarm(explanation)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## SHAP dependence plots"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also obtain partial dependence plots from SHAP values. For instance, we can plot again how the *Age* influences the probabilities of high-earnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap.plots.scatter(explanation[:,\"Age\"], color=explanation)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The plot shows a scatterplot with a dot for each datapoint in the test set, organized by their *Age* and corresponding SHAP value. To make the plot easier to understand, SHAP automatically selects another highly discriminative feature and uses it to color the dots. This provides more information than a scikit-learn partial dependence plot."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n", "\n", "***\n", "\n", "<font color=#ad3e26>\n", "     Generate SHAP partial dependences plot for the random feature we introduced, as well as for the other relevant feature you plotted in the partial dependencies exercise above. Can you spot differences between scikit-learn and SHAP plots?\n", "\n", "***"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.11"}}, "nbformat": 4, "nbformat_minor": 1}