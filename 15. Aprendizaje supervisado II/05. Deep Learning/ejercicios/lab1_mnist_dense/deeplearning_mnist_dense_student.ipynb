{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6iAgGLWZ1NH",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "# Ejercicio: clasificando dígitos con redes densas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rVh6qzyZ1NM",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/mnist.jpeg\" style=\"width:480px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRaLyDS-Z1NN",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "En este ejercicio vamos a tratar de identificar imágenes de dígitos escritos a mano. Usaremos este problema como un campo de pruebas para utilizar diferentes arquitecturas de red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elg3BKYBZ1NO",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "## Guía general"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2hzHlD6Z1NO",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "A lo largo del notebook encontrarás celdas que debes rellenar con tu propio código. Sigue las instrucciones del notebook y presta atención a los siguientes iconos:\n",
        "\n",
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Deberás resolver el ejercicio escribiendo tu propio código o respuesta en la celda inmediatamente inferior.</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcLH4ibdZ1NQ"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "Esto es una pista u observación de utilidad que puede ayudarte a resolver el ejercicio. Presta atención a estas pistas para comprender el ejercicio en mayor profundidad.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gED8eQV_Z1NR"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "Este es un ejercicio avanzado que te puede ayudar a profundizar en el tema. ¡Buena suerte!</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nUzXa9WZ1NS"
      },
      "source": [
        "Para evitar problemas con imports o incompatibilidades se recomienda ejecutar este notebook en uno de los [entornos de Deep Learning recomendados](https://github.com/albarji/teaching-environments-deeplearning), o hacer uso [Google Colaboratory](https://colab.research.google.com/). Si usas Colaboratory, asegúrate de [conectar una GPU](https://colab.research.google.com/notebooks/gpu.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K28WcwAaZ1NT"
      },
      "source": [
        "El siguiente código mostrará todas las gráficas en el propio notebook en lugar de generar una nueva ventana."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BZI8BtOMZ1NU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHsfWjVDHlzp"
      },
      "source": [
        "También vamos a fijar las semillas aleatorias de numpy y tensorflow para obtener resultados reproducibles entre varias ejecuciones del notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F4-Xljb2Hlzq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqKm77JQZ1NV"
      },
      "source": [
        "Finalmente, si necesitas ayuda en el uso de cualquier función Python, coloca el cursor sobre su nombre y presiona Shift+Tab. Aparecerá una ventana con su documentación. Esto solo funciona dentro de celdas de código.\n",
        "\n",
        "¡Vamos alla!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ukqxnv3Z1NY"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWPhQmgMZ1NY"
      },
      "source": [
        "El dataset de reconocimiento de dígitos que vamos a trabajar ya está incluído en la librería Keras. Para cargalo solo necesitamos ejecutar lo siguiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dwoAkfrZZ1NY",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87aa87b-94e9-4d0f-a544-fef0879dab0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtUM1t1CZ1NZ"
      },
      "source": [
        "Las variables **X** que se han cargado están formadas por los dígitos manuscritos a clasificar, mientras que las variables **y** nos indican las etiquetas de las imágenes correspondientes en X. Usaremos los datos de **train** para entrenar nuestra red neuronal, y los datos de **test** para medir el rendimiento de la red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk92T9T8Z1Na"
      },
      "source": [
        "Podemos comprobar cuántas imágenes tenemos para entrenar y testear de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqFTKjEUZ1Na",
        "outputId": "08dcf2dc-c462-4261-c2e6-bfaa2fdef706"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-mO-0w2Z1Nc",
        "outputId": "cff2d447-61e4-475d-dc33-edfb7269ff6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_aBgCbNZ1Nc"
      },
      "source": [
        "También podemos comprobar la forma (ancho y alto en píxeles) de una imagen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rYORERfZ1Nc",
        "outputId": "bf355ac8-3881-41c5-9f61-e9e00c9e8ee7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X_test[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N243y_TyZ1Nd"
      },
      "source": [
        "Podemos visualizar las imágenes del dataset a través de la librería **matplotlib**. En la siguiente celda tomamos la primera imagen de entrenamiento y la visualicemos en una escala de grises. También estamos imprimiendo la clase correspondiente a esa imagen, para comprobar que el etiquetado de la imagen es correcto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "xQHQi6omZ1Nd",
        "outputId": "79890961-5f59-4c7f-a7e0-b12a5702d57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digit class: 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(X_train[0], 'gray')\n",
        "print(\"Digit class:\", y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3xs5__1Z1Ne"
      },
      "source": [
        "## Preparación de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I7E7MkXZ1Ne"
      },
      "source": [
        "### Normalización de las entradas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTb8p2saZ1Nf"
      },
      "source": [
        "Ante de construir una red neuronal, primero debemos siempre normalizar los datos. Generalmente la normalización implica restar la media y dividir la desviación estándar de los datos. No obstante, para el caso de imágenes en escala de gris como estas, cada valor de los datos representa la intensidad de un pixel, un valor que está acotado en el rango [0, 255]. Podemos por tanto realizar una normalización sencilla consistente en dividir los datos por 255 para que se queden ajustados al rango [0, 1]. También necesitaremos tranformar el tipo de datos a `float`, o de otro modo no podremos representar valores decimales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AAmRXuUuZ1Nf"
      },
      "outputs": [],
      "source": [
        "X_train_norm = X_train.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PAbCmbgZ1Nf"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Realiza la misma normalización para los datos de entrada de test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "acvP9jrQZ1Ng"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "X_test_norm = X_test.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPGLpjNpZ1Ng"
      },
      "source": [
        "### Codificación de las salidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2EO1xO7Z1Ng"
      },
      "source": [
        "Los datos de salida para una red neuronal no requieren normalización, pero sí deben ser codificados siguiendo un formato en particular. En lugar de tener un valor entero en el rango [0, 9] que indique la clase de la imagen, utilizaremos una codificación de tipo <a href=\"https://en.wikipedia.org/wiki/One-hot\">one-hot</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "39jiX5H-Z1Nh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train_encoded = to_categorical(y_train, 10) # We have 10 classes to codify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI_sfK-9Z1Nh"
      },
      "source": [
        "Podemos comprobar que la transformación ha generado una codificación one-hot correcta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEZYZaqhZ1Ni",
        "outputId": "e796c0d4-8834-4a61-9e12-03cc3b8c846a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "y_train_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K10ISFigZ1Ni"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Realiza la misma codificación para los datos de salida de test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vwsMCUTiZ1Ni"
      },
      "outputs": [],
      "source": [
        "y_test_encoded = to_categorical(y_test, 10) # We have 10 classes to codify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSi64MJOZ1Nj"
      },
      "source": [
        "### Aplanado (flattening) de las entradas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTbHsiFIZ1Nj"
      },
      "source": [
        "Nuestra preparación de datos está casi terminada, pero falta un pequeño detalle: nuestros datos son imágenes bidimensionales, pero una red neuronal estándar solo es capaz de trabajar con datos en la forma de vectores unidimensionales de variables. Debemos transformar los datos a vectores antes de introducirlos en la red neuronal, algo que podemos hacer con el método `reshape`. Dado que tenemos 60000 imágenes de entrenamiento de 28x28 (784 en total) píxeles, el `reshape` a realizar es:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z8kWfhozZ1Nj"
      },
      "outputs": [],
      "source": [
        "trainvectors = X_train_norm.reshape(60000, 784)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ZGZr4AZ1Nj"
      },
      "source": [
        "Podemos comprobar ahora que nuestros datos de entrenamiento se han convertido en una matriz de 60000 datos (filas) y 784 variables (columnas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXK4c3CqZ1Nk",
        "outputId": "1d1e2775-45bc-4d9c-f89d-6ca114c3561e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "trainvectors.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdvuGPFnZ1Nk"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Realiza una transformación similar para los datos de entrada de test (X_test), guardando los datos tras el reshape en una nueva varible llamada <b>testvectors</b>. Ten en cuenta que en test contamos con 10000 imágenes, en lugar de las 60000 de entrenamiento.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "M-v5GQvpZ1Nk"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "testvectors = X_test_norm.reshape(10000, 784)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vrVCbf7Z1Nl"
      },
      "source": [
        "Ten en mente que este aplanado en vectores unidimensionales es algo que solo necesitamos hacer para el tipo de redes que trabajaremos en este notebook. Cuando en otros notebooks avancemos a redes especializadas en imágenes, podremos hacer que la red utilice directamente las imágenes como entradas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArqdD4oNZ1Nl"
      },
      "source": [
        "## Perceptrón"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7v_ZF6UZ1Nl"
      },
      "source": [
        "Empezaremos tratando el problema con la red neuronal más sencilla: un perceptrón. Esto quiere decir que tendremos una red neuronal sin capas ocultas, únicamente conexiones desde las entradas a las salidas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_AlpjQdZ1Nl"
      },
      "source": [
        "### Definiendo la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-71j5rZRZ1Nn"
      },
      "source": [
        "Para construir una red en Keras primero debemos definir el tipo de arquitectura:\n",
        "\n",
        "* **Sequential**: cada nueva capa se conecta a la capa declarada inmediatamente antes en la red, siguiendo una cadena.\n",
        "* **Functional**: cada capa puede conectarse a la salida de cualquier otra capa declarada antes en la red, siempre y cuando no se formen ciclos.\n",
        "\n",
        "Para este ejercicio será suficiente con una arquitectura de tipo Sequential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9Dp2-DiYZ1Nn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "perceptron = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtNzCoZtZ1Nn"
      },
      "source": [
        "Una vez la red ha sido inicializada, podemos ir añadiendo las capas deseadas de manera iterativa. Para construir un perceptrón solo nos hará falta una capa \"clásica\" de pesos desde las entradas a la salida. En Deep Learning moderno este tipo de capas se llaman **Dense**, porque conectan todas las variables de entrada con todas las salidas de la capa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "02ooaQjTZ1Nn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xiq_3MrZ1No"
      },
      "source": [
        "Para crear una capa densa suele bastar con declarar el número de unidades de salida (o desde un punto de vista clásico, el número de neuronas). No obstante, en un modelo de tipo Sequential estamos obligados a declarar el número de variables de entrada a la red neuronal cuando creamos nuestra primera capa. En este problema tenemos 784 variables explicativas (28x28 píxeles). En cuanto a las salidas, en este problema tenemos 10 clases, por lo que deberemos crear 10 unidades de salida. También debemos tener en consideración que estamos trabajando un problema multiclase, y por tanto debemos escoger una función de activación que delimite los valores de salida de la red al rango [0, 1], asegurando que la suma de todos estos valores es `1`. La activación `softmax` es la adecuada en estos casos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t6OfS1qjZ1No"
      },
      "outputs": [],
      "source": [
        "perceptron.add(Dense(10, input_dim=784, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfmVQv3iZ1No"
      },
      "source": [
        "Con esto, la definición de la red está completa. Podemos confirmar que la hemos construído correctamente pidiendo a Keras un resumen del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYpHwgdAZ1Np",
        "outputId": "2cb9fc03-49fb-40ea-9335-b326a338ab0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "perceptron.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-djMZ0PUZ1Np"
      },
      "source": [
        "### Compilando la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t06_WzQfZ1Np"
      },
      "source": [
        "Tras definir la red, debemos realizar la compilación de la misma. La compilación es un proceso automático que transforma la definición de la red en una formulación simbólica equivalente para la que pueden calcularse las derivadas, permitiendo así ejecutar el algoritmo de retropropagación (backpropagation). Los únicos parámetros que debemos especificar son la función de pérdida o error que la red debe minimizar, y el optimizador a usar durante el aprendizaje.\n",
        "\n",
        "Dado que estamos tratando con un problema de clasificación multiclase, la función de pérdida más adecuada es la **categorical crossentropy**. En cuanto al optimizador, de momento utilizaremos el **Stochastic Gradient Descent**. Como parámetro opcional, solicitaremos que como métrica se nos informe de la **accuracy** durante el entrenamiento de la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GHnCE0z-Z1Np"
      },
      "outputs": [],
      "source": [
        "perceptron.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYqX_OaDZ1Nq"
      },
      "source": [
        "### Entrenando la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7QCiJQRZ1Nq"
      },
      "source": [
        "Ahora podemos invocar al método `fit` de la red, el encargado de ejecutar el proceso de entrenamiento. Para este problema utilizaremos un tamaño de batch de 128, y 20 épocas de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ2rrI6NZ1Nq",
        "outputId": "7c647ecb-640e-4694-bcd6-531b9c32db2b",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 - 3s - loss: 1.2887 - accuracy: 0.7005 - 3s/epoch - 7ms/step\n",
            "Epoch 2/20\n",
            "469/469 - 1s - loss: 0.7146 - accuracy: 0.8439 - 1s/epoch - 3ms/step\n",
            "Epoch 3/20\n",
            "469/469 - 1s - loss: 0.5860 - accuracy: 0.8604 - 1s/epoch - 2ms/step\n",
            "Epoch 4/20\n",
            "469/469 - 1s - loss: 0.5245 - accuracy: 0.8697 - 1s/epoch - 3ms/step\n",
            "Epoch 5/20\n",
            "469/469 - 1s - loss: 0.4872 - accuracy: 0.8759 - 1s/epoch - 2ms/step\n",
            "Epoch 6/20\n",
            "469/469 - 1s - loss: 0.4615 - accuracy: 0.8801 - 1s/epoch - 3ms/step\n",
            "Epoch 7/20\n",
            "469/469 - 1s - loss: 0.4425 - accuracy: 0.8840 - 1s/epoch - 3ms/step\n",
            "Epoch 8/20\n",
            "469/469 - 1s - loss: 0.4276 - accuracy: 0.8865 - 1s/epoch - 2ms/step\n",
            "Epoch 9/20\n",
            "469/469 - 1s - loss: 0.4156 - accuracy: 0.8891 - 1s/epoch - 2ms/step\n",
            "Epoch 10/20\n",
            "469/469 - 1s - loss: 0.4058 - accuracy: 0.8903 - 1s/epoch - 3ms/step\n",
            "Epoch 11/20\n",
            "469/469 - 1s - loss: 0.3973 - accuracy: 0.8920 - 1s/epoch - 3ms/step\n",
            "Epoch 12/20\n",
            "469/469 - 1s - loss: 0.3900 - accuracy: 0.8941 - 1s/epoch - 3ms/step\n",
            "Epoch 13/20\n",
            "469/469 - 1s - loss: 0.3837 - accuracy: 0.8953 - 1s/epoch - 2ms/step\n",
            "Epoch 14/20\n",
            "469/469 - 1s - loss: 0.3781 - accuracy: 0.8965 - 1s/epoch - 3ms/step\n",
            "Epoch 15/20\n",
            "469/469 - 1s - loss: 0.3731 - accuracy: 0.8976 - 1s/epoch - 2ms/step\n",
            "Epoch 16/20\n",
            "469/469 - 1s - loss: 0.3685 - accuracy: 0.8986 - 1s/epoch - 3ms/step\n",
            "Epoch 17/20\n",
            "469/469 - 1s - loss: 0.3644 - accuracy: 0.8999 - 1s/epoch - 3ms/step\n",
            "Epoch 18/20\n",
            "469/469 - 1s - loss: 0.3607 - accuracy: 0.9009 - 1s/epoch - 3ms/step\n",
            "Epoch 19/20\n",
            "469/469 - 1s - loss: 0.3573 - accuracy: 0.9014 - 1s/epoch - 3ms/step\n",
            "Epoch 20/20\n",
            "469/469 - 1s - loss: 0.3541 - accuracy: 0.9024 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8560132490>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "perceptron.fit(\n",
        "    trainvectors, # Training data\n",
        "    y_train_encoded, # Labels of training data\n",
        "    batch_size=128, # Batch size for the optimizer algorithm\n",
        "    epochs=20, # Number of epochs to run the optimizer algorithm\n",
        "    verbose=2 # Level of verbosity of the log messages\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8CrwOFaZ1Nr"
      },
      "source": [
        "Ahora que nuestra red está entrenada, podemos obtener predicciones para el conjunto de test como"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RwFG6kYSZ1Nr"
      },
      "outputs": [],
      "source": [
        "probs = perceptron.predict(testvectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2YthyGXZ1Ns"
      },
      "source": [
        "Las predicciones se obtienen como una matriz con forma `(n_datos, clases)`, que pueden interpretarse como las probabilidades de que la imagen pertenezca a cada una de las clases posibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSiYjuzRZ1Ns",
        "outputId": "6f325872-fcb2-4ae4-873f-7428cc30515f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.58102722e-04, 8.01071394e-07, 1.99522023e-04, ...,\n",
              "        9.94534850e-01, 1.76103087e-04, 2.72493181e-03],\n",
              "       [1.26140425e-02, 1.63576915e-04, 8.68396878e-01, ...,\n",
              "        3.76514350e-07, 7.84975663e-03, 4.99391444e-06],\n",
              "       [1.99456161e-04, 9.50817883e-01, 1.63212810e-02, ...,\n",
              "        4.42804350e-03, 9.89920553e-03, 1.84979360e-03],\n",
              "       ...,\n",
              "       [7.10017912e-06, 2.92644909e-05, 1.47686122e-04, ...,\n",
              "        6.66955533e-03, 2.16466989e-02, 5.35607561e-02],\n",
              "       [6.61989488e-03, 8.03831033e-03, 2.10193591e-03, ...,\n",
              "        1.33099617e-03, 4.83725011e-01, 2.25255056e-03],\n",
              "       [3.76263139e-04, 2.18210729e-08, 1.20737671e-03, ...,\n",
              "        7.74323894e-08, 8.18955778e-06, 1.93739811e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekhCjQtzZ1Ns"
      },
      "source": [
        "Si usando estas probabilidades tuviéramos que apostar por qué digito es el que se representa en la imagen, lo más sensato sería decantarnos por el que tiene mayor probabilidad. Podemos hacer esto fácilmente empleando la función [argmax de numpy](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WBG7APVGZ1Nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c911f6-7a2d-429e-a4cc-78321d897da3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 8, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(probs, axis=-1)\n",
        "preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2EvRHIYZ1Nt"
      },
      "source": [
        "Ahora que tenemos predicciones, si por ejemplo tomamos el primer ejemplo de test, podemos ver su imagen y la correspondiente clase predicha por la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "QgwsP-nJZ1Nt",
        "outputId": "db1223e1-440e-4681-b3a0-3e30417bf708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real class 7 predicted class 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(X_test[0], 'gray')\n",
        "print(\"Real class\", y_test[0], \"predicted class\", preds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoeKnsLdgOeY"
      },
      "source": [
        "Puede que para este caso particular hayamos acertado pero, ¿qué ocurre con el resto del dataset de test? Una forma rápida de encontrar todas las imágenes de test en las que hemos fallado es usando la función [where](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.where.html) de numpy, que nos devuelve las posiciones en las que una cierta lista contiene el valor `True`. Para este caso, construimos una lista de los índices de las imágenes en las que las predicciones no son correctas, comparando la lista de predicciones con la lista de clases reales de los dígitos de test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgZaKwrygmhG",
        "outputId": "01ac77aa-335e-4676-fe3f-62b8f0a17936"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   8,   33,   46,   63,   77,   92,  111,  124,  149,  193,  195,\n",
              "        217,  233,  241,  245,  247,  259,  290,  300,  307,  313,  318,\n",
              "        320,  321,  340,  341,  352,  358,  362,  389,  403,  435,  444,\n",
              "        445,  448,  449,  457,  464,  468,  478,  479,  483,  495,  502,\n",
              "        507,  511,  515,  524,  530,  531,  536,  543,  550,  551,  553,\n",
              "        565,  569,  578,  582,  591,  605,  610,  613,  619,  624,  627,\n",
              "        628,  629,  638,  658,  659,  667,  684,  691,  707,  717,  720,\n",
              "        728,  738,  740,  741,  760,  791,  800,  839,  844,  846,  857,\n",
              "        877,  881,  898,  900,  924,  930,  938,  939,  944,  947,  950,\n",
              "        956,  959,  965,  975,  982,  992,  999, 1012, 1014, 1028, 1032,\n",
              "       1033, 1039, 1044, 1050, 1062, 1068, 1073, 1082, 1096, 1101, 1107,\n",
              "       1112, 1114, 1119, 1128, 1147, 1153, 1173, 1181, 1191, 1192, 1194,\n",
              "       1198, 1200, 1202, 1204, 1206, 1208, 1217, 1224, 1226, 1228, 1232,\n",
              "       1233, 1234, 1242, 1247, 1248, 1251, 1256, 1260, 1272, 1283, 1289,\n",
              "       1291, 1299, 1315, 1319, 1326, 1328, 1337, 1339, 1345, 1347, 1375,\n",
              "       1378, 1391, 1393, 1394, 1402, 1410, 1429, 1433, 1440, 1444, 1453,\n",
              "       1465, 1466, 1467, 1494, 1500, 1514, 1522, 1525, 1527, 1530, 1549,\n",
              "       1553, 1559, 1581, 1587, 1609, 1621, 1634, 1640, 1641, 1678, 1681,\n",
              "       1695, 1696, 1709, 1716, 1717, 1718, 1722, 1727, 1741, 1751, 1754,\n",
              "       1759, 1765, 1772, 1773, 1774, 1782, 1790, 1800, 1813, 1819, 1828,\n",
              "       1839, 1842, 1850, 1857, 1865, 1868, 1874, 1878, 1883, 1899, 1901,\n",
              "       1903, 1911, 1917, 1926, 1930, 1938, 1940, 1948, 1952, 1955, 1956,\n",
              "       1968, 1970, 1973, 1981, 1982, 1984, 1989, 2016, 2024, 2035, 2037,\n",
              "       2040, 2043, 2044, 2053, 2068, 2070, 2093, 2098, 2099, 2105, 2109,\n",
              "       2110, 2115, 2118, 2129, 2130, 2134, 2135, 2138, 2148, 2168, 2182,\n",
              "       2185, 2186, 2189, 2192, 2215, 2224, 2266, 2269, 2272, 2282, 2293,\n",
              "       2298, 2299, 2305, 2325, 2351, 2362, 2369, 2371, 2378, 2380, 2381,\n",
              "       2387, 2393, 2394, 2395, 2397, 2404, 2406, 2408, 2422, 2425, 2433,\n",
              "       2449, 2488, 2516, 2525, 2534, 2542, 2556, 2559, 2560, 2573, 2574,\n",
              "       2578, 2586, 2589, 2598, 2604, 2607, 2610, 2628, 2631, 2635, 2648,\n",
              "       2654, 2670, 2695, 2705, 2713, 2730, 2740, 2751, 2770, 2771, 2780,\n",
              "       2805, 2810, 2820, 2832, 2850, 2851, 2866, 2896, 2905, 2906, 2907,\n",
              "       2914, 2919, 2925, 2927, 2945, 2953, 2969, 2970, 2990, 2995, 3005,\n",
              "       3060, 3073, 3102, 3106, 3110, 3114, 3117, 3130, 3133, 3136, 3139,\n",
              "       3145, 3151, 3157, 3160, 3166, 3183, 3189, 3193, 3206, 3218, 3225,\n",
              "       3240, 3254, 3262, 3269, 3280, 3284, 3288, 3302, 3316, 3329, 3330,\n",
              "       3333, 3369, 3394, 3436, 3437, 3448, 3450, 3468, 3475, 3490, 3503,\n",
              "       3520, 3543, 3549, 3558, 3565, 3567, 3573, 3578, 3580, 3597, 3598,\n",
              "       3604, 3629, 3645, 3662, 3664, 3681, 3702, 3716, 3718, 3725, 3726,\n",
              "       3730, 3732, 3751, 3752, 3757, 3763, 3767, 3769, 3776, 3780, 3794,\n",
              "       3796, 3801, 3806, 3808, 3811, 3817, 3820, 3821, 3833, 3836, 3838,\n",
              "       3839, 3846, 3848, 3850, 3853, 3855, 3862, 3869, 3876, 3884, 3893,\n",
              "       3902, 3906, 3926, 3941, 3943, 3946, 3951, 3952, 3954, 3962, 3976,\n",
              "       3984, 3985, 3986, 4000, 4002, 4017, 4044, 4059, 4063, 4065, 4072,\n",
              "       4075, 4076, 4078, 4093, 4111, 4131, 4140, 4145, 4152, 4154, 4156,\n",
              "       4159, 4163, 4173, 4176, 4177, 4180, 4199, 4201, 4205, 4211, 4212,\n",
              "       4224, 4238, 4239, 4248, 4254, 4256, 4261, 4265, 4271, 4284, 4289,\n",
              "       4297, 4300, 4302, 4306, 4313, 4315, 4317, 4325, 4330, 4341, 4344,\n",
              "       4355, 4356, 4359, 4374, 4405, 4423, 4427, 4433, 4435, 4449, 4451,\n",
              "       4454, 4477, 4497, 4498, 4500, 4521, 4523, 4540, 4567, 4571, 4575,\n",
              "       4578, 4583, 4601, 4615, 4633, 4639, 4640, 4671, 4699, 4722, 4724,\n",
              "       4731, 4740, 4751, 4761, 4785, 4807, 4808, 4812, 4814, 4823, 4827,\n",
              "       4829, 4837, 4838, 4852, 4863, 4874, 4876, 4879, 4880, 4886, 4890,\n",
              "       4896, 4910, 4915, 4939, 4943, 4950, 4952, 4954, 4956, 4966, 4968,\n",
              "       4978, 4990, 4995, 5001, 5009, 5038, 5046, 5054, 5065, 5067, 5068,\n",
              "       5078, 5086, 5100, 5135, 5138, 5140, 5176, 5177, 5210, 5217, 5246,\n",
              "       5288, 5298, 5299, 5331, 5360, 5457, 5562, 5600, 5617, 5620, 5634,\n",
              "       5642, 5649, 5653, 5662, 5677, 5688, 5695, 5714, 5718, 5734, 5735,\n",
              "       5749, 5801, 5821, 5835, 5852, 5862, 5867, 5874, 5887, 5888, 5891,\n",
              "       5912, 5913, 5922, 5936, 5937, 5955, 5973, 5975, 5985, 6004, 6023,\n",
              "       6035, 6037, 6042, 6043, 6059, 6065, 6071, 6081, 6091, 6109, 6112,\n",
              "       6124, 6157, 6166, 6168, 6172, 6173, 6174, 6304, 6324, 6347, 6385,\n",
              "       6391, 6392, 6421, 6425, 6426, 6432, 6494, 6495, 6501, 6505, 6517,\n",
              "       6555, 6560, 6568, 6569, 6574, 6577, 6597, 6598, 6603, 6613, 6625,\n",
              "       6632, 6641, 6642, 6651, 6688, 6694, 6706, 6716, 6721, 6740, 6744,\n",
              "       6746, 6768, 6769, 6775, 6785, 6796, 6817, 6836, 6847, 6885, 6906,\n",
              "       6909, 6919, 6926, 7035, 7094, 7107, 7121, 7130, 7212, 7220, 7233,\n",
              "       7235, 7241, 7248, 7262, 7265, 7304, 7338, 7432, 7434, 7436, 7451,\n",
              "       7454, 7459, 7473, 7492, 7498, 7511, 7541, 7542, 7545, 7580, 7603,\n",
              "       7637, 7641, 7672, 7673, 7724, 7797, 7800, 7821, 7826, 7839, 7842,\n",
              "       7847, 7849, 7850, 7856, 7857, 7858, 7859, 7870, 7876, 7886, 7888,\n",
              "       7899, 7900, 7905, 7917, 7918, 7920, 7928, 7945, 8020, 8044, 8047,\n",
              "       8062, 8072, 8081, 8091, 8094, 8095, 8165, 8183, 8196, 8198, 8246,\n",
              "       8272, 8277, 8279, 8294, 8308, 8332, 8339, 8353, 8408, 8410, 8426,\n",
              "       8444, 8456, 8476, 8520, 8522, 8553, 8639, 9006, 9007, 9009, 9010,\n",
              "       9013, 9015, 9016, 9017, 9019, 9024, 9026, 9036, 9045, 9071, 9110,\n",
              "       9141, 9168, 9182, 9211, 9214, 9245, 9280, 9316, 9317, 9426, 9446,\n",
              "       9456, 9465, 9482, 9534, 9544, 9554, 9560, 9587, 9595, 9614, 9624,\n",
              "       9634, 9642, 9643, 9662, 9679, 9680, 9700, 9712, 9716, 9719, 9726,\n",
              "       9729, 9732, 9733, 9735, 9740, 9741, 9744, 9745, 9749, 9751, 9752,\n",
              "       9764, 9768, 9770, 9777, 9779, 9792, 9808, 9811, 9832, 9839, 9840,\n",
              "       9847, 9855, 9856, 9858, 9867, 9874, 9879, 9883, 9888, 9890, 9893,\n",
              "       9905, 9925, 9940, 9941, 9943, 9944, 9953, 9970, 9975, 9980, 9982,\n",
              "       9986, 9998])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "(fails,) = np.where(y_test != preds)\n",
        "fails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX9iVzP5gtzR"
      },
      "source": [
        "Vamos a visualizar alguna de las imágenes en las que la red se ha equivocado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "25hYGzhgg9jr",
        "outputId": "b64971a2-cccd-471f-e91d-4d4a50680d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real class 5 predicted class 6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(X_test[fails[0]], 'gray')\n",
        "print(\"Real class\", y_test[fails[0]], \"predicted class\", preds[fails[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-u5spGsZ1Nv"
      },
      "source": [
        "Podríamos probar cualquier otra imagen fallida del conjunto de test para analizar en qué tipo de imágenes nuestra red está cometiendo errores. Pero si queremos tener una idea más general de cómo de bien está funcionando la red, podemos calcular su acierto (accuracy). Esto se hace con el método `evaluate` de la red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMJpKeQQZ1Nv",
        "outputId": "a0040e42-3427-4060-c64f-42464b4f0512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3356 - accuracy: 0.9085\n",
            "Test loss 0.33563685417175293\n",
            "Test accuracy 0.9085000157356262\n"
          ]
        }
      ],
      "source": [
        "score = perceptron.evaluate(testvectors, y_test_encoded)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyNKnJ7ThcWD"
      },
      "source": [
        "Ahora podríamos plantearnos, ¿es este nivel de acierto suficientemente bueno? Imaginemos que nuestro detector de dígitos se usa para escanear los códigos postales de envío de un paquete. Si consideramos que los [códigos postales extendidos de los Estados Unidos](https://en.wikipedia.org/wiki/ZIP_Code) están conformados por 9 dígitos, y que con fallar uno de los dígitos estaríamos enviando un paquete al lugar equivocado... ¿cuál es la probabilidad de mandar el paquete correctamente?\n",
        "\n",
        "Calcular esta probabilidad es sencillo. Sabemos la probabilidad de que nuestro modelo haga una buena predicción para un solo dígito (la accuracy), así que solo debemos calcular la probabilidad de que el modelo acierte a la vez para los 9 dígitos del código postal. Esta probabilidad conjunta se calcularía como $P(acierto) * P(acierto) * P(acierto) ... = P(acierto)^9$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLls_3z-h0xY",
        "outputId": "e817a3f4-74e8-49f2-80bf-fb2267894614"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42162315378032617"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "score[1]**9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9yBuqVtiAfz"
      },
      "source": [
        "Podemos concluir que usar un perceptrón en este problema tendría resultados catastróficos: solo un 41% de paquetes enviados a la dirección correcta. ¡Necesitamos hacerlo mejor!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIl4zPDjZ1Nw"
      },
      "source": [
        "## Perceptrón multicapa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOEuLlmnZ1Nw"
      },
      "source": [
        "Pasando a los años 80, podemos mejorar nuestra red introduciendo **capas ocultas**. En Keras esto se implementa de forma sencilla introduciendo capas Dense adicionales. Por ejemplo, podemos crear una red con una capa oculta de 32 neuronas de la siguiente manera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Ix2gNNg1Z1Nx"
      },
      "outputs": [],
      "source": [
        "mlp = Sequential()\n",
        "mlp.add(Dense(32, input_dim=784, activation=\"sigmoid\"))\n",
        "mlp.add(Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34lur0NjZ1Ny"
      },
      "source": [
        "La red quedaría, por tanto, configurada así"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjjO1xM6Z1Ny",
        "outputId": "b5458e75-c411-4e19-f3ec-8c91e02aa36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 32)                25120     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,450\n",
            "Trainable params: 25,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "mlp.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9fnUPSGZ1Ny"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Compila el perceptrón multicapa y entrénalo con los datos de train. ¿Qué nivel de acierto se obtiene al evaluarlo en test? ¿Has mejorado respecto del perceptrón anterior?\n",
        "</font>\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcfiAKfjZ1Nz",
        "outputId": "e607c2ff-93cf-4345-8b21-26042e4b2356",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 2.1455 - accuracy: 0.4245\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.7803 - accuracy: 0.6281\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.4917 - accuracy: 0.7042\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.2580 - accuracy: 0.7529\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.0812 - accuracy: 0.7856\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.9503 - accuracy: 0.8050\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.8525 - accuracy: 0.8200\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.7776 - accuracy: 0.8299\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.7186 - accuracy: 0.8397\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6710 - accuracy: 0.8479\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6317 - accuracy: 0.8544\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.5986 - accuracy: 0.8604\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.5704 - accuracy: 0.8653\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.5461 - accuracy: 0.8695\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.5249 - accuracy: 0.8738\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.5063 - accuracy: 0.8766\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4898 - accuracy: 0.8793\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4752 - accuracy: 0.8819\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4620 - accuracy: 0.8840\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4501 - accuracy: 0.8860\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84eaeb5f10>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "mlp.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mlp.fit(\n",
        "    trainvectors, # Training data\n",
        "    y_train_encoded, # Labels of training data\n",
        "    batch_size=128, # Batch size for the optimizer algorithm\n",
        "    epochs=20, # Number of epochs to run the optimizer algorithm\n",
        "    verbose=1 # Level of verbosity of the log messages\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = mlp.evaluate(testvectors, y_test_encoded)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgiZygaRO7kX",
        "outputId": "c4a4fe38-4c16-4984-cc56-dd0f1149716f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4293 - accuracy: 0.8931\n",
            "Test loss 0.429255872964859\n",
            "Test accuracy 0.8931000232696533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESHcARZlZ1Nz"
      },
      "source": [
        "### Mejorando el diseño de la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW3Q7WUZZ1Nz"
      },
      "source": [
        "Para mejorar el rendimiento del perceptrón multicapa utilizaremos las siguientes técnicas:\n",
        "* Mayor número de unidades ocultas\n",
        "* Mejor función de activación: ReLU\n",
        "* Mejor optimizador: adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lsChdOOZ1Nz"
      },
      "source": [
        "Definiremos por tanto la red de la seguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4j3EItImZ1N0"
      },
      "outputs": [],
      "source": [
        "mlp_fine = Sequential()\n",
        "mlp_fine.add(Dense(100, input_dim=784, activation=\"relu\"))\n",
        "mlp_fine.add(Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbM_U_LgZ1N0"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Compila la red definida arriba, escogiendo \"adam\" como algoritmo de optimización, y entrénala con los datos de train. Mide el rendimiento sobre los datos de test. ¿Han ayudado estos cambios a mejorar el acierto?\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrNOPfCbZ1N0",
        "outputId": "a7425aeb-c130-45e3-9936-91d031642d2f",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3722 - accuracy: 0.8986\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1705 - accuracy: 0.9523\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1260 - accuracy: 0.9639\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0996 - accuracy: 0.9711\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0822 - accuracy: 0.9761\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0691 - accuracy: 0.9800\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0591 - accuracy: 0.9833\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0503 - accuracy: 0.9855\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0430 - accuracy: 0.9882\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0371 - accuracy: 0.9894\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0324 - accuracy: 0.9910\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0282 - accuracy: 0.9922\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0241 - accuracy: 0.9937\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0212 - accuracy: 0.9949\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0184 - accuracy: 0.9959\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0163 - accuracy: 0.9962\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0140 - accuracy: 0.9970\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0129 - accuracy: 0.9973\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0113 - accuracy: 0.9976\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0093 - accuracy: 0.9983\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84ead5a790>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "####### INSERT YOUR CODE HERE\n",
        "mlp_fine.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "mlp_fine.fit(\n",
        "    trainvectors, # Training data\n",
        "    y_train_encoded, # Labels of training data\n",
        "    batch_size=128, # Batch size for the optimizer algorithm\n",
        "    epochs=20, # Number of epochs to run the optimizer algorithm\n",
        "    verbose=1 # Level of verbosity of the log messages\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = mlp_fine.evaluate(testvectors, y_test_encoded)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJPe1rUTOgSV",
        "outputId": "4a8ab8b6-1a42-4287-b4a5-36a50cba4a09"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0860 - accuracy: 0.9767\n",
            "Test loss 0.08597765862941742\n",
            "Test accuracy 0.9767000079154968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na2EgqadZ1N0"
      },
      "source": [
        "### Más capas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrstMzprZ1N1"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Define una nueva red con 2 capas ocultas, cada una de ellas de 512 unidades ocultas con activación ReLU. Para la capa de salida, recuerda utilizar la activación softmax. Compila la red definida, escogiendo \"adam\" como algoritmo de optimización, y entrénala con los datos de entrenamiento. Mide entonces el rendimiento de esta red sobre los datos de test. ¿Has obtenido mejoras?\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "tXO82RjzZ1N1",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "mlp2 = Sequential()\n",
        "mlp2.add(Dense(512, input_dim=784, activation=\"relu\"))\n",
        "mlp2.add(Dense(512, activation=\"relu\"))\n",
        "mlp2.add(Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "mlp2.fit(\n",
        "    trainvectors, # Training data\n",
        "    y_train_encoded, # Labels of training data\n",
        "    batch_size=128, # Batch size for the optimizer algorithm\n",
        "    epochs=20, # Number of epochs to run the optimizer algorithm\n",
        "    verbose=1 # Level of verbosity of the log messages\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqUAAA3NQDBe",
        "outputId": "078c9601-f5e0-40cc-9692-482de7659617"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.2235 - accuracy: 0.9337\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0802 - accuracy: 0.9752\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0509 - accuracy: 0.9837\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0358 - accuracy: 0.9884\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0265 - accuracy: 0.9909\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0232 - accuracy: 0.9924\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0190 - accuracy: 0.9938\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0156 - accuracy: 0.9949\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0150 - accuracy: 0.9950\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0155 - accuracy: 0.9947\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0122 - accuracy: 0.9959\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0117 - accuracy: 0.9960\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0097 - accuracy: 0.9966\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0080 - accuracy: 0.9976\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0086 - accuracy: 0.9972\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0128 - accuracy: 0.9962\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0063 - accuracy: 0.9978\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0045 - accuracy: 0.9985\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0107 - accuracy: 0.9969\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0079 - accuracy: 0.9974\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8560072b10>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = mlp2.evaluate(testvectors, y_test_encoded)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuxPEgJAQOJR",
        "outputId": "de845c7f-84dc-426d-8d73-430d60fc0c0b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1049 - accuracy: 0.9785\n",
            "Test loss 0.10494855791330338\n",
            "Test accuracy 0.9785000085830688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEccUfBqZ1N1"
      },
      "source": [
        "### Controlando el sobreajuste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9uJRFlqZ1N2"
      },
      "source": [
        "Los métodos de **regularización** pueden ayudar a mejorar el rendimiento de una red neuronal, especialmente si el número de parámetros de la red es muy grande y esto nos lleva a tener sobreajuste. Uno de los métodos de regularización más simples y efectivos es el **dropout**. En Keras, el dropout se utiliza como una capa más, denominada `Dropout`, la cual anula aleatoriamente parte las salidas producidas por la capa anterior, reemplazando sus valores por $0$.\n",
        "\n",
        "Por ejemplo, para crear una red con una capa oculta de un 30% de probabilidad de dropout hacemos lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "YPUpLmfnZ1N3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "sample_network = Sequential()\n",
        "sample_network.add(Dense(512, input_dim=784, activation=\"relu\"))\n",
        "sample_network.add(Dropout(0.3))\n",
        "sample_network.add(Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlmADUIZ1N4"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "   Define una nueva red con 2 capas ocultas, cada una de ellas de 512 unidades ocultas con actuvación ReLU. Tras ambas capas ocultas, añade una capa Dropout del 40%. Para la capa de salida, recuerda utilizar la activación softmax. Compila la red definida, escogiendo \"adam\" como algoritmo de optimización, y entrénala con los datos de entrenamiento. Mide entonces el rendimiento de esta red sobre los datos de test. ¿Ha ayudado el dropout?\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "BopJ9lY-Z1N5",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "mlp3 = Sequential()\n",
        "mlp3.add(Dense(512, input_dim=784, activation=\"relu\"))\n",
        "mlp3.add(Dropout(0.4))\n",
        "mlp3.add(Dense(512, activation=\"relu\"))\n",
        "mlp3.add(Dropout(0.4))\n",
        "mlp3.add(Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "mlp3.fit(\n",
        "    trainvectors, # Training data\n",
        "    y_train_encoded, # Labels of training data\n",
        "    batch_size=128, # Batch size for the optimizer algorithm\n",
        "    epochs=20, # Number of epochs to run the optimizer algorithm\n",
        "    verbose=1 # Level of verbosity of the log messages\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG8m4VzaTId7",
        "outputId": "10278c14-15ef-4149-ed3d-9211c9cf3b51"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.3016 - accuracy: 0.9079\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1326 - accuracy: 0.9587\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1007 - accuracy: 0.9691\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0845 - accuracy: 0.9729\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0736 - accuracy: 0.9767\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0657 - accuracy: 0.9790\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0600 - accuracy: 0.9804\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0541 - accuracy: 0.9823\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0506 - accuracy: 0.9840\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0477 - accuracy: 0.9843\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0421 - accuracy: 0.9864\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0406 - accuracy: 0.9861\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0407 - accuracy: 0.9863\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0409 - accuracy: 0.9865\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0365 - accuracy: 0.9877\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0344 - accuracy: 0.9891\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0327 - accuracy: 0.9893\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0322 - accuracy: 0.9893\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0318 - accuracy: 0.9899\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0312 - accuracy: 0.9894\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84ea86af10>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = mlp3.evaluate(testvectors, y_test_encoded)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHU7ouW9TOrI",
        "outputId": "68a9beff-0a9c-4db4-e086-4499ee52efa5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0643 - accuracy: 0.9834\n",
            "Test loss 0.0643109679222107\n",
            "Test accuracy 0.9833999872207642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12piKIJkZ1N5"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "    Intenta crear una red con más capas ocultas. ¿Consigues mejorar el rendimiento en test de esta manera?\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elwCfQ5zZ1N5",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "ESHcARZlZ1Nz",
        "Na2EgqadZ1N0",
        "NEccUfBqZ1N1"
      ],
      "name": "deeplearning_mnist_dense_student.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}