{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX8zKrS-hRJk"
      },
      "source": [
        "# Práctica: Los Juegos del Hambre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XysGzsSAmBvu"
      },
      "source": [
        "ESTUDIANTE: Javier Herraez, Pablo Peman\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vanYztAMhRJt"
      },
      "source": [
        "<table><tr>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/breakfast.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/hamburger.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/fruits.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "</tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E7BW750hRJu"
      },
      "source": [
        "En esta práctica vamos a enfrentarnos a un problema desafiante de clasificación de imágenes, construyendo una red neuronal profunda que sea capaz de clasificar entre diferentes tipos de comida. ¡Que comiencen los Juegos del Hambre!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnn_FAm8hRJv"
      },
      "source": [
        "## Guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspUM6n3hRJw"
      },
      "source": [
        "A lo largo del notebook encontrarás celdas que debes rellenar con tu propio código. Sigue las instrucciones del notebook y presta atención a los siguientes iconos:\n",
        "\n",
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Deberás resolver el ejercicio escribiendo tu propio código o respuesta en la celda inmediatamente inferior.</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZHQpQXrhRJw"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "Esto es una pista u observación de utilidad que puede ayudarte a resolver el ejercicio. Presta atención a estas pistas para comprender el ejercicio en mayor profundidad.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghhJf_HhRJx"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "Este es un ejercicio avanzado que te puede ayudar a profundizar en el tema, y a conseguir una calificación más alta. ¡Buena suerte!</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWAbqrofhRJy"
      },
      "source": [
        "Para evitar problemas con imports o incompatibilidades se recomienda ejecutar este notebook en uno de los [entornos de Deep Learning recomendados](https://github.com/albarji/teaching-environments-deeplearning), o hacer uso [Google Colaboratory](https://colab.research.google.com/). Si usas Colaboratory, asegúrate de [conectar una GPU](https://colab.research.google.com/notebooks/gpu.ipynb), y de haber [deactivado otras sesiones que tuvieras activas](https://stackoverflow.com/a/53441194/2436578)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZmf8R9DhRJy"
      },
      "source": [
        "El siguiente código mostrará todas las gráficas en el propio notebook en lugar de generar una nueva ventana."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxmdgWqNhRJz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UgDu1WUhRJ0"
      },
      "source": [
        "Finalmente, si necesitas ayuda en el uso de cualquier función Python, coloca el cursor sobre su nombre y presiona Shift+Tab. Aparecerá una ventana con su documentación. Esto solo funciona dentro de celdas de código.\n",
        "\n",
        "¡Vamos alla!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K733uf0ghRJ2"
      },
      "source": [
        "## Obtención de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZYPquwFhRJ2"
      },
      "source": [
        "Vamos a usar un dataset de imágenes de comida disponible en [Kaggle](https://www.kaggle.com/trolukovich/food11-image-dataset). Para descargarlo, necesitarás crear una cuenta de usuario en Kaggle, y obtener tus credenciales de la API. Puedes hacerlo siguiendo las instrucciones de [esta sección](https://github.com/Kaggle/kaggle-api#api-credentials). ¡Ojo! Tus credenciales de la API no son lo mismo que la contraseña que utilizas para acceder a tu cuenta en Kaggle.\n",
        "\n",
        "Una vez tengas el fichero JSON con tus credenciales, puedes declararlas en este notebook asignando las variables de entorno adecuadas, de la siguiente manera\n",
        "\n",
        "    import os\n",
        "\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = \"YOUR KAGGLE USERNAME HERE\"\n",
        "    os.environ[\"KAGGLE_KEY\"] = \"YOUR KAGGLE KEY HERE\"\n",
        "    \n",
        "Cuando lo hayas hecho, podrás descargar el dataset a la máquina donde esté corriendo este notebook usando el siguiente comando\n",
        "\n",
        "    !kaggle datasets download trolukovich/food11-image-dataset --unzip -p YOUR_LOCAL_FOLDER\n",
        "    \n",
        "donde debes indicar el nombre de un directorio válido como \"YOUR_LOCAL_FOLDER\". Si prefieres descargar los datos en la misma carpeta que este notebook, puedes quitar la parte `-p YOUR_LOCAL_FOLDER` del comando."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRX8BQsZhRJ2"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Crea tu propia cuenta de Kaggle (si no tienes ya una), obtén tus credenciales, y usa la celda inferior para declarar tu nombre de usuario de Kaggle y tu clave como variables de entorno. A continuación, usa la misma celda para descargar el dataset de imágenes.\n",
        "    \n",
        "¡Ojo! Debes mantener estas credenciales en secreto, ya que son personales a tu usuario de Kaggle. Recuerda borrarlas de la celda antes de entregar este notebook.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiFYSULfhRJ3",
        "outputId": "f9c16a79-47d3-4557-90da-23c5141b0d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading food11-image-dataset.zip to /content\n",
            "100% 1.08G/1.08G [00:05<00:00, 233MB/s]\n",
            "100% 1.08G/1.08G [00:05<00:00, 215MB/s]\n"
          ]
        }
      ],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"pablopeman\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"86a13bd99e3707adb88f667956d2414f\"\n",
        "\n",
        "!kaggle datasets download trolukovich/food11-image-dataset --unzip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmh27gIEhRJ4"
      },
      "source": [
        "Revisa ahora la carpeta en la que has descargado los datos. Verás que contiene 3 subdirectorios:\n",
        "\n",
        "* **training**, contiene las imágenes a utilizar para entrenar el modelo.\n",
        "* **validation**, contiene imágenes adicionales que podrías usar como datos de entrenamiento adicionales, o para algún tipo de estrategia de validación como Early Stopping.\n",
        "* **evaluation**, contiene las imágenes que debes utilizar para testear el modelo. Las imágenes de esta carpeta **solo** pueden utilizarse para medir el rendimiento del modelo tras su entrenamiento, y para nada más.\n",
        "\n",
        "Además de esto, dentro de cada una de estas carpetas encontrarás una subcarpeta para cada una de las 11 clases de comida:\n",
        "\n",
        "* Bread (panes)\n",
        "* Dairy product (lácteos)\n",
        "* Dessert (postres)\n",
        "* Egg (huevos)\n",
        "* Fried food (fritos)\n",
        "* Meat (carnes)\n",
        "* Noodles-Pasta (pasta)\n",
        "* Rice (arroz)\n",
        "* Seafood (pescado y marisco)\n",
        "* Soup (sopas)\n",
        "* Vegetable-Fruit (vegetales y frutas)\n",
        "\n",
        "Esta es una forma estándar de organizar los datasets de imágenes: una carpeta para cada clase. Para facilitar los pasos de procesamiento que vendrán a continuación, vamos a definir algunas variables que nos indiquen dónde están almacenados los diferentes conjuntos de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJrE8F8zhRJ4"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Crea variables <b>TRAINDIR</b>, <b>VALDIR</b> y <b>TESTDIR</b>, cada una conteniendo una cadena de texto con la ruta al directorio donde están los datos de entrenamiento, validación y evaluación, respectivamente.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzK2VI3ShRJ5"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "TRAINDIR = \"/content/training\"\n",
        "VALDIR= \"/content/validation\"\n",
        "TESTDIR= \"/content/evaluation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikYtQyR-hRJ6"
      },
      "source": [
        "### Reducción de clases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWsLot-7hRJ6"
      },
      "source": [
        "Con el fin de hacer este problema más accesible de cara a la práctica, vamos a centrarnos solo en seis de las clases de comida disponibles: `Bread`, `Dairy product`, `Dessert`, `Egg`, `Fried food` y `Meat`. Para ello, se provee el código siguiente, que elimina de los datos descargados las carpetas correspondientes a imágenes de las otras clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXoNw1-thRJ7",
        "outputId": "7750e635-2a51-4849-d8d2-0d9cb99a7020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting /content/training/Soup...\n",
            "Deleting /content/training/Seafood...\n",
            "Deleting /content/training/Vegetable-Fruit...\n",
            "Deleting /content/training/Noodles-Pasta...\n",
            "Deleting /content/training/Rice...\n",
            "Deleting /content/validation/Soup...\n",
            "Deleting /content/validation/Seafood...\n",
            "Deleting /content/validation/Vegetable-Fruit...\n",
            "Deleting /content/validation/Noodles-Pasta...\n",
            "Deleting /content/validation/Rice...\n",
            "Deleting /content/evaluation/Soup...\n",
            "Deleting /content/evaluation/Seafood...\n",
            "Deleting /content/evaluation/Vegetable-Fruit...\n",
            "Deleting /content/evaluation/Noodles-Pasta...\n",
            "Deleting /content/evaluation/Rice...\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "\n",
        "valid_classes = {\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \"Meat\"}\n",
        "datasets = {TRAINDIR, VALDIR, TESTDIR}\n",
        "\n",
        "for dataset in datasets:\n",
        "    for classdir in glob(f\"{dataset}/*\"):  # Find subfolders with classes\n",
        "        if classdir.split(\"/\")[-1] not in valid_classes:  # Ignore those in valid_classes\n",
        "            print(f\"Deleting {classdir}...\")\n",
        "            for fname in glob(f\"{classdir}/*.jpg\"):  # Remove each image file\n",
        "                os.remove(fname)\n",
        "            os.rmdir(classdir)  # Remove folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fncCfOYihRJ7"
      },
      "source": [
        "## Procesando imágenes desde ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wTWrPr-hRJ7"
      },
      "source": [
        "Este dataset de imágenes es grande, con imágenes de mayor resolución que las que hemos utilizado en el tutorial del MNIST, y cada una de ellas teniendo diferentes tamaños y relación de aspecto. Además, mientras que para el MNIST teníamos una función de keras que preparaba los datos para nosotros, en esta ocasión tendremos que realizar el trabajo de carga y procesamiento de las imágenes.\n",
        "\n",
        "Una forma conveniente de hacer todo este trabajo es a través de la función Keras `image_dataset_from_directory`. Esta función crea un objeto `Dataset` de TensorFlow con todas las imágenes de un directorio, cargándolas en memoria de forma dinámica solo cuando la red neuronal necesita utilizarlas. Esta función también nos permite especificar algunas opciones de preprocesamiento muy útiles.\n",
        "\n",
        "Por ejemplo, podemos crear un `Dataset` con los datos en la carpeta de training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-edpP7-NhRJ8",
        "outputId": "643b47dc-be39-4ba0-cef8-e1b8bc6d0ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "image_size = 32\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    TRAINDIR, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = 'categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGEdEuF5hRJ8"
      },
      "source": [
        "Observa los parámetros que se han utilizado para configurar el dataset:\n",
        "\n",
        "* El **directorio** desde el que cargar las imágenes.\n",
        "* Un **tamaño de imagen (image_size)** que se utilizará para redimensionar todas las imágenes cargadas a ese tamaño común, en este caso 32x32 píxeles.\n",
        "* El **tamaños de los lotes (batch_size)** de imágenes a ser generados. Nótese que definimos aquí este parametro en lugar de en el paso `fit` de la red, como hemos hecho en otros ejercicios, porque el objeto `Dataset` resultante hará uso de esta información para mantener en memoria solo algunos batches de imágenes, ahorrando así memoria.\n",
        "* El **modo de etiquetado (label_mode)**, esto es, la codificación de las etiquetas a utilizar. `categorical` significa que utilizaremos la ya conocida codificación one-hot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgeGjQDVhRJ9"
      },
      "source": [
        "Un objeto `Dataset` funciona de manera muy similar a un generador de Python, lo que significa que podemos iterar sobre él para obtener batches de imágenes ya preprocesadas. Por ejemplo, el siguiente código inicia un bucle para extraer todos los batches del `Dataset`, nos muestra el contenido el primero, y detiene la iteración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPAIrHu7hRJ9",
        "outputId": "000dbd7f-c203-47c4-a00c-d168b0749585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input batch: (64, 32, 32, 3)\n",
            "Shape of output batch: (64, 6)\n",
            "Input batch:\n",
            "[[[[5.05000000e+01 6.45000000e+01 3.50000000e+01]\n",
            "   [1.18750000e+02 9.12500000e+01 3.70000000e+01]\n",
            "   [1.59750000e+02 1.29750000e+02 7.45000000e+01]\n",
            "   ...\n",
            "   [7.70000000e+01 5.35000000e+01 3.30000000e+01]\n",
            "   [6.95000000e+01 4.75000000e+01 2.50000000e+01]\n",
            "   [6.75000000e+01 4.10000000e+01 2.30000000e+01]]\n",
            "\n",
            "  [[4.70000000e+01 4.15000000e+01 1.40000000e+01]\n",
            "   [8.77500000e+01 7.37500000e+01 3.22500000e+01]\n",
            "   [1.36000000e+02 1.11750000e+02 6.47500000e+01]\n",
            "   ...\n",
            "   [6.80000000e+01 4.60000000e+01 2.27500000e+01]\n",
            "   [6.37500000e+01 4.30000000e+01 1.77500000e+01]\n",
            "   [5.92500000e+01 3.85000000e+01 2.05000000e+01]]\n",
            "\n",
            "  [[3.35000000e+01 3.47500000e+01 1.60000000e+01]\n",
            "   [6.42500000e+01 5.62500000e+01 2.62500000e+01]\n",
            "   [1.19750000e+02 9.62500000e+01 5.22500000e+01]\n",
            "   ...\n",
            "   [6.65000000e+01 4.30000000e+01 1.85000000e+01]\n",
            "   [6.62500000e+01 4.75000000e+01 2.27500000e+01]\n",
            "   [1.17250000e+02 9.40000000e+01 5.27500000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[8.00000000e+01 1.24250000e+02 1.27250000e+02]\n",
            "   [9.70000000e+01 1.38500000e+02 1.42250000e+02]\n",
            "   [8.25000000e+01 1.25000000e+02 1.31250000e+02]\n",
            "   ...\n",
            "   [4.65000000e+01 3.90000000e+01 2.45000000e+01]\n",
            "   [3.85000000e+01 3.55000000e+01 1.70000000e+01]\n",
            "   [5.10000000e+01 4.82500000e+01 2.75000000e+01]]\n",
            "\n",
            "  [[1.53500000e+02 1.67250000e+02 1.69750000e+02]\n",
            "   [4.50000000e+01 2.15000000e+01 1.25000000e+01]\n",
            "   [4.22500000e+01 9.75000000e+00 7.25000000e+00]\n",
            "   ...\n",
            "   [4.42500000e+01 4.22500000e+01 2.72500000e+01]\n",
            "   [4.87500000e+01 4.27500000e+01 2.42500000e+01]\n",
            "   [5.42500000e+01 5.17500000e+01 3.95000000e+01]]\n",
            "\n",
            "  [[3.25000000e+01 8.50000000e+00 2.00000000e+00]\n",
            "   [4.27500000e+01 1.45000000e+01 9.25000000e+00]\n",
            "   [4.80000000e+01 1.15000000e+01 8.00000000e+00]\n",
            "   ...\n",
            "   [5.55000000e+01 5.22500000e+01 3.25000000e+01]\n",
            "   [6.20000000e+01 6.15000000e+01 4.50000000e+01]\n",
            "   [6.52500000e+01 6.30000000e+01 4.20000000e+01]]]\n",
            "\n",
            "\n",
            " [[[1.02500000e+01 1.12500000e+01 5.75000000e+00]\n",
            "   [8.25000000e+00 9.25000000e+00 4.25000000e+00]\n",
            "   [1.10000000e+01 1.00000000e+01 5.00000000e+00]\n",
            "   ...\n",
            "   [1.02500000e+01 1.02500000e+01 2.25000000e+00]\n",
            "   [1.17500000e+01 1.17500000e+01 3.75000000e+00]\n",
            "   [9.50000000e+00 8.50000000e+00 3.50000000e+00]]\n",
            "\n",
            "  [[9.75000000e+00 1.07500000e+01 5.75000000e+00]\n",
            "   [1.50000000e+01 1.40000000e+01 1.00000000e+01]\n",
            "   [9.25000000e+00 1.02500000e+01 5.25000000e+00]\n",
            "   ...\n",
            "   [1.27500000e+01 9.75000000e+00 2.75000000e+00]\n",
            "   [1.12500000e+01 9.25000000e+00 3.25000000e+00]\n",
            "   [1.07500000e+01 9.75000000e+00 4.75000000e+00]]\n",
            "\n",
            "  [[3.47500000e+01 3.57500000e+01 2.97500000e+01]\n",
            "   [1.25000000e+01 1.15000000e+01 6.50000000e+00]\n",
            "   [1.30000000e+01 1.20000000e+01 7.00000000e+00]\n",
            "   ...\n",
            "   [1.37500000e+01 1.07500000e+01 4.75000000e+00]\n",
            "   [1.27500000e+01 1.12500000e+01 3.75000000e+00]\n",
            "   [1.25000000e+01 1.17500000e+01 4.00000000e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.31750000e+02 2.40750000e+02 2.39750000e+02]\n",
            "   [2.34750000e+02 2.43750000e+02 2.42750000e+02]\n",
            "   [2.39500000e+02 2.47000000e+02 2.46500000e+02]\n",
            "   ...\n",
            "   [2.30250000e+02 2.41750000e+02 2.35750000e+02]\n",
            "   [4.47500000e+01 5.50000000e+01 3.57500000e+01]\n",
            "   [9.42500000e+01 1.26500000e+02 1.65250000e+02]]\n",
            "\n",
            "  [[1.98000000e+02 2.08500000e+02 2.10250000e+02]\n",
            "   [2.38750000e+02 2.47750000e+02 2.42750000e+02]\n",
            "   [2.37000000e+02 2.46000000e+02 2.43000000e+02]\n",
            "   ...\n",
            "   [2.26500000e+02 2.20250000e+02 3.37500000e+01]\n",
            "   [6.75000000e+01 8.15000000e+01 4.67500000e+01]\n",
            "   [2.03250000e+02 1.98500000e+02 1.80000000e+01]]\n",
            "\n",
            "  [[1.72250000e+02 1.62500000e+02 1.46750000e+02]\n",
            "   [1.40750000e+02 1.57250000e+02 1.54500000e+02]\n",
            "   [2.35000000e+02 2.44500000e+02 2.41250000e+02]\n",
            "   ...\n",
            "   [1.52750000e+02 1.53750000e+02 3.32500000e+01]\n",
            "   [2.17000000e+02 2.03500000e+02 8.50000000e+00]\n",
            "   [2.07500000e+02 1.93500000e+02 4.25000000e+00]]]\n",
            "\n",
            "\n",
            " [[[9.52500000e+01 6.52500000e+01 3.52500000e+01]\n",
            "   [1.50500000e+02 1.26000000e+02 8.92500000e+01]\n",
            "   [1.10750000e+02 7.75000000e+01 4.97500000e+01]\n",
            "   ...\n",
            "   [1.10250000e+02 3.22500000e+01 2.00000000e+01]\n",
            "   [1.22250000e+02 4.02500000e+01 2.77500000e+01]\n",
            "   [1.10250000e+02 3.32500000e+01 2.15000000e+01]]\n",
            "\n",
            "  [[1.28250000e+02 1.02500000e+02 6.52500000e+01]\n",
            "   [1.52750000e+02 1.30750000e+02 9.27500000e+01]\n",
            "   [1.24500000e+02 9.95000000e+01 7.67500000e+01]\n",
            "   ...\n",
            "   [1.09750000e+02 5.90000000e+01 3.42500000e+01]\n",
            "   [1.17000000e+02 3.80000000e+01 2.57500000e+01]\n",
            "   [1.28250000e+02 5.22500000e+01 3.62500000e+01]]\n",
            "\n",
            "  [[2.42500000e+02 2.13500000e+02 1.69000000e+02]\n",
            "   [1.44500000e+02 1.22000000e+02 8.65000000e+01]\n",
            "   [1.23000000e+02 1.02250000e+02 6.90000000e+01]\n",
            "   ...\n",
            "   [1.12250000e+02 3.42500000e+01 2.10000000e+01]\n",
            "   [1.12000000e+02 3.30000000e+01 2.25000000e+01]\n",
            "   [1.24250000e+02 2.97500000e+01 3.02500000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.90500000e+02 1.35000000e+02 7.70000000e+01]\n",
            "   [2.27250000e+02 2.03250000e+02 1.68250000e+02]\n",
            "   [2.41000000e+02 2.22250000e+02 1.90250000e+02]\n",
            "   ...\n",
            "   [1.85000000e+02 1.26000000e+02 7.60000000e+01]\n",
            "   [1.74000000e+02 1.20000000e+02 6.50000000e+01]\n",
            "   [1.74500000e+02 1.24500000e+02 7.50000000e+01]]\n",
            "\n",
            "  [[2.21500000e+02 1.66750000e+02 1.02250000e+02]\n",
            "   [1.95750000e+02 1.43500000e+02 8.57500000e+01]\n",
            "   [2.25000000e+02 1.99000000e+02 1.64000000e+02]\n",
            "   ...\n",
            "   [1.68750000e+02 1.11500000e+02 5.57500000e+01]\n",
            "   [1.66750000e+02 1.12000000e+02 5.92500000e+01]\n",
            "   [1.68000000e+02 1.17500000e+02 6.55000000e+01]]\n",
            "\n",
            "  [[2.23250000e+02 1.84500000e+02 1.26500000e+02]\n",
            "   [2.12000000e+02 1.62000000e+02 1.06000000e+02]\n",
            "   [1.97250000e+02 1.60000000e+02 1.12250000e+02]\n",
            "   ...\n",
            "   [1.69750000e+02 1.04750000e+02 4.92500000e+01]\n",
            "   [1.73500000e+02 1.16750000e+02 5.75000000e+01]\n",
            "   [1.71250000e+02 1.19750000e+02 7.22500000e+01]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[1.32718750e+02 1.14359375e+02 8.71562500e+01]\n",
            "   [1.38000000e+02 1.15000000e+02 9.13437500e+01]\n",
            "   [1.35500000e+02 1.16500000e+02 9.35000000e+01]\n",
            "   ...\n",
            "   [1.36703125e+02 1.39703125e+02 1.25796875e+02]\n",
            "   [1.38343750e+02 1.51343750e+02 1.32343750e+02]\n",
            "   [1.31875000e+02 1.41875000e+02 1.23875000e+02]]\n",
            "\n",
            "  [[1.04984375e+02 8.49843750e+01 5.54843750e+01]\n",
            "   [1.48375000e+02 1.33140625e+02 1.10906250e+02]\n",
            "   [1.39796875e+02 1.27390625e+02 1.02390625e+02]\n",
            "   ...\n",
            "   [1.82875000e+02 1.06578125e+02 7.09375000e+00]\n",
            "   [1.78234375e+02 1.01500000e+02 1.92187500e+00]\n",
            "   [1.91437500e+02 1.10937500e+02 1.04375000e+01]]\n",
            "\n",
            "  [[1.59218750e+01 1.39218750e+01 2.81250000e-01]\n",
            "   [3.26718750e+01 3.75156250e+01 1.51718750e+01]\n",
            "   [8.29531250e+01 8.14531250e+01 5.94531250e+01]\n",
            "   ...\n",
            "   [2.17390625e+02 1.40390625e+02 2.53906250e+01]\n",
            "   [2.26343750e+02 1.46343750e+02 2.78437500e+01]\n",
            "   [2.10359375e+02 1.30359375e+02 1.18593750e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.48078125e+02 8.97968750e+01 1.64375000e+01]\n",
            "   [1.57343750e+02 9.93437500e+01 3.18437500e+01]\n",
            "   [1.63718750e+02 1.04921875e+02 3.43125000e+01]\n",
            "   ...\n",
            "   [4.93281250e+01 6.73281250e+01 5.53281250e+01]\n",
            "   [4.35312500e+01 5.95312500e+01 4.85312500e+01]\n",
            "   [4.31406250e+01 5.91406250e+01 4.61406250e+01]]\n",
            "\n",
            "  [[3.51562500e+01 2.86562500e+01 3.65625000e+00]\n",
            "   [5.16875000e+01 3.32656250e+01 2.54687500e+00]\n",
            "   [1.59125000e+02 1.05500000e+02 3.33906250e+01]\n",
            "   ...\n",
            "   [1.12000000e+02 1.26109375e+02 1.21406250e+02]\n",
            "   [6.68906250e+01 8.21250000e+01 7.46562500e+01]\n",
            "   [4.34218750e+01 6.54218750e+01 5.29218750e+01]]\n",
            "\n",
            "  [[3.62656250e+01 3.02656250e+01 8.26562500e+00]\n",
            "   [2.68437500e+01 2.28437500e+01 0.00000000e+00]\n",
            "   [3.39062500e+01 2.47968750e+01 0.00000000e+00]\n",
            "   ...\n",
            "   [6.06406250e+01 8.54218750e+01 7.54218750e+01]\n",
            "   [6.81093750e+01 9.33750000e+01 8.61093750e+01]\n",
            "   [1.05828125e+02 1.25828125e+02 1.16828125e+02]]]\n",
            "\n",
            "\n",
            " [[[8.47500000e+01 8.02500000e+01 7.72500000e+01]\n",
            "   [8.77500000e+01 8.17500000e+01 8.17500000e+01]\n",
            "   [8.65000000e+01 8.25000000e+01 8.15000000e+01]\n",
            "   ...\n",
            "   [2.92500000e+01 1.62500000e+01 7.25000000e+00]\n",
            "   [2.75000000e+01 1.45000000e+01 5.50000000e+00]\n",
            "   [9.75000000e+00 6.75000000e+00 0.00000000e+00]]\n",
            "\n",
            "  [[8.85000000e+01 8.45000000e+01 8.15000000e+01]\n",
            "   [9.07500000e+01 8.97500000e+01 8.77500000e+01]\n",
            "   [9.07500000e+01 9.00000000e+01 8.80000000e+01]\n",
            "   ...\n",
            "   [2.95000000e+01 1.60000000e+01 6.00000000e+00]\n",
            "   [2.82500000e+01 1.52500000e+01 6.50000000e+00]\n",
            "   [1.25000000e+01 8.75000000e+00 7.50000000e-01]]\n",
            "\n",
            "  [[7.65000000e+01 7.25000000e+01 6.70000000e+01]\n",
            "   [7.80000000e+01 7.40000000e+01 6.95000000e+01]\n",
            "   [9.12500000e+01 8.92500000e+01 8.37500000e+01]\n",
            "   ...\n",
            "   [2.65000000e+01 1.35000000e+01 4.50000000e+00]\n",
            "   [2.55000000e+01 1.55000000e+01 6.50000000e+00]\n",
            "   [1.65000000e+01 8.50000000e+00 5.00000000e-01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.82500000e+02 1.92500000e+02 1.91500000e+02]\n",
            "   [1.81750000e+02 1.93750000e+02 1.91750000e+02]\n",
            "   [1.81500000e+02 1.97500000e+02 1.94500000e+02]\n",
            "   ...\n",
            "   [1.92750000e+02 1.99250000e+02 2.01750000e+02]\n",
            "   [1.90000000e+02 1.95000000e+02 1.98000000e+02]\n",
            "   [1.81000000e+02 1.86000000e+02 1.89000000e+02]]\n",
            "\n",
            "  [[1.83750000e+02 1.94750000e+02 1.93250000e+02]\n",
            "   [1.76750000e+02 1.92750000e+02 1.91750000e+02]\n",
            "   [1.82500000e+02 1.97750000e+02 1.94750000e+02]\n",
            "   ...\n",
            "   [1.90000000e+02 1.98000000e+02 2.00000000e+02]\n",
            "   [1.88000000e+02 1.98000000e+02 1.98000000e+02]\n",
            "   [1.88500000e+02 1.93500000e+02 1.97500000e+02]]\n",
            "\n",
            "  [[1.79750000e+02 1.94250000e+02 1.92750000e+02]\n",
            "   [1.79750000e+02 1.95000000e+02 1.93250000e+02]\n",
            "   [1.84000000e+02 1.99250000e+02 1.97500000e+02]\n",
            "   ...\n",
            "   [1.95500000e+02 1.99500000e+02 2.00500000e+02]\n",
            "   [1.89250000e+02 1.95000000e+02 1.95750000e+02]\n",
            "   [1.88000000e+02 1.92000000e+02 1.95000000e+02]]]\n",
            "\n",
            "\n",
            " [[[5.15000000e+01 5.45000000e+01 2.35000000e+01]\n",
            "   [5.92500000e+01 6.32500000e+01 2.62500000e+01]\n",
            "   [7.52500000e+01 8.17500000e+01 3.22500000e+01]\n",
            "   ...\n",
            "   [1.55000000e+01 1.75000000e+01 3.50000000e+00]\n",
            "   [0.00000000e+00 3.25000000e+00 0.00000000e+00]\n",
            "   [0.00000000e+00 3.75000000e+00 0.00000000e+00]]\n",
            "\n",
            "  [[6.90000000e+01 7.10000000e+01 3.30000000e+01]\n",
            "   [8.17500000e+01 8.47500000e+01 3.47500000e+01]\n",
            "   [9.82500000e+01 9.92500000e+01 4.17500000e+01]\n",
            "   ...\n",
            "   [5.75000000e+00 7.75000000e+00 3.25000000e+00]\n",
            "   [1.10000000e+01 1.12500000e+01 2.50000000e+00]\n",
            "   [2.22500000e+01 2.35000000e+01 1.20000000e+01]]\n",
            "\n",
            "  [[1.06750000e+02 1.07750000e+02 5.07500000e+01]\n",
            "   [9.62500000e+01 1.00500000e+02 3.67500000e+01]\n",
            "   [7.67500000e+01 7.97500000e+01 3.27500000e+01]\n",
            "   ...\n",
            "   [1.25000000e+00 6.25000000e+00 2.50000000e-01]\n",
            "   [1.27500000e+01 1.57500000e+01 7.75000000e+00]\n",
            "   [0.00000000e+00 4.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.23500000e+02 2.23750000e+02 1.34750000e+02]\n",
            "   [2.09250000e+02 1.79500000e+02 7.32500000e+01]\n",
            "   [2.53000000e+02 2.53500000e+02 2.24000000e+02]\n",
            "   ...\n",
            "   [1.64000000e+02 1.83000000e+02 1.17000000e+02]\n",
            "   [1.49000000e+02 1.65000000e+02 9.85000000e+01]\n",
            "   [1.32750000e+02 1.43000000e+02 8.02500000e+01]]\n",
            "\n",
            "  [[2.28250000e+02 2.35250000e+02 1.67000000e+02]\n",
            "   [2.47500000e+02 2.54250000e+02 1.98250000e+02]\n",
            "   [2.50500000e+02 2.53500000e+02 1.99500000e+02]\n",
            "   ...\n",
            "   [1.68000000e+02 1.83000000e+02 1.16000000e+02]\n",
            "   [1.55500000e+02 1.70500000e+02 1.05000000e+02]\n",
            "   [1.39500000e+02 1.54250000e+02 9.12500000e+01]]\n",
            "\n",
            "  [[2.18000000e+02 2.29000000e+02 1.60750000e+02]\n",
            "   [2.26500000e+02 2.34500000e+02 1.67500000e+02]\n",
            "   [2.11000000e+02 2.09000000e+02 1.14500000e+02]\n",
            "   ...\n",
            "   [1.77500000e+02 1.85000000e+02 1.13500000e+02]\n",
            "   [1.54000000e+02 1.65500000e+02 9.95000000e+01]\n",
            "   [1.20250000e+02 1.30000000e+02 6.70000000e+01]]]]\n",
            "Output batch:\n",
            "[[0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "for X_batch, y_batch in train_dataset:\n",
        "    print(f\"Shape of input batch: {X_batch.shape}\")\n",
        "    print(f\"Shape of output batch: {y_batch.shape}\")\n",
        "    print(f\"Input batch:\\n{X_batch}\")\n",
        "    print(f\"Output batch:\\n{y_batch}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2UaPCEthRJ-"
      },
      "source": [
        "Podemos ver que, efectivamente, el generador produce un tensor de datos de entrada de las dimensiones apropiadas para poder introducirlo en la red neuronal, y que las salidas también se han codificado correctamente como one-hot.\n",
        "No obstante, todavía hay un problema con los datos: los valores de los píxeles están en el rango [0, 255], lo cual puede producir problemas de entrenamiento. Resolveremos este punto después, en la definición de la red neuronal, mediante una capa especial. Por ahora vamos a cotinuar, definiendo una función que construya los `Dataset` para los datos de entrenamiento, validación y test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTQyBbf-hRJ-"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Crea una función <b>create_datasets</b> que reciba los siguiente parámetros:\n",
        "    <ul>\n",
        "      <li><b>traindir</b>: el directorio donde están localizadas las imágenes de entrenamiento.</li>\n",
        "      <li><b>valdir</b>: el directorio donde están localizadas las imágenes de validación.</li>\n",
        "      <li><b>testdir</b>: el directorio donde están localizadas las imágenes de test.</li>\n",
        "      <li><b>image_size</b>: el tamaño que se utilizará para redimensionar todas las imágenes a una resolución común.</li>\n",
        "      <li><b>batch_size</b>: el tamaño de los batches de imágenes que serán generados.</li>\n",
        "    </ul>\n",
        "    La función debe crear objetos `Dataset` para los directorios de entrenamiento, validación y test, y devolver los tres datasets creados como\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-wm19eJhRJ-"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "\n",
        "def create_datasets (traindir,valdir,testdir,image_size,batch_size):\n",
        "  train_dataset=image_dataset_from_directory(\n",
        "    traindir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = 'categorical'\n",
        "    )\n",
        "  val_dataset=image_dataset_from_directory(\n",
        "    valdir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = 'categorical'\n",
        "    )\n",
        "  test_dataset=image_dataset_from_directory(\n",
        "    testdir, \n",
        "    image_size = (image_size, image_size),\n",
        "    batch_size = batch_size, \n",
        "    label_mode = 'categorical'\n",
        "    )\n",
        "  return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wGaULQsjwJz"
      },
      "source": [
        "Probemos que la función que has implementado funciona correctamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtZlNbHhiOIR",
        "outputId": "d4dcd3b5-ad7f-4624-c30e-a345831612d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=32, batch_size=64)\n",
        "\n",
        "# Test whether all returned objects are valid Tensorflow datasets\n",
        "assert isinstance(train_dataset, tf.data.Dataset)\n",
        "assert isinstance(val_dataset, tf.data.Dataset)\n",
        "assert isinstance(test_dataset, tf.data.Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLIo6LBfhRJ-"
      },
      "source": [
        "¡Ahora que tenemos nuestros `Dataset` podemos entrenar una red profunda con ellos! Como ejemplo, vamos a construir una red convolucional extremadamente simple. Nótese cómo hemos añadido una capa especial de preprocesado llamada `Rescaling`, que será la encargada de normalizar los valores de los píxeles al rango [0, 1] cada vez que la red reciba una imagen.\n",
        "\n",
        "¡Ojo! Esta red tan simple no producirá errores al ejecutar, pero tiene algunos fallos de diseño que deberás corregir cuando crees tu propia red, más adelante en esta práctica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbwmYJiLhRJ_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, Convolution2D, Rescaling\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
        "model.add(Convolution2D(4, 3, activation='linear'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v9OxA-AhRJ_"
      },
      "source": [
        "El método `fit` de un modelo Keras puede recibir un objeto `Dataset` como datos de entrenamiento, en lugar de un par de tensores (entradas, salidas). Como al construir los `Dataset` ya especificamos el tamaño de batch, no es necesario indicarlo ahora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZXjs6CghRKA",
        "outputId": "1f95d287-5dc4-4e82-9a73-cce2b0465bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96/96 [==============================] - 13s 110ms/step - loss: 1.6872 - accuracy: 0.2738\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f17a1c2a350>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.fit(train_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ux64pchRKA"
      },
      "source": [
        "Análogamente, podemos evaluar el rendimiento de nuestro modelo sobre el `Dataset` de test de la siguiente manera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_kQhkY-hRKA",
        "outputId": "550f556c-8fee-4dc7-8838-2b284d47dc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 5s 109ms/step - loss: 1.8847 - accuracy: 0.2449\n",
            "Loss 1.88, accuracy 24.5%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXf9OBehRKB"
      },
      "source": [
        "Este nivel de acierto puede parecer pobre, pero ten en cuenta que hemos usado un modelo muy simple y que el problema es de 6 clases. ¿Serás capaz de hacerlo mejor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kZnaNQIhRKB"
      },
      "source": [
        "## Construyendo tu propia red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNd8dC2whRKB"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Diseña una red neuronal profunda que obtenga el mejor acierto posible sobre los datos de test. Puedes usar los datos de entrenamiento y validación como te parezca mejor, pero <b>sólo</b> puedes usar los datos de test para evaluar el acierto de tu modelo. Debes obtener una red capaz de alcanzar al menos un 40% de acierto sobre los datos de test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9C9GNYihRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Algunas recomendaciones y estrategias que pueden ayudar a mejorar tu diseño de red:\n",
        "\n",
        "    \n",
        "- Usa todos los trucos que has aprendido en los ejercicios anteriores: capas convolucionales + pooling, activaciones ReLU, dropout... asegúrate también de utilizar un buen optimizador, con una función de error (loss) adecuada, así como una función de activación en la capa de salida que sea adecuada para esta clase de problema (clasificación multiclase).\n",
        "- Empieza por redes pequeñas, con un número pequeño de parámetros, de forma que puedas comprobar rápidamente qué tal funcionan. Después, puedes hacer tu red más grande en tres direcciones: mayor tamaño de imágenes de entrada, más capas, y más kernels por capa convolucional o unidades por capa densa. Si aumentas el tamaño de las imágenes de entrada, asegúrate de añadir también más capas Convolution+Pooling, para que así a la capa Flatten solo lleguen imágenes muy pequeñas (10x10 píxeles o menos).\n",
        "- Configurar los `Dataset` para que carguen imágenes de mayor tamaño puede mejorar significativamente el rendimiento de tu red. Pero ten cuidado, ¡también puedes encontrarte errores de falta de memoria (CUDA memory error) si cargas imágenes a un tamaño demasiado grande! Para esta práctica, un tamaño mayor a 256 puede ser demasiado grande...\n",
        "- Si observas grandes diferencias de loss entre los datos de entrenamiento y validación o test, prueba a incrementar el nivel de Dropout en las capas Dense.\n",
        "- ¡Usa los datos de validación! Por ejemplo, usa una <a href=\"https://keras.io/api/callbacks/early_stopping/\">**estrategia de EarlyStopping**</a> para monitorizar el loss de los datos de validación, y así detener el entrenamiento cuando tras un número de épocas esa loss no haya decrecido. Configurar la EarlyStopping para restaurar los mejores parámetros encontrados durante la optimización también puede resultarte útil.\n",
        "- Si tu red obtiene resultados muy buenos, del orden del 90% o más de acierto en test... sospecha. Es posible que estés mezclando los datos de entrenamiento y test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssDs0SGghRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "    \n",
        "Como ejercicio avanzado, añade las siguiente estrategias a tu red:\n",
        "\n",
        "- Usa **técnicas de \"image augmentation\"** para aumentar artificialmente tu dataset de entrenamiento. Para ello, explora las <a href=\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/\">capas de augmentation disponibles en Keras</a>.\n",
        "- Usa capas de <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">BatchNormalization</a> para facilitar el entrenamiento de la red.\n",
        "    \n",
        "Usando estos trucos y los mencionados en el punto anterior, es posible obtener más de un 60% de acierto en el conjunto de test.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "samjADDwMTfA",
        "outputId": "2c37c2cf-d297-4d9c-ea6b-ddc2b27e44e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Epoch 1/30\n",
            "96/96 [==============================] - 21s 185ms/step - loss: 1.6144 - accuracy: 0.3234 - val_loss: 1.6033 - val_accuracy: 0.3074\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 18s 185ms/step - loss: 1.5376 - accuracy: 0.3560 - val_loss: 1.4716 - val_accuracy: 0.4146\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 17s 170ms/step - loss: 1.4787 - accuracy: 0.3959 - val_loss: 1.3828 - val_accuracy: 0.4417\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 20s 199ms/step - loss: 1.4426 - accuracy: 0.4101 - val_loss: 1.5166 - val_accuracy: 0.4065\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 18s 182ms/step - loss: 1.4321 - accuracy: 0.4211 - val_loss: 1.3958 - val_accuracy: 0.4426\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 19s 192ms/step - loss: 1.4357 - accuracy: 0.4107 - val_loss: 1.4272 - val_accuracy: 0.4483\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 18s 182ms/step - loss: 1.3745 - accuracy: 0.4489 - val_loss: 1.4249 - val_accuracy: 0.4592\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 19s 192ms/step - loss: 1.3474 - accuracy: 0.4630 - val_loss: 1.2643 - val_accuracy: 0.4891\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 17s 170ms/step - loss: 1.3249 - accuracy: 0.4735 - val_loss: 1.3384 - val_accuracy: 0.5038\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 19s 188ms/step - loss: 1.2950 - accuracy: 0.4898 - val_loss: 1.3161 - val_accuracy: 0.4976\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 17s 174ms/step - loss: 1.3118 - accuracy: 0.4854 - val_loss: 1.2278 - val_accuracy: 0.5304\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 17s 168ms/step - loss: 1.3240 - accuracy: 0.4765 - val_loss: 1.3252 - val_accuracy: 0.5043\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 17s 165ms/step - loss: 1.2729 - accuracy: 0.5015 - val_loss: 1.2890 - val_accuracy: 0.5256\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 16s 163ms/step - loss: 1.2536 - accuracy: 0.5173 - val_loss: 1.3899 - val_accuracy: 0.5019\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 17s 169ms/step - loss: 1.2224 - accuracy: 0.5247 - val_loss: 1.4030 - val_accuracy: 0.4953\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 17s 171ms/step - loss: 1.1944 - accuracy: 0.5393 - val_loss: 1.2873 - val_accuracy: 0.5365\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 17s 165ms/step - loss: 1.2019 - accuracy: 0.5327 - val_loss: 1.2680 - val_accuracy: 0.5299\n",
            "Epoch 18/30\n",
            "96/96 [==============================] - 17s 171ms/step - loss: 1.2359 - accuracy: 0.5207 - val_loss: 1.3492 - val_accuracy: 0.5213\n",
            "Epoch 19/30\n",
            "96/96 [==============================] - 17s 172ms/step - loss: 1.1687 - accuracy: 0.5549 - val_loss: 1.3687 - val_accuracy: 0.5133\n",
            "Epoch 20/30\n",
            "96/96 [==============================] - 17s 168ms/step - loss: 1.1735 - accuracy: 0.5477 - val_loss: 1.2184 - val_accuracy: 0.5764\n",
            "Epoch 21/30\n",
            "96/96 [==============================] - 18s 177ms/step - loss: 1.1395 - accuracy: 0.5682 - val_loss: 1.2895 - val_accuracy: 0.5527\n",
            "Epoch 22/30\n",
            "96/96 [==============================] - 19s 188ms/step - loss: 1.1305 - accuracy: 0.5728 - val_loss: 1.3760 - val_accuracy: 0.5294\n",
            "Epoch 23/30\n",
            "96/96 [==============================] - 18s 184ms/step - loss: 1.1219 - accuracy: 0.5699 - val_loss: 1.3544 - val_accuracy: 0.5474\n",
            "Epoch 24/30\n",
            "96/96 [==============================] - 19s 190ms/step - loss: 1.1257 - accuracy: 0.5743 - val_loss: 1.3975 - val_accuracy: 0.5365\n",
            "Epoch 25/30\n",
            "96/96 [==============================] - 18s 182ms/step - loss: 1.0710 - accuracy: 0.5985 - val_loss: 1.3434 - val_accuracy: 0.5356\n",
            "Epoch 26/30\n",
            "96/96 [==============================] - 17s 174ms/step - loss: 1.0890 - accuracy: 0.5829 - val_loss: 1.3131 - val_accuracy: 0.5574\n",
            "Epoch 27/30\n",
            "96/96 [==============================] - 18s 178ms/step - loss: 1.0577 - accuracy: 0.5914 - val_loss: 1.2446 - val_accuracy: 0.5683\n",
            "Epoch 28/30\n",
            "96/96 [==============================] - 18s 184ms/step - loss: 1.0847 - accuracy: 0.5870 - val_loss: 1.3041 - val_accuracy: 0.5674\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1790473d10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "##### INSERT YOUR CODE HERE\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip,RandomRotation,RandomZoom,RandomTranslation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import BatchNormalization, MaxPooling2D\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=128, batch_size=64)\n",
        "\n",
        "img_rows = 128\n",
        "img_cols = 128\n",
        "\n",
        "\n",
        "kernel_size = 3\n",
        "pool_size = 2 \n",
        "\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy', \n",
        "    patience=8, \n",
        "    min_delta=0.001\n",
        ")\n",
        "\n",
        "model_1 = Sequential([\n",
        "              RandomFlip('horizontal'),\n",
        "              RandomRotation(0.2),\n",
        "              RandomZoom(0.3,0.2),\n",
        "              BatchNormalization()\n",
        "             ])\n",
        "\n",
        "model_1.add(Convolution2D(32, (kernel_size, kernel_size), input_shape=(img_rows, img_cols, 3), activation=\"relu\"))\n",
        "model_1.add(MaxPooling2D(pool_size, strides=2))\n",
        "model_1.add(Convolution2D(32, (kernel_size,kernel_size), activation=\"relu\"))\n",
        "model_1.add(MaxPooling2D(pool_size, strides=2))\n",
        "model_1.add(Convolution2D(32, (kernel_size,kernel_size), activation=\"relu\"))\n",
        "model_1.add(MaxPooling2D(pool_size, strides=2))\n",
        "\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(256, activation=\"relu\"))\n",
        "model_1.add(Dropout(0.5))\n",
        "model_1.add(Dense(32, activation=\"relu\"))\n",
        "model_1.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_1.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    verbose=1 ,\n",
        "    callbacks=[custom_early_stopping],\n",
        "    validation_data=val_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT--iwn4SSVN",
        "outputId": "0078435c-bb4d-433e-ae0e-b2a94776416c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " random_flip (RandomFlip)    (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " random_rotation (RandomRota  (None, 128, 128, 3)      0         \n",
            " tion)                                                           \n",
            "                                                                 \n",
            " random_zoom (RandomZoom)    (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128, 128, 3)      12        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 126, 126, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 61, 61, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 30, 30, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               1605888   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                8224      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 198       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,633,714\n",
            "Trainable params: 1,633,708\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model_1.evaluate(test_dataset)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFKi0oqOW1oV",
        "outputId": "2c5877e2-e6a9-4754-ce5b-997aaf0a6193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 5s 122ms/step - loss: 1.3068 - accuracy: 0.5647\n",
            "Test loss 1.3068479299545288\n",
            "Test accuracy 0.5647342801094055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=128, batch_size=64)\n",
        "\n",
        "img_rows = 128\n",
        "img_cols = 128\n",
        "kernel_size = 3\n",
        "pool_size = 2 \n",
        "\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy', \n",
        "    patience=8, \n",
        "    min_delta=0.001\n",
        ")\n",
        "\n",
        "model_2 = Sequential([\n",
        "              RandomFlip('horizontal'),\n",
        "              RandomRotation(0.2),\n",
        "              RandomZoom(0.3,0.2),\n",
        "              BatchNormalization()\n",
        "             ])\n",
        "\n",
        "model_2.add(Convolution2D(64, (kernel_size, kernel_size), input_shape=(img_rows, img_cols, 3), activation=\"relu\"))\n",
        "model_2.add(MaxPooling2D(pool_size, strides=2))\n",
        "model_2.add(Convolution2D(32, (kernel_size, kernel_size), activation=\"relu\"))\n",
        "model_2.add(MaxPooling2D(pool_size, strides=2))\n",
        "model_2.add(Convolution2D(32, (kernel_size, kernel_size), activation=\"relu\"))\n",
        "model_2.add(MaxPooling2D(pool_size, strides=2))\n",
        "\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(256, activation=\"relu\"))\n",
        "model_2.add(Dropout(0.5))\n",
        "model_2.add(Dense(128, activation=\"relu\"))\n",
        "model_2.add(Dropout(0.5))\n",
        "model_2.add(Dense(32, activation=\"relu\"))\n",
        "model_2.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model_2.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    verbose=1 ,\n",
        "    callbacks=[custom_early_stopping],\n",
        "    validation_data=val_dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orncouZgVfpN",
        "outputId": "1bed66fb-1f7a-42ee-fd13-f733332b079e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Epoch 1/30\n",
            "96/96 [==============================] - 20s 181ms/step - loss: 1.6898 - accuracy: 0.2882 - val_loss: 1.6298 - val_accuracy: 0.3126\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 18s 174ms/step - loss: 1.5850 - accuracy: 0.3298 - val_loss: 1.4783 - val_accuracy: 0.3933\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 18s 175ms/step - loss: 1.5300 - accuracy: 0.3573 - val_loss: 1.4633 - val_accuracy: 0.4023\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 18s 174ms/step - loss: 1.5034 - accuracy: 0.3696 - val_loss: 1.5283 - val_accuracy: 0.3819\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 18s 178ms/step - loss: 1.5019 - accuracy: 0.3752 - val_loss: 1.4223 - val_accuracy: 0.4241\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 18s 180ms/step - loss: 1.4546 - accuracy: 0.4000 - val_loss: 1.3784 - val_accuracy: 0.4369\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 18s 175ms/step - loss: 1.4426 - accuracy: 0.4089 - val_loss: 1.3994 - val_accuracy: 0.4341\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 17s 173ms/step - loss: 1.4278 - accuracy: 0.4260 - val_loss: 1.3720 - val_accuracy: 0.4388\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 18s 174ms/step - loss: 1.4095 - accuracy: 0.4428 - val_loss: 1.3398 - val_accuracy: 0.4649\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 18s 176ms/step - loss: 1.3946 - accuracy: 0.4364 - val_loss: 1.3335 - val_accuracy: 0.4734\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 18s 174ms/step - loss: 1.3753 - accuracy: 0.4546 - val_loss: 1.3753 - val_accuracy: 0.4597\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 18s 178ms/step - loss: 1.3804 - accuracy: 0.4494 - val_loss: 1.3676 - val_accuracy: 0.4687\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 18s 173ms/step - loss: 1.3719 - accuracy: 0.4545 - val_loss: 1.3211 - val_accuracy: 0.4824\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 18s 178ms/step - loss: 1.3664 - accuracy: 0.4581 - val_loss: 1.3040 - val_accuracy: 0.4801\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 18s 180ms/step - loss: 1.3313 - accuracy: 0.4707 - val_loss: 1.4780 - val_accuracy: 0.4279\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 18s 179ms/step - loss: 1.3329 - accuracy: 0.4697 - val_loss: 1.2809 - val_accuracy: 0.4962\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 18s 176ms/step - loss: 1.3079 - accuracy: 0.4910 - val_loss: 1.4051 - val_accuracy: 0.4383\n",
            "Epoch 18/30\n",
            "96/96 [==============================] - 18s 177ms/step - loss: 1.3033 - accuracy: 0.4923 - val_loss: 1.5106 - val_accuracy: 0.4336\n",
            "Epoch 19/30\n",
            "96/96 [==============================] - 19s 186ms/step - loss: 1.2944 - accuracy: 0.5026 - val_loss: 1.4910 - val_accuracy: 0.4583\n",
            "Epoch 20/30\n",
            "96/96 [==============================] - 18s 178ms/step - loss: 1.3213 - accuracy: 0.4786 - val_loss: 1.2436 - val_accuracy: 0.5176\n",
            "Epoch 21/30\n",
            "96/96 [==============================] - 18s 179ms/step - loss: 1.2731 - accuracy: 0.5015 - val_loss: 1.5196 - val_accuracy: 0.4284\n",
            "Epoch 22/30\n",
            "96/96 [==============================] - 18s 180ms/step - loss: 1.2974 - accuracy: 0.5003 - val_loss: 1.2378 - val_accuracy: 0.5299\n",
            "Epoch 23/30\n",
            "96/96 [==============================] - 18s 177ms/step - loss: 1.2465 - accuracy: 0.5237 - val_loss: 1.1836 - val_accuracy: 0.5408\n",
            "Epoch 24/30\n",
            "96/96 [==============================] - 18s 174ms/step - loss: 1.2777 - accuracy: 0.4984 - val_loss: 1.2305 - val_accuracy: 0.5270\n",
            "Epoch 25/30\n",
            "96/96 [==============================] - 18s 174ms/step - loss: 1.2497 - accuracy: 0.5155 - val_loss: 1.3843 - val_accuracy: 0.4564\n",
            "Epoch 26/30\n",
            "96/96 [==============================] - 18s 177ms/step - loss: 1.2398 - accuracy: 0.5169 - val_loss: 1.3914 - val_accuracy: 0.4620\n",
            "Epoch 27/30\n",
            "96/96 [==============================] - 18s 181ms/step - loss: 1.3119 - accuracy: 0.4908 - val_loss: 1.2012 - val_accuracy: 0.5432\n",
            "Epoch 28/30\n",
            "96/96 [==============================] - 19s 185ms/step - loss: 1.2549 - accuracy: 0.5069 - val_loss: 1.2765 - val_accuracy: 0.5085\n",
            "Epoch 29/30\n",
            "96/96 [==============================] - 18s 181ms/step - loss: 1.2554 - accuracy: 0.5133 - val_loss: 1.1800 - val_accuracy: 0.5422\n",
            "Epoch 30/30\n",
            "96/96 [==============================] - 19s 187ms/step - loss: 1.2173 - accuracy: 0.5385 - val_loss: 1.1819 - val_accuracy: 0.5436\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f179004c950>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEBHzscVSoVa",
        "outputId": "937e586d-bfd1-4daa-a4e5-63c5ed595242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " random_flip_1 (RandomFlip)  (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " random_rotation_1 (RandomRo  (None, 128, 128, 3)      0         \n",
            " tation)                                                         \n",
            "                                                                 \n",
            " random_zoom_1 (RandomZoom)  (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 128, 128, 3)      12        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 126, 126, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 63, 63, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 61, 61, 32)        18464     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 30, 30, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 14, 14, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               1605888   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 6)                 198       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,672,626\n",
            "Trainable params: 1,672,620\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model_2.evaluate(test_dataset)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7bDgfqzSxId",
        "outputId": "dcebb960-69f3-4aaa-dcad-3adc63fbd81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 5s 124ms/step - loss: 1.1810 - accuracy: 0.5478\n",
            "Test loss 1.1809520721435547\n",
            "Test accuracy 0.5478261113166809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrLe9mFwhRKC"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5xs6_RhRKC"
      },
      "source": [
        "Aunque diseñar nuestra propia red puede producir algunos resultados aceptables, suele ser mejor aprovechar el conocimiento ya existente en una red pre-entrenada. Esto no solo nos lleva a resultados mejores, sino que además nos ahorra mucho tiempo de diseño de la red. Para ello, el módulo [Keras Applications](https://keras.io/api/applications/) contiene varios diseños de redes listos para su uso. Por ejemplo, para hacer uso de la famosa red VGG16 hacemos lo siguiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTLIqMTAhRKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d3426c-e84e-4a0d-dee0-a029d35ced00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=32, batch_size=64)\n",
        "\n",
        "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZUvx4zdhRKD"
      },
      "source": [
        "Por defecto, todas las redes de Keras Applications están precargadas con los pesos que se obtuvieron al entrenar la red sobre el dataset de la competición de [ImageNet](http://www.image-net.org/). Para adaptar la red a nuestro problema, hemos necesitado especificar la resolución de nuestras imágenes (`input_shape`), así como eliminar las capas de salida (`top`) de la red original, dado que nosotros tendremos un número diferente de clases.\n",
        "\n",
        "Ahora, ¿cómo hacemos para transferir el aprendizaje de esta red? Vamos a ver cómo implementar la estrategia de \"bottleneck features\". En primer lugar, marcamos el modelo VGG16 como no entrenable, para que sus parámetros se mantengan congelados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWsuQg0phRKD"
      },
      "outputs": [],
      "source": [
        "vgg16_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljtS1PhRhRKE"
      },
      "source": [
        "Hecho esto, vamos a construir una red neuronal que incluya la VGG16 como una de sus \"capas\". Es necesario tener en cuenta que la red VGG16 se entrenó realizando una normalización muy específica de las imágenes de entrenamiento, y nosotros debemos seguir ese mismo proceso para que la red se comporte correctamente. Convenientemente, Keras también nos da una funcionalidad para replicar la normalización que la VGG16 necesita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjFpoNRIhRKE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTBk5uRChRKF"
      },
      "source": [
        "Podemos probar esta normalización con alguna de las imágenes de nuestro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6ukW26shRKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd88ba77-0df3-4160-f863-a3ff2098181a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before normalizing: [[[69.390625 46.890625 46.390625]\n",
            "  [58.984375 35.203125 37.796875]\n",
            "  [51.53125  28.5      30.84375 ]]\n",
            "\n",
            " [[53.4375   32.734375 32.4375  ]\n",
            "  [56.203125 34.703125 35.203125]\n",
            "  [59.515625 38.015625 38.515625]]\n",
            "\n",
            " [[53.9375   32.9375   31.9375  ]\n",
            "  [54.90625  32.40625  31.90625 ]\n",
            "  [60.       37.5      37.      ]]]\n",
            "After normalizing: [[[-57.548378 -69.888374 -54.289375]\n",
            "  [-66.14213  -81.575874 -64.695625]\n",
            "  [-73.09525  -88.279    -72.14875 ]]\n",
            "\n",
            " [[-71.5015   -84.044624 -70.2425  ]\n",
            "  [-68.73588  -82.075874 -67.476875]\n",
            "  [-65.42338  -78.763374 -64.164375]]\n",
            "\n",
            " [[-72.0015   -83.8415   -69.7425  ]\n",
            "  [-72.03275  -84.37275  -68.77375 ]\n",
            "  [-66.939    -79.279    -63.68    ]]]\n"
          ]
        }
      ],
      "source": [
        "for X_batch, _ in train_dataset:\n",
        "    break\n",
        "    \n",
        "print(f\"Before normalizing: {X_batch[0, :3, :3, :]}\")\n",
        "print(f\"After normalizing: {preprocess_input(X_batch)[0, :3, :3, :]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx3EQeEThRKF"
      },
      "source": [
        "La normalización realizada por la VGG16 consiste en invertir el orden de los canales de color (RGB -> BGR), y restar los valores medios sobre todo el dataset ImageNet para cada canal de color por separado. Afortunadamente, la función `preprocess_input` que hemos importado hace todo este trabajo por nosotros. Además, podemos incrustar esta función como la primera capa de nuestra red, cumpliendo el papel de la capa `Rescaling` que utilizamos en el apartado anterior. Esto es posible gracias a la capa `Lambda`, que permite construir una capa Keras en base a cualquier función de Tensorflow. De modo que, vamos a comenzar nuestro diseño de red con esta capa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnSoUUrQhRKG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k83-_MmdhRKG"
      },
      "source": [
        "Tras esto, podemos añadir toda la red VGG16 como si fuera una nueva capa, y nuestras propias capas después de ella. A continuación tenemos un ejemplo de esta clase de diseño, aunque es importante destacar que es un diseño muy sencillo que contiene algunos fallos; una red real para hacer transfer learning debería tener un diseño mejor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f0RdEumhRKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e873e59-e4d1-4404-94e6-9713ea5f7db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda (Lambda)             (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,717,766\n",
            "Trainable params: 3,078\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.add(vgg16_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NueAGF7DhRKG"
      },
      "source": [
        "Observa cómo en el resumen del modelo podemos ver que la red completa tiene millones de parámetros, pero dado que hemos congelado toda la parte de la red perteneciente a la VGG16, solo unos pocos miles de parámetros son entrenables (trainable): aquellos correspondientes a la capa Dense que hemos colocado al final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn7iMKlQhRKH"
      },
      "source": [
        "Ya podemos compilar y entrenar el modelo a la manera habitual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eBG6zKwhRKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c14e39-0c5d-4d1a-bb11-e0856383b294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96/96 [==============================] - 11s 98ms/step - loss: 67.3942 - accuracy: 0.3311\n",
            "33/33 [==============================] - 4s 92ms/step - loss: 205.5580 - accuracy: 0.2725\n",
            "Loss 2.06e+02, accuracy 27.2%\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n",
        "model.fit(train_dataset, epochs=1)\n",
        "\n",
        "loss, acc = model.evaluate(test_dataset)\n",
        "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sb7SVSkhRKH"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Usando la estrategia \"bottleneck\" presentada, implementa una red que haga transfer learning desde la red VGG16, con un diseño correcto. Si lo haces adecuadamente, esta red debe obtener mejores resultados que con la red que diseñaste en el apartado anterior, y con al menos un 80% de acierto sobre el conjunto de test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2LQa-mhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Algunos consejos para mejorar tu diseño de red:\n",
        "    \n",
        "- Incluye una o más capas Dense, con sus funciones de activación apropiadas, antes de la capa de salida.\n",
        "- Intenta usar una capa de tipo [GlobalAveragePooling](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) en lugar de la capa Flatten. Esta capa calcula una media de todos los valores de píxeles para cada canal, y en algunas ocasiones produce mejores resultados que la capa Flatten.\n",
        "- ¡Y no olvides todos los consejos del apartado anterior! También aplican aquí.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6bbHEWhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "\n",
        "Para mejorar aún más los resultados de tu red, utiliza las siguientes ideas:\n",
        "\n",
        "- Usa las estrategias PRO del ejercicio anterio.\n",
        "- Prueba otras redes pre-entrenadas de <a href=\"https://keras.io/api/applications/\">Keras Applications</a>, como ResNet, Xception o EfficientNet.\n",
        "- Usa una estrategia de transfer learning más avanzada, como fine-tuning o una combinación de bottleneck features y fine-tuning. Revisa las diapositivas de clase para saber cómo.\n",
        "   \n",
        "Si empleas todos estos trucos, es posible alcanzar más de un 90% de acierto en el conjunto de test.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLJ8IgANhRKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd484f3-b9c3-49af-cd40-1e02e591a863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Epoch 1/30\n",
            "96/96 [==============================] - 87s 836ms/step - loss: 2.1041 - accuracy: 0.3811 - val_loss: 1.0833 - val_accuracy: 0.6020\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 76s 777ms/step - loss: 1.1451 - accuracy: 0.5807 - val_loss: 0.8638 - val_accuracy: 0.6983\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.9353 - accuracy: 0.6717 - val_loss: 0.7169 - val_accuracy: 0.7571\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 76s 780ms/step - loss: 0.8183 - accuracy: 0.7180 - val_loss: 0.6626 - val_accuracy: 0.7657\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 76s 776ms/step - loss: 0.7398 - accuracy: 0.7483 - val_loss: 0.6215 - val_accuracy: 0.7837\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 76s 777ms/step - loss: 0.6836 - accuracy: 0.7578 - val_loss: 0.5842 - val_accuracy: 0.8008\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.6396 - accuracy: 0.7774 - val_loss: 0.5548 - val_accuracy: 0.8174\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 76s 779ms/step - loss: 0.5816 - accuracy: 0.7984 - val_loss: 0.5535 - val_accuracy: 0.8069\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.5210 - accuracy: 0.8247 - val_loss: 0.5322 - val_accuracy: 0.8212\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 75s 776ms/step - loss: 0.5185 - accuracy: 0.8175 - val_loss: 0.5182 - val_accuracy: 0.8250\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 75s 776ms/step - loss: 0.4815 - accuracy: 0.8346 - val_loss: 0.5264 - val_accuracy: 0.8207\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.4438 - accuracy: 0.8481 - val_loss: 0.5216 - val_accuracy: 0.8259\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 75s 776ms/step - loss: 0.4253 - accuracy: 0.8550 - val_loss: 0.5248 - val_accuracy: 0.8212\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 75s 777ms/step - loss: 0.3866 - accuracy: 0.8630 - val_loss: 0.4879 - val_accuracy: 0.8378\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 76s 777ms/step - loss: 0.3645 - accuracy: 0.8727 - val_loss: 0.5131 - val_accuracy: 0.8321\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 76s 780ms/step - loss: 0.3633 - accuracy: 0.8754 - val_loss: 0.4940 - val_accuracy: 0.8287\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.3434 - accuracy: 0.8811 - val_loss: 0.4982 - val_accuracy: 0.8325\n",
            "Epoch 18/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.3179 - accuracy: 0.8893 - val_loss: 0.4961 - val_accuracy: 0.8344\n",
            "Epoch 19/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.2975 - accuracy: 0.8969 - val_loss: 0.5101 - val_accuracy: 0.8363\n",
            "Epoch 20/30\n",
            "96/96 [==============================] - 76s 780ms/step - loss: 0.2966 - accuracy: 0.8976 - val_loss: 0.5005 - val_accuracy: 0.8430\n",
            "Epoch 21/30\n",
            "96/96 [==============================] - 76s 777ms/step - loss: 0.3017 - accuracy: 0.8935 - val_loss: 0.4902 - val_accuracy: 0.8477\n",
            "Epoch 22/30\n",
            "96/96 [==============================] - 76s 777ms/step - loss: 0.2672 - accuracy: 0.9060 - val_loss: 0.5360 - val_accuracy: 0.8349\n",
            "Epoch 23/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.2470 - accuracy: 0.9119 - val_loss: 0.5294 - val_accuracy: 0.8392\n",
            "Epoch 24/30\n",
            "96/96 [==============================] - 76s 780ms/step - loss: 0.2327 - accuracy: 0.9188 - val_loss: 0.5325 - val_accuracy: 0.8416\n",
            "Epoch 25/30\n",
            "96/96 [==============================] - 76s 776ms/step - loss: 0.2423 - accuracy: 0.9138 - val_loss: 0.5636 - val_accuracy: 0.8354\n",
            "Epoch 26/30\n",
            "96/96 [==============================] - 75s 777ms/step - loss: 0.2709 - accuracy: 0.9073 - val_loss: 0.5206 - val_accuracy: 0.8463\n",
            "Epoch 27/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.2214 - accuracy: 0.9219 - val_loss: 0.5072 - val_accuracy: 0.8392\n",
            "Epoch 28/30\n",
            "96/96 [==============================] - 76s 780ms/step - loss: 0.2249 - accuracy: 0.9224 - val_loss: 0.5353 - val_accuracy: 0.8425\n",
            "Epoch 29/30\n",
            "96/96 [==============================] - 76s 778ms/step - loss: 0.2066 - accuracy: 0.9285 - val_loss: 0.5367 - val_accuracy: 0.8477\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f17294c21d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "image_size = 224\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=image_size, batch_size=64)\n",
        "\n",
        "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy', \n",
        "    patience=8, \n",
        "    min_delta=0.001\n",
        ")\n",
        "\n",
        "bottleneck_model = Sequential()\n",
        "\n",
        "bottleneck_model.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))\n",
        "bottleneck_model.add(vgg16_model)\n",
        "\n",
        "\n",
        "bottleneck_model.add(GlobalAveragePooling2D())\n",
        "bottleneck_model.add(Dense(512, activation=\"relu\"))\n",
        "bottleneck_model.add(Dropout(0.5))\n",
        "bottleneck_model.add(Dense(256, activation=\"relu\"))\n",
        "bottleneck_model.add(Dropout(0.5))\n",
        "bottleneck_model.add(Dense(32, activation=\"relu\"))\n",
        "bottleneck_model.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "bottleneck_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "bottleneck_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    verbose=1 ,\n",
        "    callbacks=[custom_early_stopping],\n",
        "    validation_data=val_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bottleneck_model.summary()"
      ],
      "metadata": {
        "id": "3gO2HLRYSBZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0224807b-7204-49c6-ea40-417609fcd8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_1 (Lambda)           (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 32)                8224      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 6)                 198       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,117,094\n",
            "Trainable params: 402,406\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = bottleneck_model.evaluate(test_dataset)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "id": "vXjpsk5qFxMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e1ef9a-240e-4600-b560-c613e6b0ce8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 22s 638ms/step - loss: 0.5124 - accuracy: 0.8556\n",
            "Test loss 0.5124409198760986\n",
            "Test accuracy 0.855555534362793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 224\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=image_size, batch_size=64)\n",
        "\n",
        "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy', \n",
        "    patience=8, \n",
        "    min_delta=0.001\n",
        ")\n",
        "\n",
        "bottleneck_model_2 = Sequential([\n",
        "              RandomFlip('horizontal'),\n",
        "              RandomRotation(0.2),\n",
        "              RandomZoom(0.3,0.2),\n",
        "              BatchNormalization()\n",
        "             ])\n",
        "\n",
        "bottleneck_model_2.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))\n",
        "bottleneck_model_2.add(vgg16_model)\n",
        "\n",
        "bottleneck_model_2.add(Flatten())\n",
        "bottleneck_model_2.add(Dense(512, activation=\"relu\"))\n",
        "bottleneck_model_2.add(Dropout(0.5))\n",
        "bottleneck_model_2.add(Dense(256, activation=\"relu\"))\n",
        "bottleneck_model_2.add(Dropout(0.5))\n",
        "bottleneck_model_2.add(Dense(32, activation=\"relu\"))\n",
        "bottleneck_model_2.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "bottleneck_model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "bottleneck_model_2.fit(\n",
        "    train_dataset,\n",
        "    epochs=20,\n",
        "    verbose=1 ,\n",
        "    callbacks=[custom_early_stopping],\n",
        "    validation_data=val_dataset\n",
        ")\n",
        "\n",
        "score = bottleneck_model_2.evaluate(test_dataset)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "id": "D-vX8FJJlDsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18fb3e8-712b-49a0-d21f-52aa658fef7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Epoch 1/20\n",
            "96/96 [==============================] - 135s 1s/step - loss: 2.1112 - accuracy: 0.1986 - val_loss: 1.7797 - val_accuracy: 0.2173\n",
            "Epoch 2/20\n",
            "96/96 [==============================] - 128s 1s/step - loss: 1.7944 - accuracy: 0.2213 - val_loss: 1.7685 - val_accuracy: 0.2130\n",
            "Epoch 3/20\n",
            "96/96 [==============================] - 127s 1s/step - loss: 1.7666 - accuracy: 0.2262 - val_loss: 1.7594 - val_accuracy: 0.2372\n",
            "Epoch 4/20\n",
            "96/96 [==============================] - 127s 1s/step - loss: 1.7568 - accuracy: 0.2438 - val_loss: 1.7538 - val_accuracy: 0.2372\n",
            "Epoch 5/20\n",
            "96/96 [==============================] - 128s 1s/step - loss: 1.7426 - accuracy: 0.2484 - val_loss: 1.7361 - val_accuracy: 0.2372\n",
            "Epoch 6/20\n",
            "96/96 [==============================] - 128s 1s/step - loss: 1.7395 - accuracy: 0.2440 - val_loss: 1.7306 - val_accuracy: 0.2372\n",
            "Epoch 7/20\n",
            "96/96 [==============================] - 128s 1s/step - loss: 1.7407 - accuracy: 0.2455 - val_loss: 1.7356 - val_accuracy: 0.2372\n",
            "Epoch 8/20\n",
            "96/96 [==============================] - 128s 1s/step - loss: 1.7257 - accuracy: 0.2463 - val_loss: 1.7332 - val_accuracy: 0.2372\n",
            "Epoch 9/20\n",
            "96/96 [==============================] - 128s 1s/step - loss: 1.7230 - accuracy: 0.2463 - val_loss: 1.7247 - val_accuracy: 0.2272\n",
            "Epoch 10/20\n",
            "96/96 [==============================] - 128s 1s/step - loss: 1.7195 - accuracy: 0.2463 - val_loss: 1.7251 - val_accuracy: 0.2372\n",
            "Epoch 11/20\n",
            "96/96 [==============================] - 128s 1s/step - loss: 1.7228 - accuracy: 0.2456 - val_loss: 1.7210 - val_accuracy: 0.2329\n",
            "33/33 [==============================] - 20s 568ms/step - loss: 1.7197 - accuracy: 0.2343\n",
            "Test loss 1.7196717262268066\n",
            "Test accuracy 0.23429951071739197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "image_size = 224\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=image_size, batch_size=64)\n",
        "\n",
        "resNet_model = ResNet50(include_top=False, input_shape=(image_size, image_size, 3))\n",
        "resNet_model.trainable = False\n",
        "\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy', \n",
        "    patience=8, \n",
        "    min_delta=0.001\n",
        ")\n",
        "\n",
        "bottleneck_model_3 = Sequential()\n",
        "\n",
        "bottleneck_model_3.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))\n",
        "bottleneck_model_3.add(resNet_model)\n",
        "\n",
        "bottleneck_model_3.add(GlobalAveragePooling2D())\n",
        "bottleneck_model_3.add(Dense(512, activation=\"relu\"))\n",
        "bottleneck_model_3.add(Dropout(0.5))\n",
        "bottleneck_model_3.add(Dense(256, activation=\"relu\"))\n",
        "bottleneck_model_3.add(Dropout(0.5))\n",
        "bottleneck_model_3.add(Dense(32, activation=\"relu\"))\n",
        "bottleneck_model_3.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "bottleneck_model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "bottleneck_model_3.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    verbose=1 ,\n",
        "    callbacks=[custom_early_stopping],\n",
        "    validation_data=val_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "QdgGz9hnMa4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf6fd7a-59f7-444e-df62-a37d383c4a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Epoch 1/30\n",
            "96/96 [==============================] - 62s 580ms/step - loss: 1.2570 - accuracy: 0.5243 - val_loss: 0.6790 - val_accuracy: 0.7723\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 53s 544ms/step - loss: 0.7490 - accuracy: 0.7320 - val_loss: 0.5695 - val_accuracy: 0.8022\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.6173 - accuracy: 0.7840 - val_loss: 0.4947 - val_accuracy: 0.8278\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 53s 546ms/step - loss: 0.5295 - accuracy: 0.8201 - val_loss: 0.4537 - val_accuracy: 0.8444\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 53s 543ms/step - loss: 0.4818 - accuracy: 0.8354 - val_loss: 0.4540 - val_accuracy: 0.8454\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 53s 544ms/step - loss: 0.4262 - accuracy: 0.8512 - val_loss: 0.4361 - val_accuracy: 0.8525\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.3854 - accuracy: 0.8645 - val_loss: 0.4295 - val_accuracy: 0.8534\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 53s 544ms/step - loss: 0.3642 - accuracy: 0.8775 - val_loss: 0.4479 - val_accuracy: 0.8510\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 53s 543ms/step - loss: 0.3495 - accuracy: 0.8795 - val_loss: 0.4484 - val_accuracy: 0.8539\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.3241 - accuracy: 0.8898 - val_loss: 0.5002 - val_accuracy: 0.8354\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 53s 544ms/step - loss: 0.3272 - accuracy: 0.8864 - val_loss: 0.4258 - val_accuracy: 0.8629\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 53s 546ms/step - loss: 0.2857 - accuracy: 0.9030 - val_loss: 0.4523 - val_accuracy: 0.8596\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.2736 - accuracy: 0.9073 - val_loss: 0.4764 - val_accuracy: 0.8553\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 53s 544ms/step - loss: 0.2588 - accuracy: 0.9130 - val_loss: 0.4410 - val_accuracy: 0.8629\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.2636 - accuracy: 0.9084 - val_loss: 0.4420 - val_accuracy: 0.8639\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 53s 543ms/step - loss: 0.2611 - accuracy: 0.9069 - val_loss: 0.4579 - val_accuracy: 0.8648\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.2310 - accuracy: 0.9140 - val_loss: 0.4639 - val_accuracy: 0.8648\n",
            "Epoch 18/30\n",
            "96/96 [==============================] - 53s 546ms/step - loss: 0.2231 - accuracy: 0.9232 - val_loss: 0.4613 - val_accuracy: 0.8639\n",
            "Epoch 19/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.2312 - accuracy: 0.9189 - val_loss: 0.4460 - val_accuracy: 0.8710\n",
            "Epoch 20/30\n",
            "96/96 [==============================] - 53s 546ms/step - loss: 0.1852 - accuracy: 0.9385 - val_loss: 0.5065 - val_accuracy: 0.8653\n",
            "Epoch 21/30\n",
            "96/96 [==============================] - 53s 544ms/step - loss: 0.2183 - accuracy: 0.9270 - val_loss: 0.4731 - val_accuracy: 0.8629\n",
            "Epoch 22/30\n",
            "96/96 [==============================] - 53s 543ms/step - loss: 0.1631 - accuracy: 0.9423 - val_loss: 0.5304 - val_accuracy: 0.8615\n",
            "Epoch 23/30\n",
            "96/96 [==============================] - 53s 544ms/step - loss: 0.1992 - accuracy: 0.9331 - val_loss: 0.4864 - val_accuracy: 0.8662\n",
            "Epoch 24/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.1648 - accuracy: 0.9385 - val_loss: 0.4800 - val_accuracy: 0.8710\n",
            "Epoch 25/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.1515 - accuracy: 0.9515 - val_loss: 0.4994 - val_accuracy: 0.8771\n",
            "Epoch 26/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.1561 - accuracy: 0.9482 - val_loss: 0.4988 - val_accuracy: 0.8752\n",
            "Epoch 27/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.1321 - accuracy: 0.9538 - val_loss: 0.5687 - val_accuracy: 0.8686\n",
            "Epoch 28/30\n",
            "96/96 [==============================] - 53s 544ms/step - loss: 0.1525 - accuracy: 0.9452 - val_loss: 0.5033 - val_accuracy: 0.8695\n",
            "Epoch 29/30\n",
            "96/96 [==============================] - 53s 545ms/step - loss: 0.1384 - accuracy: 0.9503 - val_loss: 0.5444 - val_accuracy: 0.8662\n",
            "Epoch 30/30\n",
            "96/96 [==============================] - 53s 546ms/step - loss: 0.1382 - accuracy: 0.9545 - val_loss: 0.5477 - val_accuracy: 0.8615\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16b38db9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bottleneck_model_3.summary()"
      ],
      "metadata": {
        "id": "XOFUwhf5RsHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02caefc8-5558-4ae8-e2c5-8109a4e42328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_3 (Lambda)           (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 32)                8224      \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 6)                 198       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,776,550\n",
            "Trainable params: 1,188,838\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = bottleneck_model_3.evaluate(test_dataset)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "id": "mswWEh1UNaaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e49a34-b473-4ca7-be66-cc7d5a2e8e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 15s 426ms/step - loss: 0.5844 - accuracy: 0.8618\n",
            "Test loss 0.5844084620475769\n",
            "Test accuracy 0.861835777759552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import Xception\n",
        "\n",
        "image_size = 299\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=image_size, batch_size=64)\n",
        "\n",
        "xception_model = Xception(include_top=False, input_shape=(image_size, image_size, 3))\n",
        "xception_model.trainable = False\n",
        "\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy', \n",
        "    patience=8, \n",
        "    min_delta=0.001\n",
        ")\n",
        "\n",
        "bottleneck_model_4 = Sequential()\n",
        "\n",
        "bottleneck_model_4.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))\n",
        "bottleneck_model_4.add(xception_model)\n",
        "\n",
        "bottleneck_model_4.add(GlobalAveragePooling2D())\n",
        "bottleneck_model_4.add(Dense(512, activation=\"relu\"))\n",
        "bottleneck_model_4.add(Dropout(0.5))\n",
        "bottleneck_model_4.add(Dense(256, activation=\"relu\"))\n",
        "bottleneck_model_4.add(Dropout(0.5))\n",
        "bottleneck_model_4.add(Dense(32, activation=\"relu\"))\n",
        "bottleneck_model_4.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "bottleneck_model_4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "bottleneck_model_4.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    verbose=1 ,\n",
        "    callbacks=[custom_early_stopping],\n",
        "    validation_data=val_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "8L5Yup2sS881",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51ad403-8c64-4e93-a484-d3d0e0ba621c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 0s 0us/step\n",
            "83697664/83683744 [==============================] - 0s 0us/step\n",
            "Epoch 1/30\n",
            "96/96 [==============================] - 79s 658ms/step - loss: 2.2924 - accuracy: 0.2159 - val_loss: 1.7571 - val_accuracy: 0.2443\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 63s 646ms/step - loss: 1.7581 - accuracy: 0.2540 - val_loss: 1.7176 - val_accuracy: 0.2951\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 64s 658ms/step - loss: 1.7211 - accuracy: 0.2688 - val_loss: 1.6824 - val_accuracy: 0.3074\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 65s 664ms/step - loss: 1.7033 - accuracy: 0.3011 - val_loss: 1.6665 - val_accuracy: 0.3178\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 66s 676ms/step - loss: 1.6553 - accuracy: 0.3172 - val_loss: 1.6271 - val_accuracy: 0.3425\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 65s 673ms/step - loss: 1.6343 - accuracy: 0.3366 - val_loss: 1.5785 - val_accuracy: 0.3615\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 67s 686ms/step - loss: 1.6089 - accuracy: 0.3353 - val_loss: 1.5567 - val_accuracy: 0.3681\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 67s 686ms/step - loss: 1.5816 - accuracy: 0.3500 - val_loss: 1.5620 - val_accuracy: 0.3615\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 66s 681ms/step - loss: 1.5852 - accuracy: 0.3497 - val_loss: 1.5723 - val_accuracy: 0.3411\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 67s 685ms/step - loss: 1.5620 - accuracy: 0.3519 - val_loss: 1.5401 - val_accuracy: 0.3544\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 67s 685ms/step - loss: 1.5416 - accuracy: 0.3727 - val_loss: 1.5429 - val_accuracy: 0.3567\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 67s 686ms/step - loss: 1.5360 - accuracy: 0.3614 - val_loss: 1.5115 - val_accuracy: 0.3757\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 67s 686ms/step - loss: 1.5205 - accuracy: 0.3681 - val_loss: 1.5095 - val_accuracy: 0.3776\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 67s 686ms/step - loss: 1.5091 - accuracy: 0.3752 - val_loss: 1.4819 - val_accuracy: 0.3885\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 66s 683ms/step - loss: 1.5046 - accuracy: 0.3870 - val_loss: 1.4851 - val_accuracy: 0.3847\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 66s 677ms/step - loss: 1.4974 - accuracy: 0.3866 - val_loss: 1.4639 - val_accuracy: 0.4032\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 67s 685ms/step - loss: 1.4905 - accuracy: 0.3828 - val_loss: 1.4824 - val_accuracy: 0.3852\n",
            "Epoch 18/30\n",
            "96/96 [==============================] - 67s 686ms/step - loss: 1.4855 - accuracy: 0.3977 - val_loss: 1.4693 - val_accuracy: 0.3980\n",
            "Epoch 19/30\n",
            "96/96 [==============================] - 66s 683ms/step - loss: 1.4693 - accuracy: 0.4002 - val_loss: 1.4591 - val_accuracy: 0.4198\n",
            "Epoch 20/30\n",
            "96/96 [==============================] - 66s 677ms/step - loss: 1.4640 - accuracy: 0.4037 - val_loss: 1.4475 - val_accuracy: 0.4303\n",
            "Epoch 21/30\n",
            "96/96 [==============================] - 65s 672ms/step - loss: 1.4552 - accuracy: 0.4170 - val_loss: 1.4466 - val_accuracy: 0.4402\n",
            "Epoch 22/30\n",
            "96/96 [==============================] - 67s 686ms/step - loss: 1.4396 - accuracy: 0.4231 - val_loss: 1.4346 - val_accuracy: 0.4331\n",
            "Epoch 23/30\n",
            "96/96 [==============================] - 67s 684ms/step - loss: 1.4262 - accuracy: 0.4191 - val_loss: 1.4404 - val_accuracy: 0.4369\n",
            "Epoch 24/30\n",
            "96/96 [==============================] - 66s 683ms/step - loss: 1.4149 - accuracy: 0.4308 - val_loss: 1.4351 - val_accuracy: 0.4303\n",
            "Epoch 25/30\n",
            "96/96 [==============================] - 66s 676ms/step - loss: 1.4124 - accuracy: 0.4342 - val_loss: 1.4280 - val_accuracy: 0.4464\n",
            "Epoch 26/30\n",
            "96/96 [==============================] - 67s 684ms/step - loss: 1.4274 - accuracy: 0.4193 - val_loss: 1.4352 - val_accuracy: 0.4402\n",
            "Epoch 27/30\n",
            "96/96 [==============================] - 66s 680ms/step - loss: 1.4001 - accuracy: 0.4341 - val_loss: 1.4828 - val_accuracy: 0.4260\n",
            "Epoch 28/30\n",
            "96/96 [==============================] - 67s 687ms/step - loss: 1.4435 - accuracy: 0.4140 - val_loss: 1.4289 - val_accuracy: 0.4497\n",
            "Epoch 29/30\n",
            "96/96 [==============================] - 66s 680ms/step - loss: 1.4035 - accuracy: 0.4344 - val_loss: 1.4240 - val_accuracy: 0.4431\n",
            "Epoch 30/30\n",
            "96/96 [==============================] - 67s 685ms/step - loss: 1.3853 - accuracy: 0.4457 - val_loss: 1.4314 - val_accuracy: 0.4426\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7eff307e3690>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bottleneck_model_4.summary()"
      ],
      "metadata": {
        "id": "t22IyINiTLau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7419f12b-69be-42a9-c2fe-4d28c8a85052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda (Lambda)             (None, 299, 299, 3)       0         \n",
            "                                                                 \n",
            " xception (Functional)       (None, 10, 10, 2048)      20861480  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                8224      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 198       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,050,318\n",
            "Trainable params: 1,188,838\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = bottleneck_model_4.evaluate(test_dataset)\n",
        "print(\"Test loss\", score[0])\n",
        "print(\"Test accuracy\", score[1])"
      ],
      "metadata": {
        "id": "bzbiwX2dTP7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1154ebb4-9a51-40d1-f571-e22a1631fb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 17s 489ms/step - loss: 1.4162 - accuracy: 0.4517\n",
            "Test loss 1.4161601066589355\n",
            "Test accuracy 0.45169082283973694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJc7DMJhRKK"
      },
      "source": [
        "## Informe final y resumen de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV2c2JCKhRKK"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Escribe en la siguiente celda un pequeño informe con:\n",
        "    <ul>\n",
        "        <li>Una tabla de resultados, indicando qué diseños de red has probado y qué resultados en test has obtenido. Puede usar un estilo de tabla como el que se muestra abajo.</li>\n",
        "        <li>De las estrategias y diseños que has ido probando, ¿qué ha funcionado y qué no?</li>\n",
        "        <li>¿Qué has aprendido con esta práctica?\n",
        "    </ul>\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EofHCraehRKL"
      },
      "source": [
        "Ejemplo para la tabla de resultados\n",
        "\n",
        "|Procesado de imágenes|Diseño de red neuronal|Estrategia de entrenamiento|Acierto en test|\n",
        "|---------------------|----------------------|---------------------------|---------------|\n",
        "|Tamaño 128x128; batch size 64; augmentation: flip, rotation, zoom; batch normalitation |(Convolutional(32) + MaxPool(3x3)) * 3 + Flatten + Dense(256) + Drop (0.5) + Dense(32) + Dense(6) |Entrenamiento desde 0|56%|\n",
        "|Tamaño 128x128; batch size 64; augmentation: flip, rotation, zoom; batch normalitation |Convolutional(64) + MaxPool(3x3) + (Convolutional(32) + MaxPool(3x3)) * 2 + Flatten + Dense(512) + Drop (0.5) + Dense(256) + Drop(0.5) + Dense(32) + Dense(6) |Entrenamiento desde 0|54%|\n",
        "|Tamaño 224x224; batch size 64 |VGG16 + GlobalAveragePool + Dense(512) + Drop(0.5) + Dense(256) + Drop(0.5) + Dense(32) + Dense(6)|Bottleneck features|85%|\n",
        "|Tamaño 224x224; batch size 64; augmentation: flip, rotation, zoom; batch normalitation |VGG16 + GlobalAveragePool + Dense(512) + Drop(0.5) + Dense(256) + Drop(0.5) + Dense(32) + Dense(6)|Bottleneck features|23%|\n",
        "|Tamaño 224x224; batch size 64 |ResNet50 + GlobalAveragePool + Dense(512) + Drop(0.5) + Dense(256) + Drop(0.5) + Dense(32) + Dense(6)|Bottleneck features|87%|\n",
        "|Tamaño 299x299; batch size 64 |Xception + GlobalAveragePool + Dense(512) + Drop(0.5) + Dense(256) + Drop(0.5) + Dense(32) + Dense(6)|Bottleneck features|45%|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo con mejor accuracy es el modelo de **transfer learning** con **ResNet50**, con un **87% accuracy**. El peor modelo, por otro lado, es el model **VGG-16**, con augmentation de imágenes, el cual se queda en un **23%** de accuracy.\n",
        "\n",
        "En los dos primeros modelos hemos usado 3 kernels, pero al aumentar su número a 5, hemos obtenido reusltados peores. \n",
        "\n",
        "El proceso de **augmentation de imágenes**, nos **ayudó** mucho en las **redes convulucionales iniciales**, pero con el **VGG-16 empeoraba** muchísimo el rendimineto del modelo.\n",
        "\n",
        "Hemos aprendido, que no siempre es mejor inventar, si no que se obtienen mejores resultados aprovechando el trabajo realizado por otros e intentando mejorarlo."
      ],
      "metadata": {
        "id": "AvpCF2N7NPZO"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "hungerGames_student.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}