---
title: "Poisson Regression: wine sales by chemical composition"
author: "MÃ¡ster en Data Science y Big Data en Finanzas"
date: 'AFI, 2022'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---


```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri(file.path("AfiLogo.jpg")), 
               alt = 'logo', 
               style = 'position:absolute; top:0; right:0; padding:10px;',
               width="173",
               height="77")
```

# Motivation

**Objective:** explain wine sales by chemical composition

Dataset with around 11,000 commercially wines containing chemical and other characteristics that are tasted first by wine distribution companies and then are purchased to be used in restaurants and stores 

Hence, the larger the wine cases purchased, the more likely the wine will be sold in restaurants and stores

Load some packages

```{r}
# delete everything
rm(list=ls()) 

library(tidyverse)
library(GGally)
library(MASS)
library(effects)
library(pscl)
```

# The dataset

Specifically, the output (TARGET) is number of wine cases purchased by wine distribution companies

The predictors are 12 chemical properties of the wines, plus LabelAppeal (marketing score indicating the appeal of label design),
plus STARS (wine rating given by experts)

```{r}
wine_training <- read.csv("wine-training-data.csv", header = TRUE)
wine_testing <- read.csv("wine-testing-data.csv", header = TRUE)

head(wine_training,10)

head(wine_testing,10)
```

Summary:

```{r}
summary(wine_training)
```

Insights?

## Missing values

Missing valus by rows:

```{r}
hist(rowMeans(is.na(wine_training)))
```

For more than 6000 wines we have complete information

For around 5000 wines we have 6-7% of NAs, etc.

Missing valus by columns:

```{r}
barplot(colMeans(is.na(wine_training)), las=2)
```

The variable STARS has many missing values (around 26%)

We can drop it or interpret a missing value as a bad rating

Hence, we are going to:

- remove first variable (just the ID) 

- convert the NAs of STARS into 1

- remove the rest of wine observations with NA

```{r}
wine_training = wine_training %>% 
  mutate(STARS = ifelse(is.na(STARS), 1, STARS)) %>% 
  dplyr::select(-INDEX) %>%
  na.omit()

wine_testing = wine_testing %>% 
  mutate(STARS = ifelse(is.na(STARS), 1, STARS)) %>% 
  dplyr::select(-IN,-TARGET) %>%
  na.omit() 
```

Take a look at STARS vs TARGET:

```{r}
wine_training %>% 
  ggplot(aes(x = as.factor(STARS), y = TARGET)) + 
  geom_boxplot(fill="lightblue") +
  labs(title = "TARGET by STARS", x = "", y = "", col = "") 
```

# Descriptive analysis

Around 80% of wines for training and the other 20% for testing:

```{r}
dim(wine_training)

dim(wine_testing)
```

Let's analyze the unconditional distribution of the target (number of wine cases purchased):

```{r}
# Analyze here the distribution of the Target

```

Insights?

Let's see now some conditional distributions

First respect to fixed acidity

```{r}
wine_training %>% 
  ggplot(aes(x = FixedAcidity, y = TARGET)) + 
  geom_point(fill="lightblue") +
  labs(title = "Cases by fixed acidity", x = "", y = "", col = "") 
```

We can try many other graphs...

Let's make multiple scatter-plots

```{r}
# Add here multiple scatter plots

```

Insights?

Note also AcidIndex has a slightly negative correlation with TARGET

The rest of correlations are close to 0

Finally, convert STARS into a factor variable

```{r}
wine_training$STARS = as.factor(wine_training$STARS)
wine_testing$STARS = as.factor(wine_testing$STARS)
```

# GLM

Let's fit first a linear model:

```{r}
# Add here a linear model (lm)

```

Insights?

Diagnosis

```{r}
# Check here the residuals

```

Insights?

But take care: if rates or counts are clearly bigger than 0, then LM may work fine

Let's try now a linear model with log transformation

Take care: the target has 0 values

```{r}
barplot(table(log(wine_training$TARGET+1)), col="darkblue")
```

```{r}
fit.lm <- lm(log(TARGET+1) ~ ., data=wine_training) 
summary(fit.lm)
```

The R2 is 40%, reasonable fit

Diagnosis

```{r}
par(mfrow=c(2,2))
plot(fit.lm)
par(mfrow=c(1,1))
```

Still, not so good: the high number of zeros makes the linear model (even with log) a bad choice

# Poisson Regression

Used for count data (e.g. number of wine cases) or rates (number of wine sales per month)

```{r}
fit.poisson <- glm(TARGET ~ ., family = "poisson", data = wine_training)
# by default we are using the log link

summary(fit.poisson)
```

## Interpretation

Fitted model: $\log(\lambda_i) = x_i'\ \beta$

That means for every 1 point increase in LabelAppeal, the wine-cases rate increases by exp(0.15)=1.17 times: this is a multiplicative effect, not additive

The coefficient is statistically significant

The same for the rest of variables:

```{r}
exp(cbind(coef(fit.poisson), confint(fit.poisson)))
```

Note values close to 1 are not significant

Note also that, after discounting for chemical variables, LabelAppeal and STARS are still significant

Meaning?

In any case, the model explains only 30% of the deviance: 1-10332/15334

This is like a pseudo-R2: 1-Residual.deviance/Null.deviance

AIC: 32079

Consider now a Poisson regression with the most significant variables:

```{r}
fit.poisson <- glm(TARGET ~ VolatileAcidity + Chlorides +
                     TotalSulfurDioxide + FreeSulfurDioxide + 
                     AcidIndex + LabelAppeal*STARS, 
                   family = "poisson", data = wine_training)
summary(fit.poisson)
```

The AIC is a bit better: 32020

## Interpretation with the effects package

STARS effect on target:

```{r}
plot(effect("STARS", fit.poisson), ci.style="band", rescale.axis=FALSE, multiline=TRUE, xlab="stars", ylab="rate", rug=FALSE, main="")
```

a very clear relation

LabelAppeal effect on target:

```{r}
plot(effect("LabelAppeal", fit.poisson), ci.style="band", rescale.axis=FALSE, multiline=TRUE, xlab="label appeal", ylab="rate", rug=FALSE, main="")
```

a very clear (but weaker) relation

Interaction effect

```{r}
plot(effect("LabelAppeal:STARS", fit.poisson), ci.style="band", rescale.axis=FALSE, multiline=TRUE, xlab="label appeal", ylab="rate", rug=FALSE, main="")
```

a very clear relation

All effects on target:

```{r}
plot(allEffects(fit.poisson), rescale.axis=FALSE, multiline=TRUE, rug=FALSE, main="")
```

Insights?

# Quasi-Poisson Regression

Useful to deal with over-dispersion

```{r}
fit.qpoisson <- glm(TARGET ~ VolatileAcidity + Chlorides + TotalSulfurDioxide + FreeSulfurDioxide + AcidIndex + LabelAppeal*STARS, family = "quasipoisson", data = wine_training)
summary(fit.qpoisson)
```

Practically the same as before. Note phi (dispersion parameter of QP) is 0.90

# Negative Binomial Regression

Useful to deal with over-dispersion

```{r}
fit.nb <- glm.nb(TARGET ~ VolatileAcidity + Chlorides + TotalSulfurDioxide + FreeSulfurDioxide + AcidIndex + LabelAppeal*STARS, data = wine_training)
summary(fit.nb)
```

Practically the same as before. Note theta (shape parameter of NB) is 47000, but possibly not significant

Why?

Indeed there is no big over-dispersion:

```{r}
mean(wine_training$TARGET)
var(wine_training$TARGET)

mean(wine_training$TARGET)/var(wine_training$TARGET)
```

# Zero-inflated Poisson model

Useful to deal with more 0's than expected

```{r}
fit.zip1 <- zeroinfl(TARGET ~ VolatileAcidity + Chlorides + TotalSulfurDioxide + FreeSulfurDioxide + AcidIndex + LabelAppeal+LabelAppeal:STARS, dist = "poisson", data = wine_training)
summary(fit.zip1)
```

Note there are two outputs because the zero-inflated model is comprised of two sub-models: one for the count process and other for the zero-mass one

One model explains log(lambda) as a linear combination of predictors

The other one explains $\log(\pi/(1-\pi))$ as a linear combination of predictors

## Interpretation

```{r}
expCoef = data.frame(matrix(exp(coef(fit.zip1)),ncol=2))
row.names(expCoef) <- names(fit.zip1$coefficients$zero)
colnames(expCoef) <- c("Count_model","Zero_inflation_model")
expCoef
```

If we focus on the zero-inflation model, the baseline odds of not selling wine cases is 0.003

If we increase the LabelAppleal by one unit, the odds is increased by 3.89

We can specify different predictors for the 0-point mass and the count component:

```{r}
fit.zip2 <- zeroinfl(TARGET ~ VolatileAcidity + Chlorides + TotalSulfurDioxide + FreeSulfurDioxide + AcidIndex + LabelAppeal*STARS | 1, dist = "poisson", data = wine_training)
summary(fit.zip2)
```

Here no regressors for zero component

Now different regressors for zero component:

```{r}
fit.zip3 <- zeroinfl(TARGET ~ VolatileAcidity + Chlorides + TotalSulfurDioxide + AcidIndex + LabelAppeal*STARS | LabelAppeal+LabelAppeal:STARS, dist = "poisson", data = wine_training)
summary(fit.zip3)
```

AIC for all the models:

```{r}
AIC.total = c(AIC(fit.poisson), AIC(fit.qpoisson), AIC(fit.nb), AIC(fit.zip1), AIC(fit.zip2), AIC(fit.zip3))
AIC.total
```

# Prediction

A linear model predicts the expected value of cases

A Poisson model predicts the rate or intensity, hence decimal points

Predicted vs actual using the log-transformed linear model model:

```{r}
plot(wine_training$TARGET, exp(predict(fit.lm, type = "response"))-1,
     xlab="actual",ylab="predicted")
abline(a=0,b=1);
cor(wine_training$TARGET,predict(fit.lm, type = "response"))^2
```

Predicted vs actual using the Poisson GLM:

```{r}
plot(wine_training$TARGET, predict(fit.poisson, type = "response"),
     xlab="actual",ylab="predicted")
abline(a=0,b=1);
cor(wine_training$TARGET,predict(fit.poisson, type = "response"))^2
```

A bit better

Predicted vs actual using the Poisson zip model:

```{r}
plot(wine_training$TARGET, predict(fit.zip3, type = "response"),
     xlab="actual",ylab="predicted")
abline(a=0,b=1);
cor(wine_training$TARGET,predict(fit.zip3, type = "response"))^2
```

A bit worse

Prediction for the wines in the testing set:

```{r}
predict(fit.zip3, newdata=wine_testing, type = "response")[1:10]

# we should round the numbers:
predictions = round(predict(fit.zip3, newdata=wine_testing, type = "response"), digits=0)
head(predictions,20)
```

## Prediction intervals

Take care: precict.zeroinfl does not have implemented computation of intervals

We can estimate the intervals using bootstrap, but more complicated

So, let's do it with a standard GLM

First, predict the link (linear predictor)

```{r}
preds = predict(fit.poisson, newdata=wine_testing, type = "link", se.fit=T)
critval <- 1.96 ## approx 95% CI
upr <- preds$fit + (critval * preds$se.fit)
lwr <- preds$fit - (critval * preds$se.fit)
fit <- preds$fit
fit[1:10] 
```

Note the predictions are in log-scale

Then, untransform the link to predict the mean (probability in this case): 

$$\hat{\mu} = g^{-1}(\text{linear predictor})$$

```{r}
fit2 <- fit.poisson$family$linkinv(fit)
upr2 <- fit.poisson$family$linkinv(upr)
lwr2 <- fit.poisson$family$linkinv(lwr)
fit2[1:10] 
```

Now the predictions are in counts (but with decimal points)

Organize and round the predictions and intervals in a data frame

```{r}
data.frame(lower=lwr2, prediction=round(fit2,digits=0), upper=upr2)[1:10,]
```

But how can we guess the predictive power of the models?

We need to build our own testing set to validate performance

```{r}
train <- sample(1:nrow(wine_training), round(nrow(wine_training)*0.80,0))
wine.train <- wine_training[train,]
wine.test <- wine_training[-train,]
```

GLM Poisson:

```{r}
fit.poisson <- glm(TARGET ~ VolatileAcidity + Chlorides + TotalSulfurDioxide + FreeSulfurDioxide + AcidIndex + LabelAppeal*STARS, family = "poisson", data = wine.train)
pred = predict(fit.poisson, newdata=wine.test, type = "response")

# conditional error:
rmse = sqrt(mean((pred-wine.test$TARGET)^2))
rmse

# unconditional error:
sd(wine.test$TARGET)

# R2 in testing set:
cor(pred,wine.test$TARGET)^2
```

Poisson regression is able to reduce 25% the original noise (sd)

ZIP Poisson

```{r}
fit.zip <- zeroinfl(TARGET ~ . + LabelAppeal:STARS| . , dist = "poisson", data = wine.train)
pred = predict(fit.zip, newdata=wine.test, type = "response")

# conditional error:
rmse = sqrt(mean((pred-wine.test$TARGET)^2))
rmse

# unconditional error:
sd(wine.test$TARGET)

# R2 in testing set:
cor(pred,wine.test$TARGET)^2
```

ZIP regression is able to reduce 30% the original noise

A bit better


