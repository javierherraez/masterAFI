---
title: "Case Study: Customer Analytics"
author: "MÃ¡ster en Data Science y Big Data en Finanzas"
date: 'AFI, 2022'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---
    
```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri(file.path("AfiLogo.jpg")), 
               alt = 'logo', 
               style = 'position:absolute; top:0; right:0; padding:10px;',
               width="173",
               height="77")
```

# Motivation

Customer churn: a customer (player, subscriber, user, etc.) ends his or her relationship with a company

High cost of churn for telco and banking companies: lost revenue and marketing costs involved with replacing those customers with new ones (who are difficult to gain)

It is more difficult and expensive to acquire a new customer than it is to retain a current one

Hence, reducing churn is a key business goal for many companies

In this case study, our **objective** will be to predict customer churn in a Telco company while explaining what features relate to customer churn, i.e. the company needs to understand who is leaving and why

Moreover, we will predict whether is at a high risk of churning and who are customers providing more income to the company than others: **risk learning**


<center>
<img src="churn.png" width="400"/>
</center>

<br>


### Available Data

Dataset from IBM Watson Telco Dataset: https://www.ibm.com/communities/analytics/watson-analytics-blog/guide-to-sample-datasets/

This telco company is concerned about the number of customers leaving their landline business for cable competitors

### Load useful libraries

```{r}
library(tidyverse)
library(skimr)
library(forcats)
library(VIM)
library(GGally)
library(MASS)
library(caret)
library(randomForest)
library(gbm)
library(neuralnet)
```

# Load and explore the data set

```{r}
churnData <- read.csv('ChurnData.csv', stringsAsFactors=T)

glimpse(churnData)
```

The dataset includes information about:

- Customers who left within the last month: The column is called Churn

- Services that each customer has signed up for: phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies

- Customer account information: how long they have been a customer, contract, payment method, paperless billing, monthly charges, and total charges

- Demographic info about customers: gender, age range, and if they have partners and dependents
  
Summary:

```{r}
summary(churnData)
```

### Some exploratory analysis 

```{r}
aggr(churnData, numbers = TRUE, sortVars = TRUE, labels = names(churnData),
     cex.axis = .5, gap = 1, ylab= c('Missing data','Pattern'))
```

11 NAs in TotalCharges, what to do?

```{r, include=F}
churnData <- churnData %>%  dplyr::select(-customerID) %>% drop_na()
```

Make some descriptive analysis

```{r}
# Insert your code here

table(churnData$Churn)
prop.table(table(churnData$Churn))
```

26% of customers left the company in the last month


### Some feature engineering

We will change "No internet service to "No" for columns: 

"OnlineSecurity", "OnlineBackup", "DeviceProtection", "TechSupport", "streamingTV", "streamingMovies"

Moreover, we will change "No phone service" to "No" for column "MultipleLines"

```{r, include=F}
for(i in c(9:14)) {
  churnData[,i] <- fct_collapse(churnData[,i], No = c("No","No internet service"))
}

churnData$MultipleLines <- fct_collapse(churnData$MultipleLines, No = c("No","No phone service"))
```


# Data splitting

```{r}
in_train <- createDataPartition(churnData$Churn, p = 0.7, list = FALSE)  # 70% for training
training <- churnData[ in_train,]
testing <- churnData[-in_train,]
nrow(training)
nrow(testing)
```

## Some descriptive analysis using the training set

```{r}
# Numeric vs categorical plot:
ggplot(training, aes(x=Churn, y=tenure)) +  geom_boxplot(fill="blue") 
# customers with more months with the company have less chances to leave

# The same, different view:
ggplot(training, aes(tenure)) + geom_density(aes(group=Churn, colour=Churn, fill=Churn), alpha=0.1) +xlab("tenure")
# customers with less than 2 years in the company tend to leave more
# customers with more than 5 years in the company tend to stay

# Numeric vs categorical plot:
ggplot(training, aes(MonthlyCharges)) + geom_density(aes(group=Churn, colour=Churn, fill=Churn), alpha=0.1) +xlab("Monthly charges")
# customers paying less than 30/month tend to stay
# those paying more than 60/month tend to leave more
```

```{r}
# Categorical vs categorical

# Three views:
table(training$Churn, training$Contract)

ggplot(training, aes(x=Churn,fill = Contract)) + geom_bar()

ggplot(training, aes(x=Contract,fill = Churn)) + geom_bar()
# a long-term contract (1-2 years) decreases chance of leaving

# More categorical vs categorical

ggplot(training, aes(x=Churn,fill = OnlineSecurity)) + geom_bar()

ggplot(training, aes(x=Churn,fill = InternetService)) + geom_bar()

# easy ways to see relationships with categorical variables

```

```{r}
ggcorr(training, label = T)
```

There are some variables with high correlations; should skip them

```{r, include=F}
training$TotalCharges = NULL
testing$TotalCharges = NULL
```


# Advanced linear classifiers

We have many predictors, hence we can use regularized LDA or penalized logistic regression

```{r}
ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE, 
                     verboseIter=T)

# We have many predictors, hence use penalized logistic regression
rdaFit <- train(Churn ~ ., 
               method = "rda",
               tuneGrid = expand.grid(gamma = seq(0, 1, 0.2), lambda = seq(0, 1, .2)),
               metric = "Kappa",
               data = training,
               preProcess = c("center", "scale"),
               trControl = ctrl)
print(rdaFit)
rdaPred = predict(rdaFit, testing)
confusionMatrix(rdaPred, testing$Churn)

# We have many predictors, hence use penalized logistic regression
lrFit <- train(Churn ~ ., 
               method = "glmnet",
               tuneGrid = expand.grid(alpha = seq(0, 1, 0.1), lambda = seq(0, .1, 0.02)),
               metric = "Kappa",
               data = training,
               preProcess = c("center", "scale"),
               trControl = ctrl)
print(lrFit)
lrPred = predict(lrFit, testing)
confusionMatrix(lrPred, testing$Churn)
```

Accuracy around 80%, but not the best performance measure here. Why?

Kappa around 0.46, not bad but it should be improved

### Variable importance

```{r}
rda_imp <- varImp(rdaFit, scale = F)
plot(rda_imp, scales = list(y = list(cex = .95)))

lr_imp <- varImp(lrFit, scale = F)
plot(lr_imp, scales = list(y = list(cex = .95)))
```

### Bayes' rule

To reduce false negatives, let's decrease the optimal threshold 

```{r}
threshold = 0.3
rdaProb = predict(rdaFit, testing, type="prob")
rdaPred = rep("No", nrow(testing))
rdaPred[which(rdaProb[,2] > threshold)] = "Yes"
confusionMatrix(factor(rdaPred), testing$Churn)
```

Note the trade-off between false negatives and false positives is much better, but also note accuracy and kappa have been worsened

# Cost-sensitive learning

Features increasing chances of leaving:

- Tenure (especially < 12 Months)
- Internet Service = Fiber Optic
- Payment Method = Electronic Check

Features decreasing chances of leaving:

- Contract = two ear
- Total/monthly charges 

Accuracy is ok, around 80%. But are the two errors equally important?

The company will be concerned with balancing: 

- the cost of a customer who is leaving and has not been targeted, 

- the cost of inadvertently targeting customers that are not planning to leave 

Usually, the first cost (associated with false negatives) is the most dangerous for the company

Hence, how can we reduce that cost (at the expense of increasing the other cost)?

Assume the following (company's data):

- Cost of true negatives is 0: the model is correctly identified a happy customer, no need to offer discounts

- Cost of false negatives is 500 (customer value): most problematic error, we lose the customer 

- Cost of false positives is 100: retention incentive

- Cost of true positives is 140: (1-gamma)\*(customer value) + gamma\*(retention incentive)

where gamma=probability a custommer accepts the incentive/offer, 0.9 in our case

Cost matrix:

| Prediction/Reference | no | yes  |
| -------------------- | -----:| ---------:|
| predicted no                |   0 |  500  |
| predicted yes             |   100 |     140  |

Unit cost is then:

0\*TN + 100\*FP + 500\*FN + 140\*TP

```{r}
cost.unit <- c(0, 100, 500, 140)
```

### Cost of Naive classifier

Unit cost for Naive classifier (no analytics knowledge)

cost = 0\*.74 + 100\*0 + 500\*.26 +  + 140\*0
      = 130eur/customer on average

Savings = (retention incentive - cost)

in the case of the naive classifier, the savings = -30eur/customer, i.e. a loss

Savings from the regularized LDA (that needs analytics knowledge)

```{r}
CM = confusionMatrix(factor(rdaPred), testing$Churn)$table
cost = sum(as.vector(CM)*cost.unit)/sum(CM)
cost
```

Savings per customer around 20 eur, i.e. 50 eur better than the naive

## Cost-sensitive classifier (expert level) 

```{r}
cost.i = matrix(NA, nrow = 50, ncol = 10)
# 50 replicates for training/testing sets for each of the 10 values of threshold

j <- 0
for (threshold in seq(0.05,0.5,0.05)){
  
  j <- j + 1
  cat(j)
  for(i in 1:50){
    
    # partition data intro training (75%) and testing sets (25%)
    d <- createDataPartition(training$Churn, p = 0.8, list = FALSE)
    # select training sample
    
    train <- churnData[d,]
    test  <- churnData[-d,]  
    
    rdaFit <- train(Churn ~ ., data=train, method = "rda",
                   tuneGrid = data.frame(gamma = 0.2, lambda = 0.6), preProcess = c("center", "scale"),
                   trControl = trainControl(method = "none", classProbs = TRUE))
    
    rdaProb = predict(rdaFit, test, type="prob")
    rdaPred = rep("No", nrow(test))
    rdaPred[which(rdaProb[,2] > threshold)] = "Yes"
    
    CM = confusionMatrix(factor(rdaPred), test$Churn)$table
    cost = sum(as.vector(CM)*cost.unit)/sum(CM)
    cost
    
    cost.i[i,j] <- cost
    
  }
}

# Threshold optimization:
boxplot(cost.i, main = "Threshold selection",
        ylab = "unit cost",
        xlab = "threshold value",
        names = seq(0.05,0.5,0.05),col="royalblue2",las=2)

# values around 0.2 are reasonable
apply(cost.i, 2, median)
```

Savings around 20eur/customer on average 

Can you imagine the savings with just 100,000 customers?

Final prediction:

```{r}
threshold = 0.2
rdaFit <- train(Churn ~ ., data=training, method = "rda",
               tuneGrid = data.frame(gamma = 0.2, lambda = 0.6), preProcess = c("center", "scale"),
               trControl = trainControl(method = "none", classProbs = TRUE))
rdaProb = predict(rdaFit, testing, type="prob")
rdaPred = rep("No", nrow(testing))
rdaPred[which(rdaProb[,2] > threshold)] = "Yes"
CM = confusionMatrix(factor(rdaPred), testing$Churn)$table
cost = sum(as.vector(CM)*cost.unit)/sum(CM)
cost
```


# Risk learning

There are customers who provide more income to the company than others

Hence, we should focus on them if their churn probability is high

Predicted probabilities by real churn status

```{r}
data.frame(pred_Yes = rdaProb[,2], obs=testing$Churn) %>% 
  ggplot(aes(x=pred_Yes, fill=obs, color=obs)) +
  geom_histogram(bins=40, alpha=0.5) +
  theme_minimal()+
  labs(title="Predicted probabilities by real churn status")
```

But are all the clients equally important?

Money at risk:

```{r}
Risk.customer.year = testing$MonthlyCharges*rdaProb[,2]*12
data.frame(Risk.customer.year = Risk.customer.year, obs=testing$Churn) %>% 
  ggplot(aes(x=Risk.customer.year, fill=obs, color=obs)) +
  geom_histogram(bins=30, alpha=0.5) +
  theme_minimal()+
  labs(title="Money at risk by real churn status")
```

We should focus on customers with money.at.risk > 700 eur

```{r}
sum(Risk.customer.year>700)/length(Risk.customer.year)
```

Around 20% of customers would incur high losses, but who are they?

```{r}
sort.risk = sort(Risk.customer.year,decreasing=T,index.return=T)

# Most risky positions:
head(sort.risk$x)
# Most risky customers:
head(sort.risk$ix)
```

We can then offer risky customers a discount or better service, etc.

# From caret to tidymodels

**tidymodels:** ecosystem of packages to implement efficient ML workflows

It contains the following packages:

- rsample provides infrastructure for efficient data splitting and resampling

- parsnip is a tidy, unified interface to models independent of the particular package syntax

- recipes is a tidy interface to data pre-processing tools for feature engineering.
workflows bundle your pre-processing, modeling, and post-processing together

- tune optimizes the hyperparameters

- yardstick provides model performance metrics

- broom converts the information in common statistical R objects into user-friendly tidy formats

- dials creates and manages tuning parameters and parameter grids.

More powerful but difficult to use than caret

## ML workflows

Standard workflow:

- Split dataset in training & test sample

This is done with the rsample function initial_split()

- Apply preprocessing steps if necessary

Can be done manually, but better by defining a recipe

- Define the models to fit

Done by setting up a model structure with parsnip

- Define a resampling strategy

We choose among diferent resampling options with the rsample package

- Tune Hyperparameters

Here we use the tune package to tune hyperparameters and the dials package to manage the hyperparameter search

- Select the best performing hyperparameter setup

Fit the final model

- Evaluate it on the test data

## The package

```{r}
library(tidymodels)
```

## Splitting

```{r}
churnData <- read.csv('ChurnData.csv', stringsAsFactors=T)

data_split <- initial_split(churnData, prop = 0.7, strata = Churn)

training <- data_split  %>%  training()
testing <- data_split %>% testing()
```

## Pre-processing: the recipe

```{r}
library(themis) # for subsampling

churn_recipe <- training %>%
  recipe(Churn ~ .) %>%
  update_role(customerID, new_role = "ID") %>%
  step_rm(customerID) %>%
  #  step_naomit(all_outcomes(), all_predictors()) %>%
  step_impute_knn(all_predictors()) %>% #  knn inputation of missing values
  step_discretize(tenure, options = list(cuts = 5)) %>% 
  step_log(TotalCharges) %>%
  step_nzv(all_numeric(), -all_outcomes()) %>% # removes predictors with near zero variability
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_interact(terms = ~ TotalCharges:Contract_Two.year + TotalCharges:tenure_bin1) %>% # interactions
  step_lincomb(-all_outcomes()) %>% # removes predictors that are linear combinations of others
  #  step_rose(Churn) %>% # subsampling
  step_normalize(all_numeric(), -all_outcomes()) 

churn_recipe

View(juice(prep(churn_recipe)))
```

## The models and engines

LDA in high dimension:

```{r}
library(discrim)

# klaR
model_lda <-
  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %>%
  set_engine('klaR')

```


## The workflow

```{r}
wfl <- workflow() %>%
  add_model(model_lda) %>%
  add_recipe(churn_recipe) 
```


## Resampling and tuning

In this case, 5-fold cross validation

```{r}
# train-validation splits
cv_splits <- vfold_cv(training, v=5, strata = Churn)
```

Then, tune the hyper-parameters using cross-validation (with training data)

```{r}
lda_tune <- wfl %>%
  tune_grid(resamples = cv_splits,
         #   grid = expand_grid(frac_common_cov = seq(0.5, 1, by = 0.1), frac_identity = seq(0.1, 5, by = 0.1)),
            grid = 25, # automatic, try 25 values in the grid
            metrics = metric_set(accuracy, kap, roc_auc),
            control = control_grid(save_pred = TRUE, verbose = TRUE)) 

lda_tune %>%
  collect_metrics()

lda_tune %>%
  autoplot()
```

## Select the best model

Choose a performance measure

```{r}
# best_lda <- select_best(lda_tune, metric = "accuracy")
# best_lda <- select_best(lda_tune, metric = "roc_auc")
best_lda <- select_best(lda_tune, metric = "kap")
```

## The final workflow

```{r}
wfl_final <- 
  wfl %>%
  finalize_workflow(best_lda) %>%
  fit(data = training)

wfl_final %>% 
  extract_fit_parsnip() 
```

## Predictions

```{r}
probs <- 
  predict(wfl_final, type = "prob", new_data = testing) %>%
  bind_cols(obs = testing$Churn) %>%
  bind_cols(predict(wfl_final, new_data = testing))

probs %>% ggplot(aes(x=.pred_Yes, fill=obs, color=obs)) +
  geom_histogram(bins=40, alpha=0.5) +
  theme_minimal()+
  labs(title="Predicted probabilities by real churn status")
```

## Performance measures

With a probability threshold = 0.50

```{r}
conf_mat(probs, obs, .pred_class)
summary(conf_mat(probs, obs, .pred_class))
```

ROC

```{r}
autoplot(roc_curve(probs, obs, .pred_Yes, event_level = "second"))
roc_auc(probs, obs, .pred_Yes, event_level = "second")
```

## Probability threshold

```{r}
library(probably)

# threshold is associated with probabilities of the first level in target
probs %>%
  mutate(.pred = make_two_class_pred(.pred_No, levels(obs), threshold = .7)) %>%
  conf_mat(obs, .pred)
```



## Risk learning

```{r}
probs <- probs %>%
  bind_cols(Risk.customer.year = testing$MonthlyCharges*probs$.pred_Yes*12) 
```

Money at risk:

```{r}
probs %>% ggplot(aes(x=Risk.customer.year, fill=obs, color=obs)) +
  geom_histogram(bins=40, alpha=0.5) +
  theme_minimal()+
  labs(title="Money at risk by real churn status")
```











