---
title: "Ejercicio práctico desarrollo modelo de credit scoring"
author: "Daniel Fuentes "
date: "Mayo 2022"
output:
  word_document: default
  html_document: default
  pdf_document: default
---



## R Markdown

EJERCICIO 1: EXTRACCIÓN Y VALIDACIÓN DE LOS DATOS

El primer paso en cualquier tipo de modelo de scoring o de rating es conocer bien los datos de los que dispones. Conocer bien el dataset sobre el que se trabaja puede determinar que un modelo sea útil y con buen poder predictivo o no.

En este caso tenemos una base de datos de préstamos obtenido en el siguiente enlace:

https://www.lendingclub.com/info/download-data.action


Se trata de una Fintech que concede préstamos a través de dinero de inversores. Cada solicitud es evaluada y catalogada en diferentes categorías de riesgo. Los inversores recibirán un mayor o menor interés en función de como de riesgosa sea la cartera de préstamos en los que invierta.

<iframe width="560" height="315" src="https://www.youtube.com/watch?v=EMpQvwxQ57U" frameborder="0" allowfullscreen></iframe>


<iframe width="560" height="315" src="https://www.youtube.com/watch?v=8_5w8yCAqDA
" frameborder="0" allowfullscreen></iframe>


Además proveen de un excel con el significado de cada variable.

El dataset era de un tamaño considerable (600Mb) y ha sido reducido para que podáis trabajar más fácilmente y rápido a modo de ejemplo.

En la mayor parte de cursos de scoring podréis ver como se centran en lo que es el propio algoritmo: regresión logística, árboles de decisión, random forest..., lanzar este tipo de modelos es fácil pero no lo más importante, sobre todo desde el punto de vista práctico, donde un analista de crédito o usuario del modelo debe comprender el modelo y estar conforme o cómodo con él.
Vamos a preparar y cargar todas las librerías necesarias para poder ejecutar todo el código que os mostramos a continuación:


```{r, echo="FALSE",results='hide',message=F, warning=F}
# if(!is.element("plyr", installed.packages()[, 1])) install.packages("plyr")
# if(!is.element("lattice", installed.packages()[, 1])) install.packages("lattice")
# if(!is.element("plotly", installed.packages()[, 1])) install.packages("plotly")
# if(!is.element("ggplot2", installed.packages()[, 1])) install.packages("ggplot2")
# if(!is.element("Hmisc", installed.packages()[, 1])) install.packages("Hmisc")
# if(!is.element("sqldf", installed.packages()[, 1])) install.packages("sqldf")
# if(!is.element("zoo", installed.packages()[, 1])) install.packages("zoo")
# if(!is.element("caTools", installed.packages()[, 1])) install.packages("caTools")
# if(!is.element("RcmdrMisc", installed.packages()[, 1])) install.packages("RcmdrMisc")
# if(!is.element("smbinning", installed.packages()[, 1])) install.packages("smbinning")
# if(!is.element("DBI", installed.packages()[, 1])) install.packages("DBI")
# if(!is.element("vcd", installed.packages()[, 1])) install.packages("vcd")
# if(!is.element("ROCR", installed.packages()[, 1])) install.packages("ROCR")
# if(!is.element("corrplot", installed.packages()[, 1])) install.packages("corrplot")
# if(!is.element("partykit", installed.packages()[, 1])) install.packages("partykit")
# if(!is.element("rpart", installed.packages()[, 1])) install.packages("rpart")
# if(!is.element("stringr", installed.packages()[, 1])) install.packages("stringr")
# if(!is.element("lubridate", installed.packages()[, 1])) install.packages("lubridate")
# if(!is.element("plyr", installed.packages()[, 1])) install.packages("plyr")
# if(!is.element("dplyr", installed.packages()[, 1])) install.packages("dplyr")
# if(!is.element("leaps", installed.packages()[, 1])) install.packages("leaps")
# if(!is.element("randomForest", installed.packages()[, 1])) install.packages("randomForest")
# if(!is.element("fBasics", installed.packages()[, 1])) install.packages("fBasics")
# if(!is.element("tabplot", installed.packages()[, 1])) install.packages("tabplot")
# if(!is.element("dataMaid", installed.packages()[, 1])) install.packages("dataMaid")
# if(!is.element("smbinning", installed.packages()[, 1])) install.packages("smbinning")
# if(!is.element("RDCOMClient", installed.packages()[, 1])) install.packages('RDCOMClient', repos = 'http://www.stats.ox.ac.uk/pub/RWin/')
# if(!is.element("ggcorrplot", installed.packages()[, 1])) install.packages("ggcorrplot")
# if(!is.element("Amelia", installed.packages()[, 1])) install.packages("Amelia")
# if(!is.element("ggplot", installed.packages()[, 1])) install.packages("ggplot")
# if(!is.element("randomForest", installed.packages()[, 1])) install.packages("randomForest")
# if(!is.element("ada", installed.packages()[, 1])) install.packages("ada")
# if(!is.element("patchwork", installed.packages()[, 1])) install.packages("patchwork")
# if(!is.element("DataExplorer", installed.packages()[, 1])) install.packages("DataExplorer ")



library(smbinning)
library(corrplot)
library(Hmisc)
library(sqldf)
library(zoo)
library(caTools)
library(RcmdrMisc)
library(ROCR)
library(vcd)
library(ggplot2)
library(plotly)
library(lattice)
library(plyr)
library(partykit)
library(rpart)
library(stringr)
library(lubridate)
library(plyr)
library(dplyr)
library(leaps)
library(randomForest)
library(fBasics)
library(dataMaid)
library(DescTools)
library(ggcorrplot)
library(Amelia)
library(DataExplorer)
library(ggplot2)
library(lubridate)
```


Vamos a cargar la base de datos que hemos preparado y analizar todas las variables que tenemos disponibles:


```{r}

# load("Database_LoansAFI.RData")

library(readxl)
Loans <- read_excel("C:/Users/danie/Desktop/AFI 2022/1-CreditScoring/Loans.xlsx")
colnames(Loans)
dim(Loans)

```


Como puede observarse tenemos 26 variables, hemos filtrado el dataset global también para que vieráis cuáles son las variables más comunes en un modelo de scoring.
Para conocer que es cada una de ellas podáis consultar el excel diccionario que también os paso. De las que tenemos en este dataset, estas son las definiciones:

 __**id:**__ A unique LC assigned ID for the loan listing.  
 __**loan_amnt:**__ The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.  
 __**term:**__ The number of payments on the loan. Values are in months and can be either 36 or 60.  
 __**int_rate:**__ Interest Rate on the loan  
 __**installment:**__ The monthly payment owed by the borrower if the loan originates.
 __**grade:**__ LC assigned loan grade
 __**emp_title:**__ The job title supplied by the Borrower when applying for the loan.*
 __**emp_length:**__ Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. 
 __**home_ownership:**__ The home ownership status provided by the borrower during registration or obtained from the credit report.Our values are: RENT, OWN, MORTGAGE, OTHER
 __**annual_inc:**__ The self-reported annual income provided by the borrower during registration.
 __**verification_status:**__ Indicates if income was verified by LC, not verified, or if the income source was verified
 __**issue_d:**__ The month which the loan was funded
 __**loan_status:**__ Current status of the loan
 __**desc:**__ Loan description provided by the borrower
 __**purpose:**__ A category provided by the borrower for the loan request. 
 __**addr_state:**__ The state provided by the borrower in the loan application
 __**dti:**__ A ratio calculated using the borrower's total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower's self-reported monthly income.
__**delinq_2yrs:**__ The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years
__**earliest_cr_line:**__ The month the borrower's earliest reported credit line was opened
__**Fico_score:**__the borrower's FICO score at loan origination belongs to.
__**inq_last_6mths:**__The number of inquiries in past 6 months (excluding auto and mortgage inquiries)
__**mths_since_last_delinq:**__ The number of months since the borrower's last delinquency.
__**mths_since_last_record:**__ The number of months since the last public record.
__**open_acc:**__ The number of open credit lines in the borrower's credit file.
__**revol_bal:**__ Total credit revolving balance
__**revol_util:**__ Revolving line utilization rate, or the amount of credit the borrower is 

Analicemos la estructura de las variables del modelo y vamos estudiando poco a poco cada una de ellas:

```{r, echo=FALSE}
str(Loans)
```

Como podéis obsevar hay variables numéricas y categóricas y precisamente su tratamiento deberá ser realizado de manera diferente. Ya se dejan ver algunas variables que tienen algunos valores missing y podemos ver cuál es nuestra variable target o la que queremos predecir en nuestros préstamos: loan_status.

Veamos los 5 primeros registros del dataset para seguir haciéndonos una idea de la información con la que contamos:

```{r}
head(Loans,n=5)
```

Lo mismo podría hacerse para ver los últimos registros del dataset

```{r}
tail(Loans,n=7)

```

Se tendrán que ir analizando variable a variable como se comportan, como se encuentran en el dataset y modifarlas si procede: 

```{r}

#Clases de cada variable
Class<-as.data.frame(sapply(Loans, class))
colnames(Class)<-"variable_class"
Class$variable_name<-row.names(Class)
row.names(Class)<-NULL

Class


```

Algunas variables aparecen como de tipo integer, para evitar problemas posteriores las vamos a convertir en formato numéricas:

```{r}
Class[Class$variable_class=="integer",]

for (i in 1:ncol(Loans)){
  
  if (class(Loans[,i])=="integer"){
    
    
    Loans[,i]<-as.numeric(Loans[,i])
    
  }
  
}

```


Vamos a ir analizando ahora variable a variable las variables categóricas y haciendo trasnformaciones si es necesario:


```{r}
Class[Class$variable_class=="character",]

```

Empecemos con la variable plazo del préstamo:


```{r}
table(Loans$term)

```

Esta variable podría convertirse a numérica pero dado que solamente toma dos valores la vamos a mantener así.
Veamos la siguiente variable: int_rate (tipo de interés del préstamo):


```{r}
head(Loans$int_rate)

```

La variable es numérica pero aparece como categórica, vamos a transformarla:

```{r}
Loans$int_rate<-as.numeric(gsub("%","",Loans$int_rate))/100

summary(Loans$int_rate)
```

Variable "grade" (Categoria de riesgo que asigna LendingClub a cada préstamo):


```{r}

table(Loans$grade)

```

La categoría A corresponde a préstamos de solicitantes con mejor perfil de pago o más solventes, siendo la G la categoría de préstamos más riesgosa:

Más variables que están disponibles: Descripción del puesto que ocupa el solicitante en la empresa:

```{r}
head(table(Loans$emp_title))

print("La variable tiene ")
(length(table(Loans$emp_title)))
print("valores diferentes.")
```

Variable "emp_length" o antigüedad en el empleo:

```{r}
table(Loans$emp_length)
```


Vamos a transformar la variable en numérica. Si el valor que toma es "< 1 year" pondremos un 0 y si es mayor que 10 se convertira en 10 años.

```{r}
Loans$emp_length<-gsub("years","",Loans$emp_length)
Loans$emp_length<-gsub("year","",Loans$emp_length)
Loans$emp_length<-gsub("< 1","0",Loans$emp_length)
Loans$emp_length<-gsub("[+]","",Loans$emp_length)
Loans$emp_length<-as.numeric(Loans$emp_length)

summary(Loans$emp_length)
```

Más variables:

"home_ownership": Situación de la vivienda habitual

```{r}
table(Loans$home_ownership)
```

"verification_status": Si los ingresos declarados por el solicitante han sido verificados o no

```{r}
table(Loans$verification_status)
```


"purpose": Destino del préstamo

```{r}
table(Loans$purpose)
```

"addr_state": Estado de residencia del solicitante, los datos son de solicitudes en Estados Unidos.

```{r}
table(Loans$addr_state)
```

Con la variable "revol_util" o porcentaje de las líneas de crédito que tiene utilizadas, ocurre lo mismo que con la variable tipo de interés. Convertimos de igual forma la variable en numérica:


```{r}
head(Loans$revol_util)

Loans$revol_util<-as.numeric(gsub("%","",Loans$revol_util))/100

summary(Loans$revol_util)


```

La variable "issue_d" es la fecha en la que el préstamo fue concedido, como está en formato carácter, vamos a convertirla en formato fecha:

```{r}
head(Loans$issue_d)
tail(Loans$issue_d)
unique(Loans$issue_d)

```

```{r}

Loans$issue_d<-ymd(paste(Loans$issue_d,"-01",sep=""))

unique(Loans$issue_d)

class(Loans$issue_d)
```


También la variable "earliest_cr_line" o fecha en la que el solicitante abrió su primera cuenta se encuentra en formato texto. La transformamos de igual forma a formato fecha:

```{r}
head(Loans$earliest_cr_line)

Loans$earliest_cr_line<-dmy(paste("01-",Loans$earliest_cr_line,sep=""))


head(Loans$earliest_cr_line)

save.image("Backup1.RData")
```

Con esta variable, vamos a calcular la antigüedad del cliente en la entidad, para ello restaremos las fechas de concesión de solicitud del préstamo con la fecha que acabamos de obtener. La variable antigüedad como cliente estará expresada en años:

```{r}
Loans$ant_cliente<-as.numeric(round((Loans$issue_d-Loans$earliest_cr_line)/365,0))

summary(Loans$ant_cliente)
table(Loans$ant_cliente)


```

Podemos ver que hay algunos registros que aparecen con un valor negativo, parece que se debe a algún tipo de error. Vemos también que la variable parece tener muchos valores no informados. Los valores negativos serán remplazados por un valor 0:

```{r}

Loans$ant_cliente<-ifelse(Loans$ant_cliente<0,0,Loans$ant_cliente)

summary(Loans$ant_cliente)

```

Vamos a crear otra variable relacionada cn esta: una marca que nos indique si el solicitante es cliente anteriormente o no:

```{r}

Loans$flag_cliente<-ifelse(Loans$ant_cliente>0,1,0)

Loans$flag_cliente<-ifelse(is.na(Loans$flag_cliente),0,1)

summary(Loans$flag_cliente)

```










EJERCICIO 2: DEFINICIÓN DE PERFORMANCE Y MUESTRAS


Normalmente la variable target suele ser la marca de default, o lo que es lo mismo, una variable que toma dos valores: 0 si el cliente pagó su préstamo o 1 en el caso de que fuera un cliente moroso.

Se suele mirar si un cliente presento problemas de pago en un periodo de tiempo concreto, lo más común es marcar a un cliente como default si tuvo incumplimientos en las cuotas de sus préstamos en más de 90 días (definición regulatoria) durante un periodo de un año o superior.

Vamos a ver qué valores toma nuestra variable target:


```{r}
table(Loans$loan_status)

```

Al parecer, de 39726 préstamos de la base de datos, 5.671 presentaron problemas de pago:


```{r}
tmp = Loans %>% group_by(loan_status) %>% summarise(ncount = n())
tmp$ncount = 100 * tmp$ncount/nrow(Loans)
tmp$ncount_p = str_c(round(tmp$ncount,2),"%")
ggplot(tmp,aes(x=loan_status,y=ncount,fill=loan_status)) + geom_bar(stat="identity") +
geom_text(aes(label=ncount_p),vjust = 2)
```

Como vemos, esto supone que el 14.25% de los préstamos presentaron problemas de pago. Vamos a transformar los valores Default y FullyPaid en valores numéricos. Así si un préstamo presenta problemas será marcado como un 1, un 0 en caso contrario:

```{r}
Loans$loan_status<-ifelse(Loans$loan_status=="Default",1,0)
```

Nos queda una última variable categórica: "desc", que no hemos analizado. En esta variable se detalla en texto libre los comentarios que introduce el solicitante en la aplicación/web cuando solicitan el préstamo, aclarando el objeto del préstamo, más allá de la categoría genérica que queda recogida en la variable "purpose":

```{r}
head(Loans$desc)
```

Esta variable no será utilizada.

Respecto a las variables numéricas, podemos hacer un análisis similar observando que valores toman las variables:

```{r}

#Clases de cada variable
Class<-as.data.frame(sapply(Loans, class))
colnames(Class)<-"variable_class"
Class$variable_name<-row.names(Class)
row.names(Class)<-NULL


numeric_vars<-Class[Class$variable_class=="numeric","variable_name"]
numeric_vars
categorical_vars<-Class[Class$variable_class=="character","variable_name"]
```


Con el paquete fBasics podemos obtener rápidamente descriptivos básicos de las variables numéricas:

```{r}
library(fBasics)

descriptives_num<-as.data.frame(t(basicStats(Loans[,numeric_vars])))

descriptives_num
```


```{r}
plot_histogram(Loans[,numeric_vars])
```


Una vez tenemos todas las variables transformadas como queremos, podemos realizar un resumen de todas ellas:

```{r}

describe(Loans[,c(numeric_vars,categorical_vars)])

```

Un mejor análisis puede ser llevado a cabo y además guardado en un documento Word:

```{r}

wrd<-GetNewWrd()

Desc(Loans[,c(numeric_vars,categorical_vars)],plotit = T,wrd=wrd)
```



Vamos a ver algunos gráficos de evolución de los préstamos que tenemos disponibles en la base de datos. Por ejemplo, la evolución de la tasa de defaults a lo largo del tiempo:

```{r}

Hist_defaults<-sqldf("SELECT issue_d,
     
      count(loan_status) as N,
      
      sum(loan_status) as Defaults,
                     
      avg(loan_status) as ODF
     
      from Loans group by issue_d")


plot(Hist_defaults$issue_d, Hist_defaults$ODF, main="Historical ODF",xlab="Date",ylab="ODF")
lines(Hist_defaults$issue_d, Hist_defaults$ODF) 


plot(Hist_defaults$issue_d, Hist_defaults$Defaults, main="Historical Defaults",xlab="Date",ylab="Defaults")
lines(Hist_defaults$issue_d, Hist_defaults$Defaults)

```

Tenemos dos cosas aqui:
- Por un lado, la tasa de default ha permanecido más o menos constante, ¿verdad? 
- Pero por otro lado vemos que los defaults han aumentado mucho. ¿Qué ha podido suceder? 


Vamos entonces a ver también la evolución en el número de préstamos:


```{r}

evolucion_prestamos<-sqldf("SELECT issue_d,
     
      count(loan_amnt) as Num_prestamos
     
      from Loans group by issue_d")
      
library(lattice)
xyplot(Num_prestamos  ~ issue_d, data=evolucion_prestamos,ylab="N?mero de pr?stamos", xlab="Date",auto.key = TRUE)


```

Vemos que aunque el número de defaults haya aumentado en gran cantidad, el número de préstamos concedidos también lo ha hecho, manteniendo un número de defaults constante debido a este motivo. 





También la evolución del importe medio concedido:

```{r}

evolucion_importe<-sqldf("SELECT issue_d,
     
      avg(loan_amnt) as Average_amount
     
      from Loans group by issue_d")


plot(evolucion_importe$issue_d, evolucion_importe$Average_amount, main="Historical amount",xlab="Date",ylab="Average Amount")
lines(evolucion_importe$issue_d, evolucion_importe$Average_amount) 
```

Ahora vamos a ver si tenemos las variables lo suficientemente informadas y si nos interesa mantener todas las variables por este motivo. Primero calculamos el número de registros missing por variable:

```{r}
data=Loans
ncol=rep(nrow(data) ,each=ncol(data))
nmsg=as.integer(as.character(as.vector(apply(data, 2, function(x) length(which(is.na(x)))))))
missingdata=as.data.frame(cbind(colnames=names(data),ncol,nmsg))
# missingdata$nmsg=as.numeric(levels(missingdata$nmsg))[missingdata$nmsg]
missingdata$nmsg<-as.numeric(missingdata$nmsg)
missingdata=cbind(missingdata,percmissing=(missingdata$nmsg/ncol*100))

print(nmsg)
print(missingdata)
```

Vamos a ver las variables que tengan más de un 10% de valores missing:

```{r}

print(missingdata[missingdata$percmissing>10,])

```


Hay que tratar de analizar los missing de las variables, ¿pueden aportar algo de información sobre la muestra? ¿Los registros vacios se comportan mejor o peor que la media de defaults de la cartera?.
La primera de las variables (earliest_cr_line), nos sirvió de ayuda para crear la variable ant_cliente. El número de missings coincide por este motivo. La variable earliest_cr_line ya no nos sirve de utilidad realmente:

```{r}
Loans$earliest_cr_line<-NULL
```

Anteriormente vimos que la tasa de malos alcanzaba el 14.25%. ¿Cuál es la tasa de malos en la muestra cuando la variable "ant_cliente" viene a missing?:

```{r}
sqldf("SELECT 
      avg(loan_status) as FDO
     
      from Loans where ant_cliente is null")
```

Parece que es ligeramente superior y que los clientes con el valor a missing se comportan ligeramente peor que la media. Podríamos asignar el valor medio de la variable a los valores faltantes pero estaríamos perdiendo información. También podemos suponer que son clientes nuevos y sin antigüedad, pudiendo asignar un cero como valor.
Eliminar los registros no es muy buena opción, pues estaríamos suprimiendo el 29% de la base de datos.

Otra opción es borrar la variable. En este caso vamos a mantener la variable sin realizar nada sobre ella pues luego agruparemos la variable y crearemos un grupo con los valores missing.

Analicemos ahora la tasa de defaults en los casos faltantes de la variable "mths_since_last_delinq" en la que recordemos que había un 65% de registros missing:

```{r}
sqldf("SELECT 
      avg(loan_status) as FDO
     
      from Loans where mths_since_last_delinq is null")
```

En estos casos, los préstamos cuya variable tiene valores missing se comportaran mejor que la media de la cartera de nuestra muestra. Parece lógico pensar que valores missing podrían recoger aquellos préstamos cuyos solicitantes nunca han tenido atrasos en los pagos. Por lo tanto este tipo de "missing" puede ser realmente un dato a tener en cuenta

Por último, vemos la tasa de default en la variable "mths_since_last_record":

```{r}
sqldf("SELECT 
      avg(loan_status) as FDO
     
      from Loans where mths_since_last_record is null")
```

El racional viene a ser el mismo que la anterior variable. 
Otra manera de ver los valores missing:

```{r}
plot_missing(Loans) 
```

Nos puede interesar suprimir aquellos registros que tengan más de un porcentaje de valores missing en todas las variables. Se podrían tratar de solicitudes de préstamos erróneas y que pueden afectar a nuestro modelo:

```{r}

Loans$missingvalues<-rowSums(is.na(Loans[,2:27]))/26
hist(Loans$missingvalues,main="Distribution of missing values",xlab="Percentage of missing values",border="blue", col="red",breaks=10)


summary(Loans$missingvalues)

```

Suelen eliminarse aquellos registros con un % de valores missing superior al 30%. En este caso no eliminaremos observaciones.

```{r}
Loans$missingvalues<-NULL
```

Visualmente se pueden ver los valores faltantes con el siguiente paquete:

```{r}

missmap(Loans, main = "Missing values vs observed",col=c("black", "grey"),,legend = FALSE)

rm(list=setdiff(ls(), c("Loans","numeric_vars","categorical_vars")))

save.image("Backup2.RData")
```


Para construir el modelo vamos a partir nuestros datos en muestra de desarrollo y validación. El 70% de los datos se usarán para desarrollar el modelo y el 30% restante para validar el modelo sobre una muestra diferente a la de construcción.
Se pretende también que la selección aleatoria de la muestra mantenga la misma tasa de default en ambas muestras:


```{r}
set.seed(1234)
index = sample.split(Loans$loan_status, SplitRatio = .70)
train<-subset(Loans, index == TRUE)
test<-subset(Loans, index == FALSE)
```

Vamos a comprobar el tamaño de ambas muestras y la tasa de default en cada una de ellas:

```{r}
print("La muestra de entrenamiento tiene un total de:")
nrow(train)
print("y tiene una tasa de defaults de:")
mean(train$loan_status)


print("La muestra de validación tiene un total de:")
nrow(test)
print("y tiene una tasa de defaults de:")
mean(test$loan_status)


print("La muestra de entrenamiento tiene un tamaño del")
nrow(train)/nrow(Loans)

```

Hasta ahora hemos descrito los datos pero vamos a hacer un modelo que relacione esas variables con el default. ¿Como es esa relación de cada variable con la variable de desempeño de los préstamos?. Vamos a hacer un análisis gráfico inicial de cada variable:



```{r}

trends<-function(variable,label){


temp <- train[,c(variable,"loan_status")]

if (class(temp[,1])=="numeric") {

temp$quartile <- as.numeric(cut2(temp[,1], g=6))   #as.numeric to number the factors
temp$quartileBounds <- cut2(temp[,1], g=6)
temp$Interval <- paste(temp$quartile," - ",temp$quartileBounds,sep='')

results<-sqldf("SELECT  Interval,
     
      count(loan_status) as N,
      
      sum(loan_status) as Defaults,
                     
      avg(loan_status) as ODF
     
      from temp group by Interval")


library(ggplot2)
# Basic line plot with points
ggplot(data=results, aes(x=Interval, y=ODF, group=1)) +
  geom_line()+
  geom_point() +   labs(title="Trend analysis", 
    subtitle=paste(label,sep=''),
         x=paste(variable, "values",sep=' '),
         fill="loan_status")

} else {
  
temp <- train[,c(variable,"loan_status")]

colnames(temp)[1]<-"var"

results<-sqldf("SELECT  var,
     
      count(loan_status) as N,
      
      sum(loan_status) as Defaults,
                     
      avg(loan_status) as ODF
     
      from temp group by  var")

results<-sqldf("SELECT  *
     
      from results group by  ODF")

library(ggplot2)
# Basic line plot with points
ggplot(data=results, aes(x=var, y=ODF, group=1)) +
  geom_line()+
  geom_point() +   labs(title="Trend analysis", 
    subtitle=paste(label,sep=''),
         x=paste(variable, "values",sep=' '),
         fill="loan_status")


}

}

trends("loan_amnt","Importe prestamo")
trends("term","Plazo prestamo")
trends("int_rate","Tipo interes")
trends("installment","Cuota")
trends("grade","Calificacion crediticia")
trends("emp_title","Profesion")
trends("home_ownership","Situacion vivienda habitual")
trends("emp_length","Antiguedad empleo")
trends("annual_inc","Ingresos anuales")
trends("verification_status","Ingresos verificados")
trends("purpose","Destino prestamo")
trends("addr_state","State")
trends("dti","Cuota de esfuerzo")
trends("delinq_2yrs","Incidencias de m?s de 30 dias en pago en los dos ultimos años")
trends("Fico_score","FICO score")
trends("inq_last_6mths","N? de consultas en los ultimos 6 meses")
trends("mths_since_last_delinq","Meses desde ultimo incumpl.")
trends("mths_since_last_record","Meses desde ultimo registro.")
trends("open_acc","El n?mero de lineas de credito abiertas ")
trends("revol_bal","Saldo total del balance de credito")
trends("revol_util","% Usado limites")
trends("ant_cliente","Antiguedad del cliente")
trends("flag_cliente","Indicador cliente")


```

Otra manera de realizar el análisis es realizar gráficos de densidad para clientes "buenos" y "malos" en un único grafico para cada variable. Es decir, analizar gráficamente que valores toma cada variable para cada tipo de cliente. Vamos a hacerlo sobre las variables numéricas únicamente:


```{r}

library(ggplot2)
theme_set(theme_classic())


#loan_amnt

ggplot(train, aes((loan_amnt))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Importe prestamo",
         x=paste("loan_amnt values",sep=''),
         fill="loan_status")
         

#int_rate

ggplot(train, aes((int_rate))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Tipo interes",
         x=paste("int_rate values",sep=''),
         fill="loan_status")
         
#installment

ggplot(train, aes((installment))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Cuota",
         x=paste("installment values",sep=''),
         fill="loan_status")
         
#emp_length

ggplot(train, aes((emp_length))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Antig?edad en el empleo",
         x=paste("emp_length values",sep=''),
         fill="loan_status")

  
#anual_inc

ggplot(train, aes(log(annual_inc))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Ingresos anuales",
         x=paste("annual_inc values",sep=''),
         fill="loan_status")
       
#dti

ggplot(train, aes((dti))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Cuota de esfuerzo",
         x=paste("dti",sep=''),
         fill="loan_status")


#delinq_2yrs

ggplot(train, aes((delinq_2yrs))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Incidencias de mas de 30 dias en pago en los dos ultimos años",
         x=paste("delinq_2yrs",sep=''),
         fill="loan_status")


#FICO_score

ggplot(train, aes((Fico_score))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="FICO_score",
         x=paste("FICO_score",sep=''),
         fill="loan_status")



#inq_last_6mths

ggplot(train, aes((inq_last_6mths))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Nº de consultas en los ultimos 6 meses",
         x=paste("inq_last_6mths",sep=''),
         fill="loan_status")

#mths_since_last_delinq

ggplot(train, aes((mths_since_last_delinq))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Meses desde ultimo incumpl.",
         x=paste("mths_since_last_delinq",sep=''),
         fill="loan_status")

#mths_since_last_record

ggplot(train, aes((mths_since_last_record))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Meses transcurridos desde el ultimo registro publico",
         x=paste("mths_since_last_record",sep=''),
         fill="loan_status")

#open_acc

ggplot(train, aes((open_acc))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="El numero de lineas de credito abiertas",
         x=paste("open_acc",sep=''),
         fill="loan_status")


#revol_bal

ggplot(train, aes(log(revol_bal))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Saldo total del balance de credito",
         x=paste("revol_bal",sep=''),
         fill="loan_status")

#revol_util

ggplot(train, aes((revol_util))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="% credito utilizado",
         x=paste("revol_util",sep=''),
         fill="loan_status")



#ant_cliente

ggplot(train, aes((ant_cliente))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Antiguedad cliente",
         x=paste("ant_cliente",sep=''),
         fill="loan_status")

#flag_cliente

ggplot(train, aes((flag_cliente))) + geom_density(aes(fill=factor(loan_status)),alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Indicador cliente",
         x=paste("flag_cliente",sep=''),
         fill="loan_status")

save.image("Backup3.RData")

```
De los gráficos anteriores podemos ver como hay algunas variables como el Fico score o el tipo de interés que parecen separar mejor a los clientes buenos y malos.









EJERCICIO 3: AGRUPACIÓN DE VARIABLES


Vamos a medir analiticamente cuál es la capacidad o poder discriminante de cada variable.
Para usar el paquete que nos calcula el poder discriminante de las variables, es necesario considerar el default o el loan_status al revés de lo que lo habiamos hecho, es decir un 0 si el préstamo resulto impagado y 1 en caso contrario: 

```{r}

source("woe_functions.R")

train$Default2<-as.numeric(as.character(train$loan_status))
train$Default2<-as.numeric(ifelse(train$Default2==1,0,1))
test$Default2<-as.numeric(as.character(test$loan_status))
test$Default2<-as.numeric(ifelse(test$Default2==1,0,1))

```




Primero calculamos el poder predictivo de las variables numéricas:


```{r echo=FALSE, warning=FALSE,message=FALSE}
library(smbinning)
options(digits=2)
IV_numeric<-smbinning.sumiv(as.data.frame(train[,c("Default2",numeric_vars)]), "Default2")
print(IV_numeric)


```

El poder discriminante de cada variable teniendo en cuenta el IV se mide a través de la siguiente guía:

Information Value  Predictive Power
< 0.02:	useless for prediction
0.02 to 0.1:	Weak predictor
0.1 to 0.3:	Medium predictor
0.3 to 0.5:	Strong predictor
 >0.5	Suspicious or too good to be true

De acuerdo a los valores que hemos calculado:

```{r}
smbinning.sumiv.plot(IV_numeric, cex = 0.9)
```

La variable con más poder predictivo es el tipo de interés aplicable a los préstamos. Esta variable no suele ser empleada en este tipo de modelos de scoring pues es muy dependiente de la política de concesión de préstamos de la entidad, situación económica....

Cuando la función anterior devuelve el valor: "No significant splits", indica que la variable no permite discriminar nada los clientes con problemas en el pago de los que no, por lo tanto podremos descartar estas variables.

Una de las variables flag_cliente aparece con el mensaje "Uniques values of x < 5". El paquete no realizara el calculo en valores numericos con menos de 5 valores unicos, ya que entiende que deberia ser casi una variable categorica:

```{r}
table(train$flag_cliente)
```

Vamos a tratar esta variable, convirtiéndola en categórica:

```{r}

train$flag_cliente<-as.factor(train$flag_cliente)
test$flag_cliente<-as.factor(test$flag_cliente)

```


Ahora calcularemos en poder predictivo de las variables categóricas (estas variables recordad que las tenemos creadas ya en el Ejercicio 1 de Exploración de Datos -línea aprox 446-). 

Recordemos que tenemos una nueva variable categórica que acabamos de crear:

```{r}
categorical_vars<-c(categorical_vars,"flag_cliente")
```

Es necesario que las convirtamos en factor para el paquete:


NOTA IMPORTANTE: Para versiones de R anteriores a la 4.01 este código siguiente sí que funciona, En versiones posteriores da fallo así que lo haremos manualmente como se indica a continuación
```{r}
# 
# for (m in 1:length(categorical_vars)){
#   
#   var_name<-as.character(categorical_vars[m])
#   train[,var_name]<-as.factor(train[,var_name])
#   test[,var_name]<-as.factor(test[,var_name])
# }
#   

```


Para versiones de R posteriores a 4.1 este código deberá utilizarse en lugar del anterior, ya que da fallo al parecer. 
```{r}

#Train
train$term=as.factor(train$term)
train$grade=as.factor(train$grade)
train$emp_title=as.factor(train$emp_title)
train$home_ownership=as.factor(train$home_ownership)
train$verification_status=as.factor(train$verification_status)
train$desc=as.factor(train$desc)
train$purpose=as.factor(train$purpose)
train$addr_state=as.factor(train$addr_state)
train$flag_cliente=as.factor(train$flag_cliente)




#Test
test$term=as.factor(test$term)
test$grade=as.factor(test$grade)
test$emp_title=as.factor(test$emp_title)
test$home_ownership=as.factor(test$home_ownership)
test$verification_status=as.factor(test$verification_status)
test$desc=as.factor(test$desc)
test$purpose=as.factor(test$purpose)
test$addr_state=as.factor(test$addr_state)
test$flag_cliente=as.factor(test$flag_cliente)






  

```

NOTA: Para versiones superiores de R de 4.1 hay que referenciar la muestra train como as.data.frame() para que funcione el código. Como vemos mas abajo

Ahora calculemos el IV de estas variables:

```{r}
IV_cat<-matrix("NA",length(categorical_vars),2)
colnames(IV_cat)<-c("Char","IV")


for (o in 1:length(categorical_vars)){
  
  name<-as.character(categorical_vars[o])  
  aux<-as.data.frame(train[,c("Default2",name)])
  IV_cat[o,1]<-name
  IV_cat[o,2]<-as.numeric(as.character(iv_categorical(aux[,1],aux[,2])))
  
}

IV_cat<-as.data.frame(IV_cat)
IV_cat$IV<-as.numeric(as.character(IV_cat$IV))
print(IV_cat)
```

Vemos que hay tres variables con un valor "Inf". La variable "desc", como dijimos la obviamos. Para las variables  "addr_state" y "emp_title" lo que ocurre es que son muy granulares y dan problemas en el cálculo. Estas variables podrían convertirse en numéricas y tratarlas como tal.
Asignemos la tasa de default observada en cada valor de las variables y convirtamos las variables así en numericas. Empecemos con la variable indicativa del estado de residencia del solicitante:

```{r}
detail_state<-sqldf("SELECT addr_state,

      avg(loan_status) as ODF_addr_state
     
      from train group by addr_state")

detail_state$ODF_addr_state<-round(detail_state$ODF_addr_state,2)

detail_state<-detail_state[order(detail_state$ODF),]
detail_state
unique(detail_state$ODF_addr_state)
```

Asignemos la ODF de cada estado en las muestras:

```{r}
train<-merge(train,detail_state,by.x=c("addr_state"))

test<-merge(test,detail_state,by.x=c("addr_state"))

```

El valor del IV de esta nueva variable numerica es:

```{r}
smbinning.sumiv(train[,c("Default2","ODF_addr_state")], "Default2")
```

Tras obtener el IV vemos que la variable del Estado no aportaría mucha información al modelo. (IV<0,01 weak predictor), no obstante la mantendremos y analizaremos más adelante. 


La variable emp_title es tan granular y diferente por cada solicitante que preferimos no considerarla en el modelo.

Respecto a la variable "purpose", consideramos que también es muy granular y no queremos tener tantos grupos como categorías. Vamos a transformarla en variable numérica también a través de su ODF:

```{r}
detail_purpose<-sqldf("SELECT purpose,

      avg(loan_status) as ODF_purpose
     
      from train group by purpose")

detail_purpose$ODF_purpose<-as.character(round(detail_purpose$ODF_purpose,2))
detail_purpose$ODF_purpose<-as.factor(detail_purpose$ODF_purpose)

detail_purpose
```


```{r}
train<-merge(train,detail_purpose,by.x=c("purpose"))
test<-merge(test,detail_purpose,by.x=c("purpose"))
```

El nuevo IV de la variable purpose sería:

```{r}
smbinning.sumiv(train[,c("Default2","ODF_purpose")], "Default2")
```




El cálculo del IV se hace realizando un groupping de los valores de cada variable para encontrar cuál es la mejor separación entre clientes buenos y malos.
Esta agrupación es luego empleada en la modelización, nuestras variables del modelo ya no serán las variables con sus valores originales, si no las variables transformadas en categorías o grupos como hemos visto en la teoría.





Los grupos son creados automáticamente y de manera estadística tratando de discriminar lo mejor posible entre los diferentes tipos de clientes (buenos y malos). Sin embargo estas agrupaciones para los analistas deben tener sentido economico. 




Lo que se pretende ahora es ver si se cumple que los grupos que se crean automáticamente tienen sentido económico o hay que retocarles. Creamos la siguiente función para realizar el análisis:

```{r}
variables_analysis<-function(variable){
 


  if (class(train[,variable])=="numeric" | class(train[,variable])=="integer") {
  
  name<-as.character(variable)  
  aux<-train[,c("loan_status",name)]
  aux$loan_status<-as.numeric(ifelse(train$loan_status==1,0,1))
  groupping<-smbinning_calc(df=aux,y="loan_status",x=name,p=0.05)
  
  par(mfrow=c(2,2))
  boxplot(aux[,2]~aux[,1], 
  horizontal=T, frame=F, col="lightgray",main="Distribution") 
  mtext(name,3) 

  smbinning.plotty(groupping,option="dist",sub=name) 
  smbinning.plotty(groupping,option="badrate",sub=name)
  smbinning.plotty(groupping,option="WoE",sub=name)

  }
  else {
  
  name<-as.character(variable)  
  aux<-train[,c("loan_status",name)]
  aux$loan_status<-as.numeric(ifelse(train$loan_status==1,0,1))
  groupping<-smbinning.factor(df=aux,y="loan_status",x=name,maxcat = 8)
  
  par(mfrow=c(2,2))

  smbinning.plotty(groupping,option="dist",sub=name) 
  smbinning.plotty(groupping,option="badrate",sub=name)
  smbinning.plotty(groupping,option="WoE",sub=name)

  }
}

```

Vamos a realizar el analisis sobre las variables que hemos visto que son mas predictivas anteriormente:

int_rate
Fico_score
revol_util
annual_inc
loan_amnt
dti
ODF_addr_state
term
grade
ODF_purpose


```{r}
variables_analysis("int_rate")
variables_analysis("Fico_score")
variables_analysis("revol_util")
variables_analysis("annual_inc")

variables_analysis("dti")
variables_analysis("ODF_addr_state")
variables_analysis("term")
variables_analysis("grade")

variables_analysis("ODF_purpose")

variables_analysis("loan_amnt")

```

Como puede verse la variable loan_amnt parece comportarse de manera extraña o al menos poco intuitiva de explicar. Cuanto mayor es el importe del préstamo mayor es la tasa de default, aunque hay un tramo (de 15.175 a 20.475 euros) que parece comportarse mejor que el anterior grupo). Vamos a unir los dos grupos intermedios para conseguir esa monotonocidad.
Creamos una variable loan_amnt2 que será la que usemos a partir de ahora:

```{r}
train$loan_amnt2<-as.factor(ifelse(train$loan_amnt<=15175,"<=15175",ifelse(train$loan_amnt<=20475,"<=20475",">20475")))

test$loan_amnt2<-as.factor(ifelse(test$loan_amnt<=15175,"<=15175",ifelse(test$loan_amnt<=20475,"<=20475",">20475")))
```

y vemos los resultados:

```{r}
variables_analysis("loan_amnt2")
```

Una vez que vemos que los grupos tienen sentido y que variables queremos emplear, realizamos el agrupamiento en las muestras, creando una nueva variable transformada para cada variable de entrada. Usaremos dos funciones:


```{r}

Groupping_numeric<-function(variable){

name<-as.character(variable)  
aux_train<-train[,c("id","loan_status",name)]
aux_test<-test[,c("id","loan_status",name)]
colnames(aux_train)[3]<-"var_name"
colnames(aux_test)[3]<-"var_name"
aux_train$loan_status<-as.numeric(ifelse(train$loan_status==1,0,1))
aux_test$loan_status<-as.numeric(ifelse(test$loan_status==1,0,1))
groupping<-smbinning_calc(df=aux_train,y="loan_status",x="var_name",p=0.05)

table_woe<-as.data.frame(groupping$ivtable)  
 for (i in 1:nrow(table_woe)){
   
   table_woe$ID[i]<-i
   
 }
table_woe$ID<-paste("0",table_woe$ID,sep='')


aux_train<-smbinning.gen(aux_train,groupping,"Bucket")
aux_train$ID<-substr(aux_train$Bucket,1,2)

aux_test<-smbinning.gen(aux_test,groupping,"Bucket")
aux_test$ID<-substr(aux_test$Bucket,1,2)


#table_woe$Bucket<-paste(row.names(table_woe),table_woe$Cutpoint,sep=' ')
#table_woe$Bucket<-paste("0",table_woe$Bucket,sep='')
table_woe<-table_woe[,c("ID","Cutpoint","WoE")]

aux_train<-merge(aux_train,table_woe,by.x=c("ID"),all.x=TRUE)
aux_train$Bucket<-NULL
aux_test<-merge(aux_test,table_woe,by.x=c("ID"),all.x=TRUE)
aux_test$Bucket<-NULL

miss_woe<-table_woe[grep("Missing",table_woe$Cutpoint),]

if (is.na(miss_woe$WoE)==FALSE){

value<- as.numeric(miss_woe$WoE)
#value<-as.numeric(gsub("NaN",0,value))
  
} else { value <-0 }

aux_train$WoE[is.na(aux_train$WoE)] <- value
aux_test$WoE[is.na(aux_test$WoE)] <- value

colnames(aux_train)[ncol(aux_train)]<-paste("WOE_",variable,sep='')
colnames(aux_test)[ncol(aux_test)]<-paste("WOE_",variable,sep='')

aux_train$sample<-"train"
aux_test$sample<-"test"

sample<-rbind(aux_train,aux_test)
sample<-sample[,c("id",paste("WOE_",variable,sep=''))]

return(sample)


}



Groupping_nominal<-function(variable){


name<-as.character(variable)  
aux_train<-train[,c("id","loan_status",name)]
aux_test<-test[,c("id","loan_status",name)]
colnames(aux_train)[3]<-"var_name"
colnames(aux_test)[3]<-"var_name"
aux_train$loan_status<-as.numeric(ifelse(train$loan_status==1,0,1))
aux_test$loan_status<-as.numeric(ifelse(test$loan_status==1,0,1))
aux_train$var_name<-as.factor(aux_train$var_name)
aux_test$var_name<-as.factor(aux_test$var_name)
groupping<-smbinning.factor(df=aux_train,y="loan_status",x="var_name",maxcat = 8)

table_woe<-as.data.frame(groupping$ivtable)  
 for (i in 1:nrow(table_woe)){
   
   table_woe$ID[i]<-i
   
 }
table_woe$ID<-paste("0",table_woe$ID,sep='')


aux_train<-smbinning.factor.gen(aux_train,groupping,"ID")

aux_test<-smbinning.factor.gen(aux_test,groupping,"ID")



table_woe$ID<-paste(table_woe$ID,table_woe$Cutpoint,sep=' ')

table_woe<-table_woe[,c("ID","Cutpoint","WoE")]

aux_train<-merge(aux_train,table_woe,by.x=c("ID"),all.x=TRUE)
aux_test<-merge(aux_test,table_woe,by.x=c("ID"),all.x=TRUE)


miss_woe<-table_woe[grep("Missing",table_woe$Cutpoint),]

if (is.na(miss_woe$WoE)==FALSE){

value<- as.numeric(miss_woe$WoE)
#value<-as.numeric(gsub("NaN",0,value))
  
} else { value <-0 }

aux_train$WoE[is.na(aux_train$WoE)] <- value
aux_test$WoE[is.na(aux_test$WoE)] <- value

colnames(aux_train)[ncol(aux_train)]<-paste("WOE_",variable,sep='')
colnames(aux_test)[ncol(aux_test)]<-paste("WOE_",variable,sep='')

aux_train$sample<-"train"
aux_test$sample<-"test"

sample<-rbind(aux_train,aux_test)
sample<-sample[,c("id",paste("WOE_",variable,sep=''))]

return(sample)


}

```

Hacemos el agrupamiento aplicando las funciones anteriores:

```{r}
train<-merge(train,Groupping_numeric("Fico_score"),by.x=c("id"))
train<-merge(train,Groupping_numeric("revol_util"),by.x=c("id"))
train<-merge(train,Groupping_numeric("annual_inc"),by.x=c("id"))
train<-merge(train,Groupping_numeric("dti"),by.x=c("id"))
train<-merge(train,Groupping_numeric("ODF_addr_state"),by.x=c("id"))
train<-merge(train,Groupping_nominal("grade"),by.x=c("id"))
train<-merge(train,Groupping_nominal("term"),by.x=c("id"))
train<-merge(train,Groupping_nominal("ODF_purpose"),by.x=c("id"))
train<-merge(train,Groupping_nominal("loan_amnt2"),by.x=c("id"))
train<-merge(train,Groupping_nominal("flag_cliente"),by.x=c("id"))

test<-merge(test,Groupping_numeric("Fico_score"),by.x=c("id"))
test<-merge(test,Groupping_numeric("revol_util"),by.x=c("id"))
test<-merge(test,Groupping_numeric("annual_inc"),by.x=c("id"))
test<-merge(test,Groupping_numeric("dti"),by.x=c("id"))
test<-merge(test,Groupping_numeric("ODF_addr_state"),by.x=c("id"))
test<-merge(test,Groupping_nominal("grade"),by.x=c("id"))
test<-merge(test,Groupping_nominal("term"),by.x=c("id"))
test<-merge(test,Groupping_nominal("ODF_purpose"),by.x=c("id"))
test<-merge(test,Groupping_nominal("loan_amnt2"),by.x=c("id"))
test<-merge(test,Groupping_nominal("flag_cliente"),by.x=c("id"))
```

Vemos las nuevas variables creadas en las muestras:

```{r}
colnames(train)
colnames(test)

save.image("Backup4.RData")

```


EJERCICIO 4: ESTIMACIÓN DEL MODELO


Aunque se ha realizado el análisis individual de cada variable ahora debemos combinarlas para que consigan separar a los clientes buenos de los malos de la mejor manera posible. 
Se pretende que los modelos sean parsimoniosos, es decir con el menor número posible de variables, para que sea comprensible y, por otro lado que no tenga variables correlacionadas. Vamos a ver las correlaciones existentes:


```{r}

rm(list=setdiff(ls(), c("Loans","train","test")))

variables_relevantes<-names(train[,32:41])

# Correlation matrix

correlations<-cor(train[, variables_relevantes], use="pairwise", method="pearson")

corrplot(correlations, mar=c(0,0,1,0))
title(main="Correlaci?n entre variables")



```

como se puede apreciar no hay una correlación muy elevada entre las variables, la variable que más correlación presenta es el Fico Score con el grade o calificación que proporciona LendingClub:


```{r}

# Generar un cl?ster jer?rquico de variables.

hc <- hclust(dist(correlations), method="average")

# Generar el dendrograma.

dn <- as.dendrogram(hc)

op <- par(mar = c(3, 4, 3, 7.14))
plot(dn, horiz = TRUE, nodePar = list(col = 3:2, cex = c(2.0, 0.75), pch = 21:22, bg=  c("light blue", "pink"), lab.cex = 0.75, lab.col = "tomato"), edgePar = list(col = "gray", lwd = 2), xlab="Height")
title(main="Clusters de correlacion de variables
 train_TRN usando Pearson")
par(op)


```

Las correlaciones entre variables no estarían correctamente calculadas ya que lo que tenemos ahora son trasnformaciones de la variable a 4 o 5 valores máximo en media, lo cual aumenta las probabilidades de correlación entre variables). 

Por este motivo es más recomendable el uso de la V de Cramer como medida de asociación entre las variables:

```{r}

get.V<-function(y){
  library(vcd)
  col.y<-ncol(y)
  V<-matrix(ncol=col.y,nrow=col.y)
  for(i in 1:col.y){
    for(j in 1:col.y){
      V[i,j]<-assocstats(table(y[,i],y[,j]))$cramer
    }
  }
  return(V)
}


Cramer_corr<-get.V(train[,variables_relevantes])
colnames(Cramer_corr)<-colnames(train[,variables_relevantes])
rownames(Cramer_corr)<-colnames(train[,variables_relevantes])

```


Se considera que dos variables estan altamente correladas si tienen una V de Cramer superior a 0.4:

```{r}
corrplot(Cramer_corr)

```


Vamos a pasar a hacer diferentes modelos para ver cuál sería nuestro modelo elegido. Al tener 9 variables (el grade lo excluiremos) tendríamos la posibilidad de realizar 2^9 -1  modelos diferentes.

Normalmente, como contamos con un número más elevado de variables no calculamos todos los modelos. En estos casos empleamos una técnica denominada Branch and Bound, que permite calcular los mejores modelos sin necesidad de llevar a cabo cada uno de ellos por separado.

El paquete que permite hacer este procedimiento en R requiere de rJava y en ocasiones no funciona en todos los ordenadores que hemos probado, sobretodo en los ordenadores de empresa donde hay más problemas de permisos.

Por este motivo vamos a calcular todos los modelos posibles esta vez. Para esto vamos a realizar todas las combinaciones de variables solamente de 8 variables (interpretemos que los analistas o el MO del modelo no quiere modelos ni de más ni de menos variables)

Además de hacer las combinaciones, vamos a calcular cuál es el poder predictivo de cada modelo sobre cada una de las muestras (train and test) y vamos a omitir modelos con variables correlacionadas o cuyo coeficiente o signo en el modelo sea contrario a lo esperado:


```{r}

combinations<-function(variables,comb){

d <- combn(variables,comb) # All combinations 2 variables
d <-as.data.frame(t(d))

for (i in 1:nrow(d)){
  
  d$id[i]<-paste(comb,"_",i,sep='')
}

return(d)
}

variables<-names(train[,variables_relevantes])



combinats8<-combinations(variables,8)



hacer_modelos<-function(modelos){

lista<-as.data.frame(modelos)
temp<-matrix("NA",0,5)
temp<-as.data.frame(temp)
colnames(temp)<-c("id","Gini_train","Gini_test","test_signos","test_correlaciones")


for (j in 1:(nrow(lista))){


aux<-as.data.frame((as.data.frame(lista[j,])))

for (i in 1:(ncol(aux))){


colnames(aux)[i]<-as.character(aux[1,i])

}

ind<-names(aux[,1:ncol(aux)-1])
temp2<-matrix("NA",1,5)
temp2[1,1]<-lista[j,"id"]
rm(aux)


Logit_model <- glm(loan_status ~ ., data=train[, c("loan_status",ind)],
    family=binomial(link="logit"))

pr_logit_train <- predict(Logit_model, type="response", newdata=train[, c("loan_status",ind)])

pred <- prediction(pr_logit_train, train$loan_status)
pe_Logit <- performance(pred, "tpr", "fpr")
au_Logit <- performance(pred, "auc")@y.values[[1]]

temp2[1,2]<-as.numeric(round(2*au_Logit-1,4))


pr_logit_test <- predict(Logit_model, type="response", newdata=test[, c("loan_status",ind)])

pred <- prediction(pr_logit_test, test$loan_status)
pe_Logit <- performance(pred, "tpr", "fpr")
au_Logit <- performance(pred, "auc")@y.values[[1]]

temp2[1,3]<-as.numeric(round(2*au_Logit-1,4))


coeficientes<-as.data.frame(Logit_model$coefficient)
coeficientes<-as.data.frame(coeficientes[coeficientes[,1]!="(Intercept)",])
temp2[1,4]<-ifelse(max(coeficientes)<0,1,0)

Correlations<-get.V(train[,ind])
diag(Correlations)<-0
test_cor<-ifelse(abs(max(Correlations))<0.4,1,0)
temp2[1,5]<-test_cor
colnames(temp2)<-colnames(temp)
temp<-rbind(temp,temp2)
rm(temp2)
print("Modelo")
print(j)
print("de")
print(nrow(lista))
}
return(temp)
}


```

Procedemos a realizar los modelos (puede llevar algo de tiempo):

```{r results='hide'}

t <- proc.time()

modelos8<-hacer_modelos(combinats8)
proc.time()-t 

#resultadosModelos<-rbind(modelos6,modelos7,modelos8)
resultadosModelos<-modelos8
head(resultadosModelos)

save.image("Backup5.RData")
```


Vamos a filtrar los modelos que no cumplen las restricciones de signo negativo de los coeficientes (al hacerse la transformación a Woes todos los valores deberían venir en negativo) y de correlaciones, es decir las variables del modelo no deberían estar correlacionadas:

```{r}

resultadosModelos<-resultadosModelos[resultadosModelos$test_signos==1 & resultadosModelos$test_correlaciones==1,]

resultadosModelos$Gini_train<-as.numeric(as.character(resultadosModelos$Gini_train))
resultadosModelos$Gini_test<-as.numeric(as.character(resultadosModelos$Gini_test))

resultadosModelos<-resultadosModelos[order(-resultadosModelos$Gini_train),]

```

Seleccionamos el mejor modelo:

```{r}
resultadosModelos<-head(resultadosModelos,1)
resultadosModelos
```

Vamos a ver que variables están dentro del modelo:

```{r}
best_model<-merge(resultadosModelos,combinats8,by.x="id")
t(best_model[,6:13])
```

Estimemos el modelo para ver los coeficientes:

```{r}
variables<-c("WOE_Fico_score","WOE_revol_util","WOE_annual_inc","WOE_ODF_addr_state","WOE_term","WOE_ODF_purpose","WOE_loan_amnt2")

Best_Logit_Model <- glm(loan_status ~ ., data=train[, c("loan_status",variables)],
    family=binomial(link="logit"))

summary(Best_Logit_Model)
```

```{r}

save.image("Backup6.RData")

```



EJERCICIO 5: PUNTOS DE CORTE

Podemos asignar una probabilidad de acuerdo a nuestro modelo para cada solicitante de nuestras muestras:

```{r}
options(digits=4)

PD <- predict(Best_Logit_Model, type="response", newdata=train[, c("loan_status",variables)])

# print(PD)

Predictions_Logit<-cbind(train,PD)
```

Vamos a transforman nuestro modelo en una tarjeta de puntuación, donde tener 500 puntos implicaría tener una probabilidad de incumpliento del 50%. Es decir existe un mal cliente por cada uno bueno.

Además, cada 20 puntos, se doblan los odds, es decir en los 520 puntos habría 2 clientes buenos por cada uno malo.

```{r}
Puntos<-500
Odds<-1
Puntos_doblar_odds<-20

Factor<-Puntos_doblar_odds/log(2)

Offset<-Puntos-(Factor*log(Odds))

Predictions_Logit$Points<-round(Offset+(log((1-PD)/PD))*Factor,0)

hist(Predictions_Logit$Points,main="Distribution of points",xlab="Points")
```
Imaginaros que el modelo antiguo, el cuál será remplazado por este modelo, aprueba automáticamente el 80% de las solicitudes de préstamos con un bad rate del 15%. 

Vamos a partir nuestro dataset en 20 puntos de corte de puntuación y analizamos como podemos usar este modelo para sustituir al modelo teórico anterior y además controlar el % de admisión y la tasa de incumplimiento esperada:

```{r}
Predictions_Logit$bins <- cut(Predictions_Logit$Points, 20, include.lowest=TRUE)

aux<-sqldf("SELECT bins,
      sum(loan_status) as DEFAULTS,
      count(loan_status) as N,
      avg(PD) as Avg_pred_PD
      from Predictions_Logit group by bins")

aux$FDO<-aux$DEFAULTS/aux$N

for (i in 1:nrow(aux)){
  ifelse(i==1,aux$N_acum[i]<-aux$N[i],aux$N_acum[i]<-aux$N[i]+aux$N_acum[i-1])
  ifelse(i==1,aux$Def_acum[i]<-aux$DEFAULTS[i],aux$Def_acum[i]<-aux$DEFAULTS[i]+aux$Def_acum[i-1])

}

aux$aceptance_above<-1-(aux$N_acum/sum(aux$N))
defaults<-sum(aux$DEFAULTS)
registros<-nrow(Predictions_Logit)

aux$defaults_above<-(defaults-aux$Def_acum)/registros

aux<-aux[,c("bins","aceptance_above","defaults_above")]
aux2<-sqldf("SELECT bins,
      max(PD) as PD_superior,
      max(Points) as Points_superior
      from Predictions_Logit group by bins")

aux<-merge(aux,aux2,by.x="bins")

```


¿Qué quiere decir esta tabla que hemos obtenido? Veamos el primer punto: la columna PD_superior es el punto de corte establecido en ese tramo. También encontramos los puntos equivalentes en el tramo.

Concretamente si rechazáramos todos los préstamos de solicitantes con un score inferior a 543 puntos, estaríamos aceptando un 75.22% de los solicitantes, lo que estaría generando una tasa de defaults del 7.69%.

En base a la información anterior nos planteamos una serie de preguntas:

1- Si queremos mantener la misma tasas de aceptación actual, ¿cuál sería el punto de corte que propondríais?

2- Y si queremos mantener la misma tasa de defaults como hasta ahora, ¿qué punto de corte podría establecerse?

```{r}

muestra_desarrollo<-cbind(train,Predictions_Logit[,42:44])
rm(list=setdiff(ls(), "muestra_desarrollo"))

save.image("Backup7.RData")

```




EJERCICIO 6: MUESTRAS OOT AND TTD

```{r}
library(Hmisc)

OOT_sample <- read.csv("OOT_sample.csv", stringsAsFactors=FALSE)

unique(OOT_sample$issue_d)
```



```{r}

print("El poder predictivo del modelo en la muestra de desarrollo era:")
(2*rcorr.cens(muestra_desarrollo$PD,muestra_desarrollo$loan_status)[1])-1

```

```{r}

print("El poder predictivo del modelo en la muestra OOT es:")
(2*rcorr.cens(OOT_sample$PD,OOT_sample$loan_status)[1])-1

```

Parece que el poder predictivo en una muestra un poco mas reciente a la que se desarrollo el modelo, parece seguir siendo valido.
Podemos evaluar si hay signos de fatiga en el modelo o de que no vaya a funcionar bien en una muestra todav?a mas reciente pero en la que aun no hay desempeño, es decir no podemos ver cual es el resultado de pago impago de los prestamos. 

Se analiza solamente la estabilidad a traves de la metrica PSI  Population Stability Index. Valores entre 0.1 y 0.25 indican que se ha producido una variacion leve en la distribucion de la poblacion en comparacion con la muestra de desarrollo. VAlores m?s altos que 0.25 indican que la distribucion de la poblacion para esa variable se ha modificado drasticamente.

Primero cargamos nuestra muestra TTD:

```{r}
TTD_sample <- read.csv("TTD_sample.csv", stringsAsFactors=FALSE)

unique(TTD_sample$issue_d)
```



```{r}

psi<-function(muestra_des,muestra_actual,variable){

a1<-data.frame(table(muestra_des[,c(variable)]))
a2<-data.frame(table(muestra_actual[,c(variable)]))

a3<-merge(a1,a2,by=c("Var1"))
colnames(a3)<-c(var,"des","actual")
a3$perc_des<-a3$des/sum(a3$des)
a3$perc_actual<-a3$actual/sum(a3$actual)
a3$psi_index<-(a3$perc_actual-a3$perc_des)*log(a3$perc_actual/a3$perc_des)
psi<-sum(a3$psi_index)

return(psi)

}

# ggplot(a3, aes(length, fill = veg)) + geom_density(alpha = 0.2)
```

```{r}
# 
# muestra_des <- muestra_desarrollo
# muestra_actual <- TTD_sample
# variable <- "WOE_revol_util"
# 
# a1<-data.frame(table(muestra_des[,c(variable)]))
# a2<-data.frame(table(muestra_actual[,c(variable)]))
# 
# a3<-merge(a1,a2,by=c("Var1"))
# colnames(a3)<-c("var","des","actual")
# a3$perc_des<-a3$des/sum(a3$des)
# a3$perc_actual<-a3$actual/sum(a3$actual)
# a3$psi_index<-(a3$perc_actual-a3$perc_des)*log(a3$perc_actual/a3$perc_des)
# psi_value<-sum(a3$psi_index)
# psi_value
```



Vamos a calcular el PSI para todas las variables del modelo:


```{r}
print("WOE_Fico_score:")
psi(muestra_desarrollo,TTD_sample,"WOE_Fico_score")

print("WOE_revol_util:")
psi(muestra_desarrollo,TTD_sample,"WOE_revol_util")

print("WOE_annual_inc:")
psi(muestra_desarrollo,TTD_sample,"WOE_annual_inc")

print("WOE_ODF_addr_state:")
psi(muestra_desarrollo,TTD_sample,"WOE_ODF_addr_state")

print("WOE_term:")
psi(muestra_desarrollo,TTD_sample,"WOE_term")

print("WOE_ODF_purpose:")
psi(muestra_desarrollo,TTD_sample,"WOE_ODF_purpose")

print("WOE_loan_amnt2:")
psi(muestra_desarrollo,TTD_sample,"WOE_loan_amnt2")

```


En este caso vemos que la única variable con un PSI elevado sería la WOE_revol_util con un 26% de PSI que indicaría que no se aprecia estabilidad en la misma. En este caso, lo que tendríamos que hacer ya que el modelo funciona bien en muestras OOT (no varía el poder predictivo) sería realizar un seguimiento cercano al modelo sobre todo para ver si más variables dejan de ser estables. 









