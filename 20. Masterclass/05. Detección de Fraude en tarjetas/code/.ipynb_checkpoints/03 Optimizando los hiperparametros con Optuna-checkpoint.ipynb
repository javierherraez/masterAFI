{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e842e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizando los hiperparámetros con Optuna\n",
    "\n",
    "Optuna es una librería dedicada a la optimización de hiperparámetros, tanto para problemas de machine learning como para otras tareas más genéricas, siempre y cuando podamos definir una función __objetivo__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c663ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Características:\n",
    "\n",
    "- Interface muy intuitivo\n",
    "- Ofrece muchas estrategias de muestreo y poda para personalizar el proceso de búsqueda\n",
    "- Ligero en cuanto a dependencias\n",
    "- Fácilmente distribuible\n",
    "- Ofrece paneles de visualización con los resultados\n",
    "- Puede funcionar con cualquier tipo de código en el que haya que optimizar una función objetivo\n",
    "\n",
    "`conda install -c conda-forge optuna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d080ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b046dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sintaxis básica de Optuna\n",
    "\n",
    "En Optuna, el proceso de optimización se denomina un __estudio__ (_study_). Un estudio necesita una __función objetivo para optimizar.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359d450",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### La función objetivo\n",
    "\n",
    "Tipicamente esta la define el usuario; ha de llamarse `objective` y tener la siguiente estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23156cb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial, **params):\n",
    "    \"\"\"Conventional optimization function\n",
    "    signature for optuna.\n",
    "    \"\"\"\n",
    "    custom_metric = ...\n",
    "    return custom_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22975f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ha de aceptar un objeto `optuna.Trial` como parámetro y los parámetros opcionales que sean necesarios. Devuelve la métrica que estamos optimizando.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58145d65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### El Estudio\n",
    "\n",
    "Un estudio es una colección de __pruebas__ (`trials`) en la que en cada una evaluamos la función objetivo usando un juego de hiperparámetros dentro del espacio de búsqueda. Cada prueba es un objeto de la clase `optuna.Trial`. Esta clase es la que Optuna usa para buscar los valores óptimos.\n",
    "\n",
    "El estudio se crea especificando si queremos maximizar (ROC AUC, accuracy, etc.) o minimizar (RMSE, RMSLE, log loss, etc.) la función objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c1b4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d308e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finalmente, se optimiza el estudio pasando la función objetivo y el número de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a9a85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Optimization with 100 trials\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f7b2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definiendo la función objetivo\n",
    "\n",
    "En nuestro caso, la función objetivo ha de contener el algoritmo de ML. Por lo tanto, tendrá que:\n",
    "\n",
    "- Definir el modelo\n",
    "- Entrenarlo\n",
    "- Predecir los nuevos datos\n",
    "- Calcular la métrica a optimizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292506e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial, X, y):\n",
    "    rf_params = {\n",
    "        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=100, high=2000),\n",
    "        \"max_depth\": trial.suggest_float(\"max_depth\", 3, 8),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", choices=[\"sqrt\", \"log2\"]\n",
    "        ),\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 1121218,\n",
    "    }\n",
    "    rf = RandomForestRegressor(**rf_params)\n",
    "    \n",
    "    scores = cross_val_score(rf, X, y, n_jobs=-1)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c3b3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para reducir el espacio de búsqueda, `suggest_float` y `suggest_int` pueden tomar argumentos `log` y `step`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72546c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial, X, y):\n",
    "    rf_params = {\n",
    "        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=10, high=3000, log=True),\n",
    "        \"max_depth\": trial.suggest_float(\"max_depth\", 1, 8, step = 2),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", choices=[\"sqrt\", \"log2\"]\n",
    "        ),\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 1121218,\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestRegressor(**rf_params)\n",
    "    \n",
    "    scores = cross_val_score(rf, X, y, n_jobs=-1)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ed3af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Cómo se muestrean los hiperparámetros?\n",
    "\n",
    "Hay varias opciones:\n",
    "\n",
    "- GridSampler: lo mismo que el `GridSearch` de `sklearn`. No muy recomendable excepto para casos muy concretos\n",
    "- RandomSampler: lo mismo que el `RandomizedSearch` de `sklearn`. \n",
    "- TPESampler: (Tree-structured Parzen Estimator) - método bayesiano de muestreo\n",
    "- CmaEsSampler: muestreo basado el el algoritmo CMA ES (no admite hiperparámetros categóricos).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23228d50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El TPESampler es el usado por defecto: típicamente muestrea los hiperparámetros tratando de mejorar la última prueba.  \n",
    "\n",
    "> On each trial, for each parameter, TPE fits one Gaussian Mixture Model (GMM) l(x) to the set of parameter values associated with the best objective values, and another GMM g(x) to the remaining parameter values. It chooses the parameter value x that maximizes the ratio l(x)/g(x).\n",
    "\n",
    "Para cambiar el método de muestreo se hace del siguiente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270581a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from optuna.samplers import CmaEsSampler, RandomSampler, GridSampler\n",
    "\n",
    "# Study with a random sampler\n",
    "study = optuna.create_study(sampler=RandomSampler(seed=1121218))\n",
    "\n",
    "# Study with a CMA ES sampler\n",
    "study = optuna.create_study(sampler=CmaEsSampler(seed=1121218))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577977a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set() # Sobreescribe los parámetros de matplotlib\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leemos los datos \n",
    "fraude = pd.read_csv('../data/01_datos_4_training_cut.txt', sep='|', nrows= 100000)\n",
    "RS = 1\n",
    "fraude = fraude.sample(frac=1, random_state = RS) \n",
    "\n",
    "# Los limpiamos\n",
    "\n",
    "fraude.drop(['IDTX'], axis = 1, inplace=True)\n",
    "fraude.FECHATRX = pd.to_datetime(fraude.FECHATRX)\n",
    "\n",
    "columnas_sin_cambios = ['IDTX', 'FECHATRX','VALOR_TRX']\n",
    "\n",
    "for columna in fraude.columns:\n",
    "    if columna not in columnas_sin_cambios:\n",
    "        fraude[columna] = fraude[columna].astype('category')\n",
    "        \n",
    "\n",
    "# Definimos los predictores\n",
    "numericos = [fraude.columns[-2]]\n",
    "categoricos = list(fraude.columns[i] for i in [3,4,5,6,13,14,15,17])\n",
    "target = list(fraude.columns[i] for i in [0,1,2,7,9,10,12,16])\n",
    "\n",
    "\n",
    "# Definimos los vectores de predictores y la respuesta\n",
    "y = fraude['REPORTE_DE_FRAUDE']\n",
    "X = fraude[numericos + categoricos]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76b836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6369fb6",
   "metadata": {},
   "source": [
    "## El Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec608f9d",
   "metadata": {},
   "source": [
    "`class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148958c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "> __Para entender qué hace cada hiperparámetro__, es obvio que necesitamos tener un conocimiento muy extenso y experiencia con el algoritmo que estamos usando\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cfc5f2",
   "metadata": {},
   "source": [
    "Para cada algoritmo, necesitamos conocer:\n",
    "\n",
    "- Cuáles son __los hiperparámetros más críticos__\n",
    "- Si aumentar sus valores aumenta o disminuye __la complejidad del algoritmo.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b3f85",
   "metadata": {},
   "source": [
    "\n",
    "> La complejidad del algoritmo es la que le hace oscilar del underfittig al overfitting. Necesitamos encontrar el valor medio donde se equilibran los errores\n",
    "\n",
    "---\n",
    "\n",
    "__Recordemos:__ Un modelo poco complejo tiene mucho sesgo (underfitting), pero uno muy complejo mucha varianza (overfitting). __Necesitamos hacer por tanto el proceso de selección de hiperparámetros con un conjunto de datos _externo___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd7464",
   "metadata": {},
   "source": [
    "<img src=\"../images/tress_hp.png\" width=\"900px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a32d8",
   "metadata": {},
   "source": [
    "- n_estimators: El número de árboles. A diferencia el lgbm, meter más árboles no conduce al overfitting. Este parámetro es mejor fijarlo a mano. Un valor bajo hace underfit, pero a partir de un cierto valor, añadir más no conduce a nada y lleva más tiempo entrenar. 50-100 es un buen valor para trabajar.\n",
    "- max_depth: `None` es profundidad ilimitada del árbol (overfitting máximo). Se puede controlar dando valores menores o a través de otros parámetros de regulariación, como el `min_samples_split`. Un valor alrededor de 7 es bueno para empezar.\n",
    "- `max_features` es el número de features que miro en cada división. Hay un valor entre medias de una o el máximo que es el óptimo, y está relacionada con el número de árboles del bosque. Afecta también a la velocidad.\n",
    "- `min_samples_split` controla el número mínimo de observaciones que tenemos que tener en una hoja para hacer una división. El mínimo es 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7852e79",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vamos a tratar de fijar primero el número de árboles.\n",
    "\n",
    "Pero antes, necesitamos encapsular nuestra función de scoring para poder usarla con la API de `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4873f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  make_scorer\n",
    "\n",
    "\n",
    "def recall_at_cutoff(y_test, y_prob, positive_class = 'SI', alarm = None):\n",
    "    \n",
    "    if not alarm:\n",
    "        alarm = sum(y_test == positive_class)/len(y_test)\n",
    "    \n",
    "    resultados = pd.DataFrame({'Prob':y_prob, 'Label':y_test.values})\n",
    "    resultados.sort_values('Prob', axis=0, ascending=False, inplace=True)\n",
    "    resultados.reset_index(inplace=True)\n",
    "    alarmas = int(alarm*len(y_test))\n",
    "    #return sum(y_test == positive_class)\n",
    "    return sum(resultados[0:alarmas].Label==positive_class)/sum(resultados.Label == positive_class)\n",
    "\n",
    "my_scorer = make_scorer(recall_at_cutoff, needs_proba=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2917d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "        \"n_estimators\": 50,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"max_depth\":None,\n",
    "        \"max_features\":\"sqrt\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 1121218,\n",
    "    }\n",
    "\n",
    "## Definimos la cañería para las columnas numéricas\n",
    "steps_num = [('Imputador', SimpleImputer(strategy='median')),\n",
    "             ('BoxCox', PowerTransformer(method='yeo-johnson'))]\n",
    "\n",
    "numeric_transformer = Pipeline(steps_num)\n",
    "\n",
    "## Lo mismo para las categóricas\n",
    "steps_cat = [('Imputador', SimpleImputer(strategy='most_frequent')),\n",
    "            ('OneHot', OneHotEncoder(handle_unknown='ignore'))]\n",
    "\n",
    "categorical_transformer = Pipeline(steps_cat)\n",
    "\n",
    "## Ensamblo las dos cañerías con ColumnTransformer\n",
    "\n",
    "preprocesado = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numericos),\n",
    "        ('cat', categorical_transformer, categoricos)])\n",
    "\n",
    "## Y ahora ensamblo el algoritmo\n",
    "\n",
    "steps = [('feat_prepro', preprocesado), \n",
    "         ('predictor', RandomForestClassifier(**rf_params))]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "pipe.fit(X, y)\n",
    "\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv= 5, scoring=my_scorer, n_jobs=-1)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508eb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()  ## array([0.0625    , 0.1875    , 0.21875   , 0.03125   , 0.12121212])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8dd98b",
   "metadata": {},
   "source": [
    "## Busquemos los mejor hiperparámetros con Optuna\n",
    "\n",
    "Para ello hay que envolver todo el modelo con la función `objective`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial, X, y, cv, scoring):\n",
    "    rf_params = {\n",
    "        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=10, high=400, log=True),\n",
    "        \"min_samples_split\": 2,\n",
    "        \"max_depth\":None,\n",
    "        \"max_features\":\"sqrt\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 1121218\n",
    "    }\n",
    "    \n",
    "    ## Definimos la cañería para las columnas numéricas\n",
    "    steps_num = [('Imputador', SimpleImputer(strategy='median')),\n",
    "                 ('BoxCox', PowerTransformer(method='yeo-johnson'))]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps_num)\n",
    "\n",
    "    ## Lo mismo para las categóricas\n",
    "    steps_cat = [('Imputador', SimpleImputer(strategy='most_frequent')),\n",
    "                ('OneHot', OneHotEncoder(handle_unknown='ignore'))]\n",
    "\n",
    "    categorical_transformer = Pipeline(steps_cat)\n",
    "\n",
    "    ## Ensamblo las dos cañerías con ColumnTransformer\n",
    "\n",
    "    preprocesado = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numericos),\n",
    "            ('cat', categorical_transformer, categoricos)])\n",
    "\n",
    "    ## Y ahora ensamblo el algoritmo\n",
    "\n",
    "    steps = [('feat_prepro', preprocesado), \n",
    "             ('predictor', RandomForestClassifier(**rf_params))]\n",
    "\n",
    "    pipe = Pipeline(steps)\n",
    "    scores = cross_val_score(pipe, X, y, cv=cv,  scoring=scoring, n_jobs=-1)\n",
    "    return scores.mean()\n",
    "    #rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece413ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "# Create study that maximizes\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "\n",
    "# Pass additional arguments inside another function\n",
    "func = lambda trial: objective(trial, X, y, cv=kf,  scoring=my_scorer)\n",
    "\n",
    "# Start optimizing with 100 trials\n",
    "study.optimize(func, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01d1f4",
   "metadata": {},
   "source": [
    "# Visualizando el estudio\n",
    "\n",
    "La verdad es que para obtener el mejor `n_estimators` tampoco era necesario usar optuna: un gridsearch corriente hubiera bastando... Pero lo que es muy interesante es la _visualización_ del estudio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the plot functions\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272c223",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the optimization history. See :func:`~optuna.visualization.plot_optimization_history` for the details.\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b68c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize high-dimensional parameter relationships. See :func:`~optuna.visualization.plot_parallel_coordinate` for the details.\n",
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36f328",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize individual hyperparameters as slice plot. See :func:`~optuna.visualization.plot_slice` for the details.\n",
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1cfb8a",
   "metadata": {},
   "source": [
    "Vamos a fija el número de árboles lo más bajo posible dentro de los mejores resultados. De este modo nos curamos del OF y no son demasiados para no reventar el tiempo de cálculo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac389b8f",
   "metadata": {},
   "source": [
    "## Optimicemos el resto de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b12bcd",
   "metadata": {},
   "source": [
    "Los hiperparámetros más críticos en el árbol ya sabemos que son:\n",
    "\n",
    "1. max_features\n",
    "2. max_depth\n",
    "3. min_samples_split \n",
    "\n",
    "Para estimar sus intervalos de variación necesitamos conocer bien las dimensiones del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_prepro = [('feat_prepro', preprocesado)]\n",
    "prepro = Pipeline(steps_prepro)\n",
    "dimensiones = prepro.fit_transform(X).shape\n",
    "dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_train = dimensiones[0]*4/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(dim_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(dimensiones[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def objective(trial, X, y, cv, scoring):\n",
    "    rf_params = {\n",
    "        \"min_samples_split\": trial.suggest_int(name=\"min_samples_split\", low=2, high=200, log=True),\n",
    "        \"max_features\":  trial.suggest_int(name=\"max_features\", low=4, high=90, log=True),\n",
    "        \"max_depth\": trial.suggest_int(name=\"max_depth\", low=15, high=1000, log=True),\n",
    "        \"n_estimators\": 150,\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 1121218\n",
    "    }\n",
    "    \n",
    "    ## Definimos la cañería para las columnas numéricas\n",
    "    steps_num = [('Imputador', SimpleImputer(strategy='median')),\n",
    "             ('BoxCox', PowerTransformer(method='yeo-johnson'))]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps_num)\n",
    "\n",
    "    ## Lo mismo para las categóricas\n",
    "    steps_cat = [('Imputador', SimpleImputer(strategy='most_frequent')),\n",
    "             ('OneHot', OneHotEncoder(handle_unknown='ignore'))]\n",
    "\n",
    "    categorical_transformer = Pipeline(steps_cat)\n",
    "\n",
    "    ## Ensamblo las dos cañerías con ColumnTransformer\n",
    "\n",
    "    preprocesado = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numericos),\n",
    "            ('cat', categorical_transformer, categoricos)])\n",
    "\n",
    "    ## Y ahora ensamblo el algoritmo\n",
    "\n",
    "    steps = [('feat_prepro', preprocesado), \n",
    "             ('predictor', RandomForestClassifier(**rf_params))]\n",
    "\n",
    "    pipe = Pipeline(steps)\n",
    "    scores = cross_val_score(pipe, X, y, cv=cv, scoring=my_scorer, n_jobs=-1)\n",
    "    return scores.mean()\n",
    "    #rf = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779714c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "# Create study that maximizes\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Wrap the objective inside a lambda with the relevant arguments\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=20201004)\n",
    "# Pass additional arguments inside another function\n",
    "func = lambda trial: objective(trial, X, y, cv=kf, scoring=my_scorer)\n",
    "\n",
    "# Start optimizing with 100 trials\n",
    "study.optimize(func, n_trials=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5f8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To get the dictionary of parameter name and parameter values:\n",
    "print(\"Return a dictionary of parameter name and parameter values:\",study.best_params)\n",
    " \n",
    "# To get the best observed value of the objective function:\n",
    "print(\"Return the best observed value of the objective function:\",study.best_value)\n",
    " \n",
    "# To get the best trial:\n",
    "print(\"Return the best trial:\",study.best_trial)\n",
    " \n",
    "# To get all trials:\n",
    "#print(\"Return all the trials:\", study.trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50b20e",
   "metadata": {},
   "source": [
    "# Visualizando el estudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88ae42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the optimization history. See :func:`~optuna.visualization.plot_optimization_history` for the details.\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c919772",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize high-dimensional parameter relationships. See :func:`~optuna.visualization.plot_parallel_coordinate` for the details.\n",
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f94cc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize individual hyperparameters as slice plot. See :func:`~optuna.visualization.plot_slice` for the details.\n",
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447305ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize parameter importances. See :func:`~optuna.visualization.plot_param_importances` for the details.\n",
    "#In this case, we have only one parameter.\n",
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize empirical distribution function. See :func:`~optuna.visualization.plot_edf` for the details.\n",
    "plot_edf(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1878f4a",
   "metadata": {},
   "source": [
    "## El modelo completo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f34f86",
   "metadata": {},
   "source": [
    "# ¿Es el número que he obtenido una buena estimación del error de mi modelo?\n",
    "\n",
    "> Pregunta: ¿Es una buena estrategia lo que hemos hecho de ir probando algoritmos sobre el conjunto de test para ir subiendo el score? ¿Por qué? ¿Es el número que sacamos una estimación fiable del error? \n",
    "\n",
    "Para poder estimar el error del modelo, tenemos que llevar a cabo una validación cruzada que separe el conjunto de datos en train y test. Pero para eso, tenemos que haber seleccionado previamente los hiperparámetros con los que entrenar.\n",
    "\n",
    "> Necesitamos por lo tanto correr dos bucles de validación uno dentro de otro!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c653c",
   "metadata": {},
   "source": [
    "<img src=\"../images/grid.jpg\" width=\"900px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e2212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643a44b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steps_num = [('Imputador', SimpleImputer(strategy='median')),\n",
    "         ('BoxCox', PowerTransformer(method='yeo-johnson'))]\n",
    "\n",
    "numeric_transformer = Pipeline(steps_num)\n",
    "\n",
    "## Lo mismo para las categóricas\n",
    "steps_cat = [('Imputador', SimpleImputer(strategy='most_frequent')),\n",
    "         ('OneHot', OneHotEncoder(handle_unknown='ignore'))]\n",
    "\n",
    "categorical_transformer = Pipeline(steps_cat)\n",
    "\n",
    "## Ensamblo las dos cañerías con ColumnTransformer\n",
    "\n",
    "preprocesado = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numericos),\n",
    "        ('cat', categorical_transformer, categoricos)])\n",
    "\n",
    "## Y ahora ensamblo el algoritmo\n",
    "\n",
    "steps = [('feat_prepro', preprocesado), \n",
    "         ('predictor', RandomForestClassifier(**study.best_params))]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "cv_results = cross_validate(pipe, \n",
    "                            X, y, \n",
    "                            cv=5, \n",
    "                            return_train_score=True,\n",
    "                            scoring=my_scorer, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results)\n",
    "print(cv_results['train_score'].mean())\n",
    "print(cv_results['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9463fd",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Optimizar los hiperparámetros del algoritmos incorporando las variables codificadas como `Target`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74551d98",
   "metadata": {},
   "source": [
    "# Moraleja de todo esto\n",
    "\n",
    "- Las operaciones de preprocesado (cuáles aplicar y a qué variables), los parámetros del algoritmo e incluso el algoritmo en sí __son elementos del modelo en pie de igualdad__, y para _seleccionar_ aquellas que son más adecuadas a mis datos __tengo que realiza un proceso riguroso de validación, tomando como criterio de selección la función objetivo que me he asignado__   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce135d74",
   "metadata": {},
   "source": [
    "# ¿Cómo seguir mejorando?\n",
    "\n",
    "- Utilizar técnicas de remuestreo para equilibrar las clases\n",
    "- Realizar el entrenamiento _por grupos_ de tarjeta\n",
    "- Introducir la estructura temporal\n",
    "- ¿Qué variables es legítimo usar?? Es importante no hacerse trampas al solitario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614c895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
